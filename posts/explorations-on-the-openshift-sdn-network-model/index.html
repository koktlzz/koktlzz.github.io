<!doctype html><html lang=en><head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#"><meta charset=utf-8><meta name=generator content="Hugo 0.88.1"><meta name=theme-color content="#fff"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no, date=no, address=no, email=no"><meta http-equiv=cache-control content="no-transform"><meta http-equiv=cache-control content="no-siteapp"><title>对 Openshift SDN 网络模型的一些探索 | Inspire Hub</title><link rel=stylesheet href=/css/meme.min.28f2f04dec4bcad2ed181d4a2a06c29247e962f3b57bcfbb8c6a8d8d1ce69f95.css><script src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js defer></script><script src=/js/meme.min.dfc1039d5d556df64bc1a24d28623313cb561960f9ca8cb162312e6b3a13cf30.js></script>
<link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&family=Comfortaa:wght@700&display=swap" media=print onload="this.media='all'"><noscript><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,400;0,500;0,700;1,400;1,700&family=Source+Code+Pro:ital,wght@0,400;0,700;1,400;1,700&family=Comfortaa:wght@700&display=swap"></noscript><meta name=author content="koktlzz"><meta name=description content="前言 在 Kubernetes Pod 是如何跨节点通信的？中，我们简单地介绍了 Kubernetes 中的两种 SDN 网络模型：Underlay 和 Overlay。而 Openshift 中的 SDN 则是由 Overlay 网络 OVS（Open vSwitch）实现的，其使用的插件如下：
 ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamespaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。  OVS 在每个 Openshift 节点上都创建了如下网络接口："><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=mask-icon href=/icons/safari-pinned-tab.svg color=#2a6df4><link rel=apple-touch-icon sizes=180x180 href=/icons/apple-touch-icon.png><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-title content="Inspire Hub"><meta name=apple-mobile-web-app-status-bar-style content="black"><meta name=mobile-web-app-capable content="yes"><meta name=application-name content="Inspire Hub"><meta name=msapplication-starturl content="../../"><meta name=msapplication-TileColor content="#fff"><meta name=msapplication-TileImage content="../../icons/mstile-150x150.png"><link rel=manifest href=/manifest.json><link rel=canonical href=/posts/explorations-on-the-openshift-sdn-network-model/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","datePublished":"2020-05-13T09:19:42+01:00","dateModified":"2021-10-28T15:20:32+08:00","url":"/posts/explorations-on-the-openshift-sdn-network-model/","headline":"对 Openshift SDN 网络模型的一些探索","description":"前言 在 Kubernetes Pod 是如何跨节点通信的？中，我们简单地介绍了 Kubernetes 中的两种 SDN 网络模型：Underlay 和 Overlay。而 Openshift 中的 SDN 则是由 Overlay 网络 OVS（Open vSwitch）实现的，其使用的插件如下：\n ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamespaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。  OVS 在每个 Openshift 节点上都创建了如下网络接口：","inLanguage":"en","articleSection":"posts","wordCount":1475,"image":["https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132046.jpeg","https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105132042.jpeg","https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/20210516142844.png","https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132044.jpeg","https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132045.jpeg"],"author":{"@type":"Person","email":"koktlgwr@gmail.com","image":"/icons/apple-touch-icon.png","url":"/","name":"koktlzz"},"license":"[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)","publisher":{"@type":"Organization","name":"Inspire Hub","logo":{"@type":"ImageObject","url":"/icons/apple-touch-icon.png"},"url":"/"},"mainEntityOfPage":{"@type":"WebSite","@id":"/"}}</script><meta property="og:title" content="对 Openshift SDN 网络模型的一些探索"><meta property="og:description" content="前言 在 Kubernetes Pod 是如何跨节点通信的？中，我们简单地介绍了 Kubernetes 中的两种 SDN 网络模型：Underlay 和 Overlay。而 Openshift 中的 SDN 则是由 Overlay 网络 OVS（Open vSwitch）实现的，其使用的插件如下：
 ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamespaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。  OVS 在每个 Openshift 节点上都创建了如下网络接口："><meta property="og:url" content="/posts/explorations-on-the-openshift-sdn-network-model/"><meta property="og:site_name" content="Inspire Hub"><meta property="og:locale" content="en"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132046.jpeg"><meta property="og:type" content="article"><meta property="article:published_time" content="2020-05-13T09:19:42+01:00"><meta property="article:modified_time" content="2021-10-28T15:20:32+08:00"><meta property="article:section" content="posts"></head><body><div class=container><header class=header><div class=header-wrapper><div class="header-inner single"><div class=site-brand><a href=/ class=brand>Inspire Hub</a></div><nav class=nav><ul class=menu id=menu><li class=menu-item><a href=/posts/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon archive"><path d="M32 448c0 17.7 14.3 32 32 32h384c17.7.0 32-14.3 32-32V160H32v288zm160-212c0-6.6 5.4-12 12-12h104c6.6.0 12 5.4 12 12v8c0 6.6-5.4 12-12 12H204c-6.6.0-12-5.4-12-12v-8zM480 32H32C14.3 32 0 46.3.0 64v48c0 8.8 7.2 16 16 16h480c8.8.0 16-7.2 16-16V64c0-17.7-14.3-32-32-32z"/></svg><span class=menu-item-name>Posts</span></a></li><li class=menu-item><a href=/tags/><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" class="icon tags"><path d="M497.941 225.941 286.059 14.059A48 48 0 00252.118.0H48C21.49.0.0 21.49.0 48v204.118a48 48 0 0014.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882.0l204.118-204.118c18.745-18.745 18.745-49.137.0-67.882zM112 160c-26.51.0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882.0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397.0h48.721a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882z"/></svg><span class=menu-item-name>Tags</span></a></li><li class=menu-item><a href></a></li><li class="menu-item search-item"><form id=search class=search role=search><label for=search-input><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon search-icon"><path d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></label><input type=search id=search-input class=search-input></form><template id=search-result hidden><article class="content post"><h2 class=post-title><a class=summary-title-link></a></h2><summary class=summary></summary><div class=read-more-container><a class=read-more-link>Read More »</a></div></article></template></li></ul></nav></div></div><input type=checkbox id=nav-toggle aria-hidden=true>
<label for=nav-toggle class=nav-toggle></label>
<label for=nav-toggle class=nav-curtain></label></header><main class="main single" id=main><nav class=contents><h2 id=contents class=contents-title>On This Page</h2><ol class=toc><li><a id=contents:前言 href=#前言>前言</a></li><li><a id=contents:pod-to-local-pod href=#pod-to-local-pod>Pod to Local Pod</a></li><li><a id=contents:pod-to-remote-pod href=#pod-to-remote-pod>Pod to Remote Pod</a><ol><li><a id=contents:packet-in-local-pod href=#packet-in-local-pod>Packet in Local Pod</a></li><li><a id=contents:packet-in-remote-pod href=#packet-in-remote-pod>Packet in Remote Pod</a></li></ol></li><li><a id=contents:pod-to-service href=#pod-to-service>Pod to Service</a></li><li><a id=contents:pod-to-external href=#pod-to-external>Pod to External</a></li><li><a id=contents:future-work href=#future-work>Future Work</a></li><li><a id=contents:参考文献 href=#参考文献>参考文献</a></li></ol></nav><div class=main-inner><article class="content post h-entry" data-small-caps=true data-align=default data-type=posts><h1 class="post-title p-name">对 Openshift SDN 网络模型的一些探索</h1><div class=post-tags><a href=/tags/openshift/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>Openshift</a>
<a href=/tags/network/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>Network</a>
<a href=/tags/cni/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>CNI</a>
<a href=/tags/open-vswitch/ rel=tag class=post-tags-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon tag-icon"><path d="M0 252.118V48C0 21.49 21.49.0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137.0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882.0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51.0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg>Open vSwitch</a></div><div class="post-body e-content"><h2 id=前言><a href=#前言 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:前言 class=headings>前言</a></h2><p>在 <a href=/posts/how-kubernetes-pods-communicate-across-nodes/>Kubernetes Pod 是如何跨节点通信的？</a>中，我们简单地介绍了 Kubernetes 中的两种 SDN 网络模型：Underlay 和 Overlay。而 Openshift 中的 SDN 则是由 Overlay 网络 OVS（Open vSwitch）实现的，其使用的插件如下：</p><ul><li>ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信；</li><li>ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 <strong>oc get netnamespaces</strong> 命令查看；</li><li>ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。</li></ul><p>OVS 在每个 Openshift 节点上都创建了如下网络接口：</p><ul><li><code>br0</code>：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离；</li><li><code>vxlan0</code>：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信；</li><li><code>tun0</code>：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信；</li><li><code>veth</code>：Pod 通过<code>veth-pair</code>连接到<code>br0</code>网桥的端点。</li></ul><p>使用 <strong>ovs-ofctl -O OpenFlow13 show br0</strong> 命令可以查看<code>br0</code>上的所有端口及其编号：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># ovs-ofctl -O OpenFlow13 show br0</span>
OFPT_FEATURES_REPLY <span class=o>(</span>OF1.3<span class=o>)</span> <span class=o>(</span><span class=nv>xid</span><span class=o>=</span>0x2<span class=o>)</span>: dpid:0000ea00372f1940
n_tables:254, n_buffers:0
capabilities: FLOW_STATS TABLE_STATS PORT_STATS GROUP_STATS QUEUE_STATS
OFPST_PORT_DESC reply <span class=o>(</span>OF1.3<span class=o>)</span> <span class=o>(</span><span class=nv>xid</span><span class=o>=</span>0x3<span class=o>)</span>:
 1<span class=o>(</span>vxlan0<span class=o>)</span>: addr:72:23:a0:a9:14:a7
     config:     <span class=m>0</span>
     state:      <span class=m>0</span>
     speed: <span class=m>0</span> Mbps now, <span class=m>0</span> Mbps max
 2<span class=o>(</span>tun0<span class=o>)</span>: addr:62:80:67:c6:38:58
     config:     <span class=m>0</span>
     state:      <span class=m>0</span>
     speed: <span class=m>0</span> Mbps now, <span class=m>0</span> Mbps max
 8381<span class=o>(</span>vethd040c191<span class=o>)</span>: addr:7a:d9:f4:12:94:5f
     config:     <span class=m>0</span>
     state:      <span class=m>0</span>
     current:    10GB-FD COPPER
     speed: <span class=m>10000</span> Mbps now, <span class=m>0</span> Mbps max
 ...
 LOCAL<span class=o>(</span>br0<span class=o>)</span>: addr:76:ab:cf:6f:e1:46
     config:     PORT_DOWN
     state:      LINK_DOWN
     speed: <span class=m>0</span> Mbps now, <span class=m>0</span> Mbps max
OFPT_GET_CONFIG_REPLY <span class=o>(</span>OF1.3<span class=o>)</span> <span class=o>(</span><span class=nv>xid</span><span class=o>=</span>0x5<span class=o>)</span>: <span class=nv>frags</span><span class=o>=</span>nx-match <span class=nv>miss_send_len</span><span class=o>=</span><span class=m>0</span>
</code></pre></td></tr></table></div></div></div><p>考虑到 Openshift 集群的复杂性，我们分别按以下几种场景分析数据包的流向：</p><ul><li>节点内 Pod 互访：Pod to Local Pod</li><li>Pod 跨节点互访：Pod to Remote Pod</li><li>Pod 访问 Service：Pod to Service</li><li>Pod 与集群外部互访：Pod to External</li></ul><p>由于高版本（3.11 以上）的 Openshift 不再以守护进程而是以 Pod 的形式部署 OVS 组件，不方便对 <a href=https://en.wikipedia.org/wiki/OpenFlow target=_blank rel=noopener>OpenFlow</a> 流表进行查看，因此本文选用的集群版本为 3.6：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># oc version </span>
oc v3.6.173.0.5
kubernetes v1.6.1+5115d708d7
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://test-cluster.ocp.koktlzz.com:8443
openshift v3.6.173.0.5
kubernetes v1.6.1+5115d708d7
</code></pre></td></tr></table></div></div></div><p>另外，实验用集群并未开启 ovs-multitenant，即未进行多租户隔离。整个集群 Pod 网络是扁平化的，所有 Pod 的 VNID 都为默认值 0。</p><h2 id=pod-to-local-pod><a href=#pod-to-local-pod class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:pod-to-local-pod class=headings>Pod to Local Pod</a></h2><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132046.jpeg alt=202205132046></p><p>数据包首先通过<code>veth-pair</code>送往 OVS 网桥<code>br0</code>，随后便进入了<code>br0</code>上的 OpenFlow 流表。我们可以用 <strong>ovs-ofctl -O OpenFlow13 dump-flows br0</strong> 命令查看流表中的规则，同时为了让输出结果更加简洁，略去 cookie 和 duration 的信息：</p><ul><li><code>table=0, n_packets=62751550874, n_bytes=25344802160312, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.130.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]->NXM_NX_REG0[],goto_table:10</code></li><li><code>table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30</code></li><li><code>table=0, n_packets=833353346930, n_bytes=329854403266173, priority=100,ip actions=goto_table:20</code></li></ul><p>table0 中关于 IP 数据包的规则主要有三条，其中前两条分别对应流入端口<code>in_port</code>为 1 号端口<code>vxlan0</code>和 2 号端口<code>tun0</code>的数据包。这两条规则的优先级<code>priority</code>都是 200，因此只有在两者均不符合情况下，才会匹配第三条规则。由于本地 Pod 发出的数据包是由<code>veth</code>端口进入的，因此将转到 table20；</p><ul><li><code>table=20, n_packets=607178746, n_bytes=218036511085, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0->NXM_NX_REG0[],goto_table:21</code></li><li><code>table=21, n_packets=833757781068, n_bytes=329871389393381, priority=0 actions=goto_table:30</code></li></ul><p>table20 会匹配源地址<code>nw_src</code>为 10.130.9.154 且流入端口<code>in_port</code>为 8422 的数据包，随后将 Pod1 的 VNID 0 作为源 VNID 存入寄存器 0 中，经由 table21 转到 table30；</p><ul><li><code>table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70</code></li><li><code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90</code></li><li><code>table=30, n_packets=21061319859, n_bytes=29568807363654, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60</code></li><li><code>table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100</code></li></ul><p>table30 中匹配数据包目的地址<code>nw_dst</code>的规则有四条，前三条分别对应本节点内 Pod 的 CIDR 网段 10.130.8.0/23、集群内 Pod 的 CIDR 网段 10.128.0.0/14 和 Service 的 ClusterIP 网段 172.30.0.0/16。第四条优先级最低，用于 Pod 对集群外部的访问。由于数据包的目的地址 10.130.9.158 符合第一条规则，且第一条规则的优先级最高，因此将转到 table70；</p><ul><li><code>table=70, n_packets=597219981, n_bytes=243824445346, priority=100,ip,nw_dst=10.130.9.158 actions=load:0->NXM_NX_REG1[],load:0x20ea->NXM_NX_REG2[],goto_table:80</code></li></ul><p>table70 匹配目的地址<code>nw_dst</code>为 Pod2 IP 10.130.9.158 的数据包，并将 Pod2 的 VNID 0 作为目的 VNID 存入寄存器 1 中。同时端口号<code>0x20ea</code>被保存到寄存器 2 中，然后转到 table80；</p><ul><li><code>table=80, n_packets=1112713040332, n_bytes=293801616636499, priority=200 actions=output:NXM_NX_REG2[]</code></li></ul><p>table80 比较寄存器 0 和寄存器 1 中保存的源/目的 VNID。若二者一致，则根据寄存器 2 中保存的端口号将数据包送出。</p><p>端口号<code>0x20ea</code>是一个十六进制数字，即十进制数 8426。而 Pod2 正是通过 8426 号端口设备<code>vethba48c6de</code>连接到<code>br0</code>上，因此数据包便最终通过它流入到了 Pod2 中。</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># ovs-ofctl -O OpenFlow13 show br0 | grep 8426</span>
 8426<span class=o>(</span>vethba48c6de<span class=o>)</span>: addr:e6:b2:7e:42:41:91
</code></pre></td></tr></table></div></div></div><h2 id=pod-to-remote-pod><a href=#pod-to-remote-pod class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:pod-to-remote-pod class=headings>Pod to Remote Pod</a></h2><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105132042.jpeg alt=202105132042></p><h3 id=packet-in-local-pod><a href=#packet-in-local-pod class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:packet-in-local-pod class=headings>Packet in Local Pod</a></h3><p>数据包依然首先通过<code>veth-pair</code>送往 OVS 网桥<code>br0</code>，随后便进入了<code>br0</code>上的 OpenFlow 流表：</p><ul><li><code>table=0, n_packets=830232155588, n_bytes=328613498734351, priority=100,ip actions=goto_table:20</code></li><li><code>table=20, n_packets=1901, n_bytes=299279, priority=100,ip,in_port=6635,nw_src=10.130.9.154 actions=load:0->NXM_NX_REG0[],goto_table:21</code></li><li><code>table=21, n_packets=834180030914, n_bytes=330064497351030, priority=0 actions=goto_table:30</code></li></ul><p>与 Pod to Local Pod 的流程一致，数据包根据规则转到 table30；</p><ul><li><code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90</code></li><li><code>table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70</code></li></ul><p>数据包的目的地址为 Pod2 IP 10.131.8.206，不属于本节点 Pod 的 CIDR 网段 10.130.8.0/23，而属于集群 Pod 的 CIDR 网段 10.128.0.0/14，因此转到 table90；</p><ul><li><code>table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]->NXM_NX_TUN_ID[0..31],set_field:10.122.28.8->tun_dst,output:1</code></li></ul><p>table90 根据目的 IP 的所属网段 10.131.8.0/23 判断其位于 Node2 上，于是将 Node2 IP 10.122.28.8 设置为<code>tun_dst</code>。并且从寄存器 0 中取出 VNID 的值，从 1 号端口<code>vxlan0</code>输出。</p><p><code>vxlan0</code>作为一个 VTEP 设备（参见 <a href=/posts/kubernetes-sdn/#overlay-network>Overlay Network</a>），将根据 table90 发来的信息，对数据包进行一层封装：</p><ul><li>目的地址（dst IP） &ndash;> <code>tun_dst</code> &ndash;> 10.122.28.8</li><li>源地址（src IP） &ndash;> Node1 IP &ndash;> 10.122.28.7</li><li>源 VNID &ndash;> <code>NXM_NX_TUN_ID[0..31]</code> &ndash;> 0</li></ul><p>由于封装后的数据包源/目的地址均为节点 IP，因此从 Node1 的网卡流出后，可以通过物理网络设备转发到 Node2 上。</p><h3 id=packet-in-remote-pod><a href=#packet-in-remote-pod class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:packet-in-remote-pod class=headings>Packet in Remote Pod</a></h3><p>Node2 上的<code>vxlan0</code>对数据包进行解封，随后从<code>br0</code>上的 1 号端口进入 OpenFlow 流表中：</p><ul><li><code>table=0, n_packets=52141153195, n_bytes=17269645342781, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.131.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]->NXM_NX_REG0[],goto_table:10</code></li></ul><p>table0 判断数据包的流入端口<code>in_port</code>、源 IP 所属网段<code>nw_src</code>和目的 IP 所属网段<code>nw_dst</code>均符合该条规则，于是保存数据包中的源 VNID 到寄存器 0 后转到 table10；</p><ul><li><code>table=10, n_packets=10147760036, n_bytes=4060517391502, priority=100,tun_src=10.122.28.7 actions=goto_table:30</code></li></ul><p>table10 确认 VxLAN 隧道的源 IP<code>tun_src</code>就是节点 Node1 的 IP 地址，于是转到 table30；</p><ul><li><code>table=30, n_packets=678759566065, n_bytes=172831151192704, priority=200,ip,nw_dst=10.131.8.0/23 actions=goto_table:70</code></li></ul><p>table30 确认数据包的目的 IP（即 Pod2 IP）存在于 Node2 中 Pod 的 CIDR 网段内，因此转到 table70；</p><ul><li><code>table=70, n_packets=193211683, n_bytes=27881218388, priority=100,ip,nw_dst=10.131.8.206 actions=load:0->NXM_NX_REG1[],load:0x220->NXM_NX_REG2[],goto_table:80</code></li></ul><p>table70 发现数据包的目的 IP 与 Pod2 IP 相符，于是将 Pod2 的 VNID 作为目的 VNID 存于寄存器 1 中，将<code>0x220</code>（十进制数 544）保存在寄存器 2 中，然后转到 table80；</p><ul><li><code>table=80, n_packets=676813794014, n_bytes=172576112594488, priority=200 actions=output:NXM_NX_REG2[]</code></li></ul><p>table80 会检查保存在寄存器 0 和寄存器 1 中的源/目的 VNID，若相等（此例中均为 0），则从 544 号端口输出。</p><p><code>br0</code>上的 544 端口对应的网络接口是<code>vethe9f523a9</code>，因此数据包便最终通过它流入到了 Pod2 中。</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node2 ~<span class=o>]</span><span class=c1># ovs-ofctl -O OpenFlow13 show br0 | grep 544</span>
 544<span class=o>(</span>vethe9f523a9<span class=o>)</span>: addr:b2:a1:61:00:dc:3b
</code></pre></td></tr></table></div></div></div><h2 id=pod-to-service><a href=#pod-to-service class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:pod-to-service class=headings>Pod to Service</a></h2><p>在本例中，Pod1 通过 Service 访问其后端的 Pod2，其 ClusterIP 为 172.30.107.57，监听的端口为 8080：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># oc get svc</span>
NAME             CLUSTER-IP      EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>    AGE
myService        172.30.107.57   &lt;none&gt;        8080/TCP   2y
</code></pre></td></tr></table></div></div></div><ul><li><code>table=30, n_packets=21065939280, n_bytes=29573447694924, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60</code></li></ul><p>数据包在送到 OpenFlow 流表 table30 前的步骤与 Pod to Local Pod 和 Pod to Remote Pod 中的情况一致，但数据包的目的地址变为了 myService 的 ClusterIP。因此将匹配<code>nw_dst</code>中的 172.30.0.0/16 网段，转到 table60；</p><ul><li><code>table=60, n_packets=0, n_bytes=0, priority=100,tcp,nw_dst=172.30.107.57,tp_dst=8080 actions=load:0->NXM_NX_REG1[],load:0x2->NXM_NX_REG2[],goto_table:80</code></li></ul><p>table60 匹配目的地址<code>nw_dst</code>为 172.30.107.57 且目的端口为 8080 的数据包，并将 Pod1 的 VNID 0 保存到寄存器 1 中，将<code>0x2</code>（十进制数字 2）保存到寄存器 2 中，转到 table80；</p><ul><li><code>table=80, n_packets=1113435014018, n_bytes=294106102133061, priority=200 actions=output:NXM_NX_REG2[]</code></li></ul><p>table80 首先检查目的 Service 的 VNID 是否与寄存器 1 中的 VNID 一致，然后根据寄存器 2 中的数字将数据包从 2 号端口<code>tun0</code>送出，最后进入节点的 iptables 规则中。</p><p>iptables 对数据包的处理流程如下图所示：</p><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/20210516142844.png alt=20210516142844></p><p>由于 Service 的实现依赖于 NAT（上图中的紫色方框），因此我们可以在 NAT 表中查看到与之相关的规则：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># iptables -t nat -nvL</span>
Chain OUTPUT <span class=o>(</span>policy ACCEPT <span class=m>4753</span> packets, 489K bytes<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
2702M  274G KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */

Chain KUBE-SERVICES <span class=o>(</span><span class=m>2</span> references<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination
    <span class=m>4</span>   <span class=m>240</span> KUBE-SVC-QYWOVDCBPMWAGC37  tcp  --  *      *       0.0.0.0/0            172.30.107.57        /* demo/myService:8080-8080 cluster IP */ tcp dpt:8080
</code></pre></td></tr></table></div></div></div><p>本机产生的数据包（Locally-generated Packet）首先进入<code>OUTPUT</code>链，然后匹配到自定义链<code>KUBE-SERVICES</code>。由于其目的地址为 Service 的 ClusterIP 172.30.107.57，因此将再次跳转到对应的<code>KUBE-SVC-QYWOVDCBPMWAGC37</code>链：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>Chain KUBE-SVC-QYWOVDCBPMWAGC37 <span class=o>(</span><span class=m>1</span> references<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
    <span class=m>1</span>    <span class=m>60</span> KUBE-SEP-AF5DIL6JV3XLLV6G  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */ statistic mode random probability 0.50000000000
    <span class=m>1</span>    <span class=m>60</span> KUBE-SEP-ADAJHSV7RYS5DUBX  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */

Chain KUBE-SEP-ADAJHSV7RYS5DUBX <span class=o>(</span><span class=m>1</span> references<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
    <span class=m>0</span>     <span class=m>0</span> KUBE-MARK-MASQ  all  --  *      *       10.131.8.206         0.0.0.0/0            /* demo/myService:8080-8080 */
    <span class=m>0</span>     <span class=m>0</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */ tcp to:10.131.8.206:8080

Chain KUBE-SEP-AF5DIL6JV3XLLV6G <span class=o>(</span><span class=m>1</span> references<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
    <span class=m>0</span>     <span class=m>0</span> KUBE-MARK-MASQ  all  --  *      *       10.128.10.57         0.0.0.0/0            /* demo/myService:8080-8080 */
   <span class=m>23</span>  <span class=m>1380</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */ tcp to:10.128.10.57:8080
</code></pre></td></tr></table></div></div></div><p><code>KUBE-SVC-QYWOVDCBPMWAGC37</code>链下有两条完全相同的匹配规则，对应了该 Service 后端的两个 Pod。<code>KUBE-SEP-ADAJHSV7RYS5DUBX</code>链和 <code>KUBE-SEP-AF5DIL6JV3XLLV6G</code>链能够执行 DNAT 操作，分别将数据包的目的地址转化为 Pod IP 10.131.8.206 和 10.128.10.57。在一次通信中只会有一条链生效，这体现了 Service 的负载均衡能力。</p><p>完成<code>OUTPUT</code>DNAT 的数据包将进入节点的路由判断（Routing Decision）。由于当前目的地址已经属于集群内 Pod 的 CIDR 网段 10.128.0.0/14，因此将再次从<code>tun0</code>端口再次进入 OVS 网桥<code>br0</code>中。</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>rootnode1 ~<span class=o>]</span><span class=c1># route -n</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.122.28.1     0.0.0.0         UG    <span class=m>0</span>      <span class=m>0</span>        <span class=m>0</span> eth0
10.122.28.0     0.0.0.0         255.255.255.128 U     <span class=m>0</span>      <span class=m>0</span>        <span class=m>0</span> eth0
10.128.0.0      0.0.0.0         255.252.0.0     U     <span class=m>0</span>      <span class=m>0</span>        <span class=m>0</span> tun0
169.254.0.0     0.0.0.0         255.255.0.0     U     <span class=m>1008</span>   <span class=m>0</span>        <span class=m>0</span> eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     <span class=m>0</span>      <span class=m>0</span>        <span class=m>0</span> docker0
172.30.0.0      0.0.0.0         255.255.0.0     U     <span class=m>0</span>      <span class=m>0</span>        <span class=m>0</span> tun0
</code></pre></td></tr></table></div></div></div><p>不过数据包在进入<code>br0</code>之前，还需要经过 iptables 中的<code>POSTROUTING</code>链，完成一次 MASQUERADE 操作：数据包的源地址转换为其流出端口的 IP，即<code>tun0</code>的 IP 10.130.8.1。</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># iptables -t nat -nvL </span>
Chain POSTROUTING <span class=o>(</span>policy ACCEPT <span class=m>5083</span> packets, 524K bytes<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination                
2925M  288G OPENSHIFT-MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* rules <span class=k>for</span> masquerading OpenShift traffic */

Chain OPENSHIFT-MASQUERADE <span class=o>(</span><span class=m>1</span> references<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
 321M   19G MASQUERADE  all  --  *      *       10.128.0.0/14        0.0.0.0/0            /* masquerade pod-to-service and pod-to-external traffic */
<span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># ip a | grep tun0</span>
16: tun0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span class=m>1450</span> qdisc noqueue state UNKNOWN qlen <span class=m>1000</span>
    inet 10.130.8.1/23 scope global tun0
</code></pre></td></tr></table></div></div></div><p>本例中 Service 的后端 Pod 均在 Pod1 所在的节点外，因此数据包第二次进入 OpenFlow 流表时匹配的规则基本与 Pod to Remote Pod 一致：</p><ul><li><code>table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30</code></li><li><code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90</code></li><li><code>table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]->NXM_NX_TUN_ID[0..31],set_field:10.122.28.8->tun_dst,output:1</code></li></ul><p>其传递流程如下图所示：</p><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132044.jpeg alt=202205132044></p><p>Pod2 返回的数据包在到达 Node1 后将被<code>vxlan0</code>解封装，然后根据其目的地址<code>tun0</code>进入 OpenFlow 流表：</p><ul><li><code>table=0, n_packets=1084362760247, n_bytes=297224518823222, priority=200,ip,in_port=2 actions=goto_table:30</code></li><li><code>table=30, n_packets=20784385211, n_bytes=4742514750371, priority=300,ip,nw_dst=10.130.8.1 actions=output:2</code></li></ul><p>数据包从 2 号端口<code>tun0</code>流出后进入节点的 iptables 规则，随后将触发 iptables 的 <a href=https://superuser.com/questions/1269859/linux-netfilter-how-does-connection-tracking-track-connections-changed-by-nat target=_blank rel=noopener>Connection Tracking</a> 操作：根据 /proc/net/nf_conntrack 文件中的记录进行“DeNAT”。返回数据包的源/目的地址从 Pod2 IP 10.131.8.206 和 tun0 IP 10.130.8.1，变回 Service 的 ClusterIP 172.30.107.57 和 Pod1 IP 10.130.9.154。</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=o>[</span>root@node1 ~<span class=o>]</span><span class=c1># cat /proc/net/nf_conntrack | grep -E &#34;src=10.130.9.154.*dst=172.30.107.57.*dport=8080.*src=10.131.8.206&#34;</span>
ipv4     <span class=m>2</span> tcp      <span class=m>6</span> <span class=m>431986</span> ESTABLISHED <span class=nv>src</span><span class=o>=</span>10.130.9.154 <span class=nv>dst</span><span class=o>=</span>172.30.107.57 <span class=nv>sport</span><span class=o>=</span><span class=m>80</span> <span class=nv>dport</span><span class=o>=</span><span class=m>8080</span> <span class=nv>src</span><span class=o>=</span>10.131.8.206 <span class=nv>dst</span><span class=o>=</span>10.130.8.1 <span class=nv>sport</span><span class=o>=</span><span class=m>8080</span> <span class=nv>dport</span><span class=o>=</span><span class=m>80</span> <span class=o>[</span>ASSURED<span class=o>]</span> <span class=nv>mark</span><span class=o>=</span><span class=m>0</span> <span class=nv>secctx</span><span class=o>=</span>system_u:object_r:unlabeled_t:s0 <span class=nv>zone</span><span class=o>=</span><span class=m>0</span> <span class=nv>use</span><span class=o>=</span><span class=m>2</span>
</code></pre></td></tr></table></div></div></div><h2 id=pod-to-external><a href=#pod-to-external class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:pod-to-external class=headings>Pod to External</a></h2><p>数据包依然首先通过<code>veth-pair</code>送往 OVS 网桥<code>br0</code>，随后便进入了<code>br0</code>上的 OpenFlow 流表：</p><ul><li><code>table=0, n_packets=837268653828, n_bytes=331648403594327, priority=100,ip actions=goto_table:20</code></li><li><code>table=20, n_packets=613807687, n_bytes=220557571042, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0->NXM_NX_REG0[],goto_table:21</code></li><li><code>table=21, n_packets=837674296060, n_bytes=331665441915651, priority=0 actions=goto_table:30</code></li><li><code>table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100</code></li><li><code>table=100, n_packets=761732023982, n_bytes=282091648536325, priority=0 actions=output:2</code></li></ul><p>数据包从<code>tun0</code>端口流出后进入节点的路由表及 iptables 规则：</p><div class=highlight><div class=chroma><div class=table-container><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash>Chain POSTROUTING <span class=o>(</span>policy ACCEPT <span class=m>2910</span> packets, 299K bytes<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
    <span class=m>0</span>     <span class=m>0</span> MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0           
2940M  289G OPENSHIFT-MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* rules <span class=k>for</span> masquerading OpenShift traffic */

Chain OPENSHIFT-MASQUERADE <span class=o>(</span><span class=m>1</span> references<span class=o>)</span>
 pkts bytes target     prot opt in     out     <span class=nb>source</span>               destination         
 322M   19G MASQUERADE  all  --  *      *       10.128.0.0/14        0.0.0.0/0            /* masquerade pod-to-service and pod-to-external traffic */
</code></pre></td></tr></table></div></div></div><p>访问集群外部显然需要通过节点的默认网关，因此数据包将从节点网卡<code>eth0</code>送出。而在<code>POSTROUTING</code>链中，数据包的源地址由 Pod IP 转换为了<code>eth0</code>的 IP 10.122.28.7。完整流程如下图所示（图中的 Router 指的是路由器而非 Openshift 中的概念）：</p><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132045.jpeg alt=202205132045></p><h2 id=future-work><a href=#future-work class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:future-work class=headings>Future Work</a></h2><ul><li>本文并未涉及 External to Pod 的场景，它是如何实现的？我们都知道 Openshift 是通过 Router（HAProxy）来暴露集群内部服务的，那么数据包在传输过程中的 NAT 操作是怎样进行的？</li><li>除了本文提到的几种网络接口外，Openshift 节点上还存在着<code>ovs-system</code>和<code>vxlan_sys_4789</code>。它们的作用是什么？</li><li>Openshift 4.X 版本的网络模型与本文实验用的 3.6 版本相比有那些变化？</li></ul><h2 id=参考文献><a href=#参考文献 class=anchor-link><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon anchor-icon"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96.0-59.27-59.26-59.27-155.7.0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757.0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037.0 01-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482.0 0120.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96.0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454.0 0020.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037.0 00-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639.0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"/></svg></a><a href=#contents:参考文献 class=headings>参考文献</a></h2><p><a href=https://en.wikipedia.org/wiki/OpenFlow target=_blank rel=noopener>OpenFlow - Wikipedia</a></p><p><a href=https://medoc.readthedocs.io/en/latest/docs/ovs/sharing/cloud_usage.html target=_blank rel=noopener>OVS 在云项目中的使用</a></p><p><a href=https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html target=_blank rel=noopener>OpenShift SDN - OpenShift Container Platform 3.11</a></p><p><a href=https://www.cnblogs.com/sammyliu/p/10064450.html target=_blank rel=noopener>理解 OpenShift（3）：网络之 SDN</a></p><p><a href=https://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/ target=_blank rel=noopener>[译] 深入理解 iptables 和 netfilter 架构</a></p><p><a href=https://superuser.com/questions/1269859/linux-netfilter-how-does-connection-tracking-track-connections-changed-by-nat target=_blank rel=noopener>Linux Netfilter: How does connection tracking track connections changed by NAT?</a></p></div><p class=edit-page><a href=https://github.com/koktlzz/koktlzz.github.io/blob/main/content/posts/explorations-on-the-openshift-sdn-network-model.md><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-edit-2"><path d="M17 3a2.828 2.828.0 114 4L7.5 20.5 2 22l1.5-5.5L17 3z"/></svg>Edit this page on GitHub</a></p></article><div id=utterances></div></div></main><div id=back-to-top class=back-to-top><a href=#><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon arrow-up"><path d="M34.9 289.5l-22.2-22.2c-9.4-9.4-9.4-24.6.0-33.9L207 39c9.4-9.4 24.6-9.4 33.9.0l194.3 194.3c9.4 9.4 9.4 24.6.0 33.9L413 289.4c-9.5 9.5-25 9.3-34.3-.4L264 168.6V456c0 13.3-10.7 24-24 24h-32c-13.3.0-24-10.7-24-24V168.6L69.2 289.1c-9.3 9.8-24.8 10-34.3.4z"/></svg></a></div><footer id=footer class=footer><div class=footer-inner><div class=site-info>©&nbsp;2020–2021&nbsp;<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon footer-icon"><path d="M462.3 62.6C407.5 15.9 326 24.3 275.7 76.2L256 96.5l-19.7-20.3C186.1 24.3 104.5 15.9 49.7 62.6c-62.8 53.6-66.1 149.8-9.9 207.9l193.5 199.8c12.5 12.9 32.8 12.9 45.3.0l193.5-199.8c56.3-58.1 53-154.3-9.8-207.9z"/></svg>&nbsp;koktlzz</div><div class=powered-by>Powered by <a href=https://github.com/gohugoio/hugo target=_blank rel=noopener>Hugo</a> | Theme is <a href=https://github.com/reuixiy/hugo-theme-meme target=_blank rel=noopener>MemE</a></div><div class=site-copyright><a href=https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en target=_blank rel=noopener>CC BY-NC-SA 4.0</a></div><ul class=socials><li class=socials-item><a href=/rss.xml target=_blank rel="external noopener" title=RSS><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M19.199 24C19.199 13.467 10.533 4.8.0 4.8V0c13.165.0 24 10.835 24 24h-4.801zM3.291 17.415c1.814.0 3.293 1.479 3.293 3.295.0 1.813-1.485 3.29-3.301 3.29C1.47 24 0 22.526.0 20.71s1.475-3.294 3.291-3.295zM15.909 24h-4.665c0-6.169-5.075-11.245-11.244-11.245V8.09c8.727.0 15.909 7.184 15.909 15.91z"/></svg></a></li><li class=socials-item><a href=mailto:gwrhnu@163.com target=_blank rel="external noopener" title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon social-icon"><path d="M464 64H48C21.49 64 0 85.49.0 112v288c0 26.51 21.49 48 48 48h416c26.51.0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 4e2V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V4e2H48z"/></svg></a></li><li class=socials-item><a href=https://github.com/koktlzz target=_blank rel="external noopener" title=GitHub><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" class="icon social-icon"><path d="M12 .297c-6.63.0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577.0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93.0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176.0.0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22.0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22.0 1.606-.015 2.896-.015 3.286.0.315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg></a></li></ul></div></footer></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.css integrity="sha256-gPJfuwTULrEAAcI3X4bALVU/2qBU+QY/TpoD3GO+Exw=" crossorigin=anonymous><script>if(typeof renderMathInElement=='undefined'){var getScript=b=>{var a=document.createElement('script');a.defer=!0,a.crossOrigin='anonymous',Object.keys(b).forEach(c=>{a[c]=b[c]}),document.body.appendChild(a)};getScript({src:'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/katex.min.js',integrity:'sha256-YTW9cMncW/ZQMhY69KaUxIa2cPTxV87Uh627Gf5ODUw=',onload:()=>{getScript({src:'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/mhchem.min.js',integrity:'sha256-yzSfYeVsWJ1x+2g8CYHsB/Mn7PcSp8122k5BM4T3Vxw=',onload:()=>{getScript({src:'https://cdn.jsdelivr.net/npm/katex@0.13.0/dist/contrib/auto-render.min.js',integrity:'sha256-fxJzNV6hpc8tgW8tF0zVobKa71eTCRGTgxFXt1ZpJNM=',onload:()=>{renderKaTex()}})}})}})}else renderKaTex();function renderKaTex(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\[",right:"\\]",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1}]})}</script><script>function loadComments(){(function(){var b=document.getElementById("utterances"),a=document.createElement('script');a.src='https://utteranc.es/client.js',a.async=!0,a.crossOrigin='anonymous',a.setAttribute('repo','koktlzz/koktlzz.github.io'),a.setAttribute('issue-term','pathname'),a.setAttribute('theme','github-light'),a.setAttribute('label','comment'),b.appendChild(a)})()}</script><script src=https://cdn.jsdelivr.net/npm/medium-zoom@latest/dist/medium-zoom.min.js></script>
<script>let imgNodes=document.querySelectorAll('div.post-body img');imgNodes=Array.from(imgNodes).filter(a=>a.parentNode.tagName!=="A"),mediumZoom(imgNodes,{background:'hsla(var(--color-bg-h), var(--color-bg-s), var(--color-bg-l), 0.95)'})</script><script src=https://cdn.jsdelivr.net/npm/instant.page@5.1.0/instantpage.min.js type=module defer></script></body></html>