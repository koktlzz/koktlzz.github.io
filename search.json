[{"content":"链接（Linking）是将各部分代码和数据收集并组成单个文件的过程，该文件可以被加载（复制）到内存中执行。链接可以在编译时（即源代码被翻译成机器代码时）执行，也可以在加载时（即程序被加载到内存并由加载器执行时）执行，甚至还可以在运行时通过应用程序执行。链接是由称为链接器（Linker）的程序自动执行的。\n链接器支持单独编译（Separate Compilation），因此在软件开发中起着至关重要的作用。与其将大型应用程序组织为一个单一的大块源文件，不如将其分解为一些更小、更易于管理并且可以单独修改和编译的模块。如果我们修改了其中某个模块，只需重新对其编译并链接应用程序，而无需重新编译其他文件。\n编译器驱动 大多数编译系统都提供了一个编译器驱动（Compiler Driver），它根据用户需求调用语言预处理器、编译器、汇编器和链接器等。例如要在 GNU 编译系统中构建下列程序，我们可以使用命令gcc -Og -o prog main.c sum.c:\n编译器驱动将示例程序从 ASCII 源文件转换为可执行目标文件时的过程如下图所示：\n首先，驱动运行 C 预处理器（C Preprocessor，CPP），将源文件main.c转换为 ASCII 中间文件main.i；接下来，驱动运行 C 编译器 (C Compiler，CC)，将main.i转换为 ASCII 汇编语言文件main.s；然后，驱动运行汇编器，将main.s转换为二进制可重定位（Relocatable）目标文件main.o（sum.o的生成过程相同）；最后，驱动运行链接器，将main.o、sum.o和一些必要的系统目标文件组合，创建二进制可执行目标文件prog。\n静态链接 静态链接器（Static Linker）将可重定位目标文件和命令行参数作为输入，生成完全链接的可执行目标文件。作为输入的可重定位目标文件由各种代码和数据组成，其中每个部分都是一个连续的字节序列。指令、初始化的全局变量和未初始化的变量分别处于不同部分。\n链接器需要完成两个主要任务：\n 符号解析（Symbol Resolution）：目标文件定义并引用符号，每个符号对应一个函数、全局变量或静态变量（即使用static声明的任何变量）。符号解析的目的是将每个符号引用与一个符号定义相关联； 重定位（Relocation）：编译器和汇编器生成的代码和数据段是从地址 0 开始的，而链接器会将一个内存位置与每个符号定义相关联，从而重定位这些段。它随后修改所有的符号引用，使其指向该内存位置。链接器使用由汇编器生成的详细指令（称为 重定位条目）盲目地执行重定位操作。  请明确：目标文件只是字节块的集合。这些块中可能包含代码，也可能包含数据，还有可能包含指导链接器和加载器的数据结构。链接器将它们连接在一起，确定整体的运行时位置，并修改代码和数据块中的各个位置。\n目标文件 目标文件（Object File）有三种形式：\n 可重定位目标文件：包含二进制代码和数据，可以在编译时与其他可重定位目标文件组合以创建可执行目标文件； 可执行目标文件：包含二进制代码和数据形式，可直接被复制到内存中执行； 共享目标文件：一种特殊类型的可重定位目标文件，可以在加载时或运行时加载到内存中并动态链接。  可重定位目标文件 典型的 ELF（Executable and Linkable Format）可重定位目标文件格式如下图所示：\n ELF 头（ELF Header）：开头是一个表征系统字长（Word Size）和字节顺序（Byte Ordering）的 16 字节序列。其余部分包括 ELF 头的大小、目标文件的类型（如可重定位、可执行或共享）、机器类型（如 x86-64）、节头表（Section Header Table）的文件偏移量以及其中的条目； 节头表：描述了目标文件中每个 Section 的位置和大小； Section：位于 ELF 头和节头表之间，包括：  .text：编译后程序的机器码； .rodata：只读数据，例如printf语句中的格式字符串，以及switch语句的跳转表； .data：已初始化的全局变量和静态变量。非静态局部变量在运行时位于栈中，不会出现在 .data 或 .bss 中； .bss：未初始化的静态变量，以及初始化为 0 的全局变量和静态变量。此 Section 只是一个占位符，在目标文件中不占用实际空间，因此可以提升空间效率。这些变量在运行时被分配到内存中，初始值为零； .symtab ：包含在程序中定义和引用的函数和全局变量信息的符号表（Symbol Table）。与编译器中的符号表不同，.symtab 中的符号表不包含任何局部变量； .rel.text：当链接器将此目标文件与其他文件组合时，.text 中的许多位置都需要被修改。通常，任何调用外部函数或引用全局变量的指令都需要被修改，而调用局部函数的指令则不变。可执行目标文件一般不需要重定位信息，因此可以省略； .rel.data：被引用或定义的任何全局变量的重定位信息。通常，所有初始值为全局变量地址或外部定义函数地址的已初始化全局变量都需要修改； .debug：调试符号表，仅在使用-g选项调用编译器驱动时出现； .line：原始程序中行号与 .text 中机器代码指令之间的映射，仅在使用-g选项调用编译器驱动时出现； .strtab：一个以 Null 结尾，包含 .symtab 和 .debug 中的符号表以及 Section 名称的字符串序列。    符号和符号表 每个目标文件都有一个符号表，其中包含了该文件所定义和引用的符号信息。符号有以下三种：\n 全局符号（Global Symbols）：由该文件定义且可以被其他文件引用的符号； 外部符号（Externals）：被该文件引用但由其他文件定义的符号； 局部符号（Local Symbols）：由该文件定义且无法被其他文件引用的符号，即使用static声明的函数和变量。  上节提到，非静态局部变量在运行时位于栈中，与链接器无关。而静态局部变量则保存在 .data 或 .bss 中，编译器会在符号表中为其创建名称唯一的局部符号。例如同一文件中的两个函数都定义了静态局部变量x：\n1 2 3 4 5 6 7 8 9 10  int f() { static int x = 0; return x; } int g() { static int x = 1; return x; }   则编译器可能将x.1作为函数f()中的变量符号，将x.2作为函数g()中的变量符号，然后发送给汇编器。汇编器使用接收到的 .s 文件中的符号构建符号表。ELF 符号表中每个条目的数据结构为：\n name：符号名在字符串表 .strtab 中的偏移量； value：对于可重定位目标文件是符号在其 Section 中的偏移量，对于可执行目标文件是符号的运行时地址； size：符号的大小； type：符号的类型； binding：符号是局部的还是全局的； section：符号所在的 Section 在节头表中的索引。  值得一提的是，有三个伪 Section 在节头表中没有条目：\n ABS：不应重定位的符号； UNDEF：在此文件中引用但在其他文件中定义的符号； COMMON：未初始化的全局符号。  上述三个 Section 仅存在于可重定位目标文件，而在可执行目标文件中并不存在。我们可以使用 READELF 工具阅读目标文件中的内容，示例程序 main.c 生成的目标文件符号表条目如下：\nREADELF 通过整数索引 Ndx 标识每个 Section，1 表示 .text，3 表示 .data。全局符号main和array分别位于两个 Section 的首部，因此偏移量value均为 0。外部符号sum未在本文件中定义，因此位于 UNDEF。\n符号解析 链接器通过将每个符号引用与符号表中的符号定义相关联来完成符号解析（Symbol Resolution）。当编译器遇到未在当前文件中定义的符号时，它会假设该符号已在其他文件中定义，然后生成对应的符号表条目。如果链接器无法在任何输入文件中找到该符号的定义，那么它就会终止链接。\n不同文件可能定义了相同名称的全局符号。对于这种情况，链接器要么直接报错，要么选取其中之一。\n C++ 和 Java 允许重载名称相同但参数列表不同的方法。编译器会将每个方法和参数列表组合为一个唯一的名称，这样链接器就可以区分它们。例如，Foo::bar(int, long)会被编码为bar__3Fooil。其中，3 代表类名 Foo 的字符数，i 和 l 则分别代表参数列表中的int和long。\n 解析名称重复的符号 Linux 编译系统会在编译时将全局符号分为两种类型：函数和已初始化的全局变量是强符号，未初始化的全局变量是弱符号。汇编器将符号的强弱信息隐式地编码到目标文件的符号表中。\n链接器解析名称重复的符号的规则为：\n 不允许多个强符号名称重复； 若一个强符号和多个弱符号名称重复，选择强符号； 若多个弱符号名称重复，从中任选其一。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  /* foo3.c */ #include \u003cstdio.h\u003evoid f(void); int x = 15213; int main() { f(); printf(\"x = %d\\n\", x); return 0; } /* bar3.c */ int x; void f() { x = 15212; }   文件bar3.c中的函数f是强符号，文件foo3.c中的函数f是弱符号，因此主函数的输出x的值为 15212。\n上文提到，未初始化的全局变量保存在 COMMON 中，而未初始化的静态变量，以及初始化为 0 的全局变量和静态变量保存在 .bss 中。这是因为前者是弱符号，编译器无法知晓其他文件中是否定义了相同名称的变量，必须将其分配到 COMMON 中并随后由链接器处理。已初始化为 0 的全局变量是强符号，根据第二条解析规则，该变量一定是唯一的，因此编译器可以安心地把它放到 .bss 中。静态变量无法被其他文件引用，自然也无需担心名称重复的问题。\n与静态库链接 编译系统将一些相关的目标模块打包到一个文件中，该文件称为静态库（Static Library）。在构建可执行目标文件时，链接器仅复制静态库中被应用程序引用的目标模块，从而减小了磁盘和内存中可执行文件的大小。静态库为我们提供了 I/O、字符串操作和数学运算等标准函数。\n在 Linux 系统中，静态库以特定的文件格式（后缀为 .a）存储在磁盘上。应用程序可以通过在命令行中指定文件名来使用静态库中定义的任何函数（实际上，C 编译器驱动总是将libc.a传递给链接器）：\n1  linux\u003e gcc main.c /usr/lib/libm.a /usr/lib/libc.a   我们可以使用 AR 命令将下列程序打包为静态库文件libvector.a：\n1 2  linux\u003e gcc -c addvec.c multvec.c linux\u003e ar rcs libvector.a addvec.o multvec.o   程序main2.c在头文件vector.h中定义了该库文件中的函数原型：\n我们使用如下命令编译并链接main2.o和libvector.a：\n1 2  linux\u003e gcc -c main2.c linux\u003e gcc -static -o prog2c main2.o ./libvector.a   -static参数表示链接器应当构建一个完全链接的可执行目标文件，该文件可以被加载到内存中运行而无需进一步地链接。完整的链接流程如下图所示：\n静态库的符号解析 符号解析时，链接器会按照从左到右的顺序依次扫描命令行中的目标文件和静态库。在这个过程中，E 为可重定位目标文件的集合，U 为被引用但还未找到定义的符号，D 为已扫描过的文件定义的符号。开始时三者均为空。\n 若命令行中的输入文件为可重定位目标文件，则链接器将其添加到 E 中并更新 U 和 D 中的符号； 若命令行中的输入文件为静态库，则链接器会将 U 中的符号与该静态库中定义的符号相匹配。匹配成功的模块会被添加到 E 中，随后链接器更新 U 和 D 中的符号。当 U 和 D 中的符号不再改变时，匹配结束，任何不在 E 中的静态库模块都将被直接丢弃； 若扫描全部完成时 U 为空，则链接器合并并重定位 E 中所有的目标文件以构建可执行文件。否则，链接器将报错并终止。  链接器的这种行为限制了命令行中的文件顺序。如果定义符号的静态库出现在引用该符号的目标文件之前，链接就会失败。\n重定位 符号解析完成后，链接器会将代码中的每个符号引用与一个符号定义相关联。接下来，链接器将开始对目标文件重定位：\n 重定位 Section 和符号定义：链接器将所有输入模块中相同类型的 Section 合并为一个新的聚合 Section，然后将运行时地址分配给每个 Section 和符号； 在 Section 内重定位符号引用：链接器修改代码和数据段中的每个符号引用，使其指向正确的运行时地址。  重定位条目 汇编器在生成目标文件时，并不知晓代码、数据和引用的外部符号在内存中的最终位置。它只会为每个引用生成一个重定位条目（Relocation Entry），指导链接器如何修改它们。上文提到，代码的重定位条目放在 .rel.text 中，数据的重定位条目则放在 .rel.data 中。\nELF 重定位条目的数据结构为：\n1 2 3 4 5 6  typedef struct { long offset; /* Offset of the reference to relocate */ long type:32, /* Relocation type */ symbol:32; /* Symbol table index */ long addend; /* Constant part of relocation expression */ } Elf64_Rela;   offset是需要被修改的引用在其所在 Section 的偏移量；symbol是被修改的引用指向的符号；type告知链接器如何修改引用；addend是一个有符号常量，某些类型的重定位使用它来偏置被修改的引用的值。\n最基本的两种重定位类型为：\n R_X86_64_PC32：使用 32 位 PC 相对地址重定位引用。当 CPU 执行一条使用 PC 相对地址的指令时，它会将指令中的目标地址与 PC 当前值（即下一条指令在内存中的地址）相加得到有效地址（在 跳转指令 一节中我们讨论过这一问题）； R_X86_64_32：使用 32 位绝对地址重定位引用。CPU 直接使用指令中的目标地址作为有效地址，无需进一步地修改。  重定位符号引用 重定位算法的伪码如下图所示：\n链接器遍历每个 Section（s）中的每个重定位条目（r），s是一个字节数组，r是上一节介绍的 Elf64_Rela 类型的结构体。假设该算法运行时，链接器已经为每个 Section 和每个符号选择了运行时地址 ADDR(s)和ADDR(r.symbol)。\n链接器使用此算法对 示例程序 进行重定位引用的结果如下（objdump -dx main.o）。重定位条目（图中第 5 行和第 7 行）告知链接器对符号array的引用使用绝对地址重定位，而对符号sum()的引用则使用 PC 相对地址重定位：\nPC 相对地址重定位 如上图第 6 行所示：指令callq在其所在 Section 中的偏移量为 0xe，它包含一个一字节的指令码 0xe8 和一个用于指向sum()的 32 位 PC 相对引用的占位符。该引用对应的重定位条目为：\n1 2 3 4  r.offset = 0xf r.symbol = sum r.type = R_X86_64_PC32 r.addend = -4   上述字段告诉链接器需要修改从偏移量 0xf 开始的 32 位 PC 相对引用，使其在运行时指向sum()。 假设ADDR(s) = ADDR(.text) = 0x4004d0，ADDR(r.symbol) = ADDR(sum) = 0x4004e8，那么首先我们可以计算得到该引用的运行时地址为：\n1 2 3  refaddr = ADDR(s) + r.offset = 0x4004d0 + 0xf = 0x4004df   然后根据上节中的算法更新引用使其指向sum()：\n1 2 3  *refptr = (unsigned) (ADDR(r.symbol) + r.addend - refaddr) = (unsigned) (0x4004e8 + (-4) - 0x4004df) = (unsigned) (0x5)   指令callq的运行时地址为 0x4004de（refaddr -1）。当 CPU 执行该指令时，PC 中的值为该指令的下一条指令的地址 0x4004e3（refaddr + 4）。CPU 将该值压入栈中，然后加上 0x5（即*refptr），就得到了sum()的地址 0x4004e8。\n绝对地址重定位 如上图中的第四行所示：指令mov将数组地址拷贝到寄存器 %edi 中，它在 Section 中的偏移量为 0x9，包含一个一字节的指令码 0xbf 和一个用于指向array的 32 位绝对引用的占位符。该引用的重定位条目为：\n1 2 3 4  r.offset = 0xa r.symbol = array r.type = R_X86_64_32 r.addend = 0   上述字段告诉链接器需要修改从偏移量 0xa 开始的 32 位 PC 绝对引用，使其在运行时指向array。 假设ADDR(r.symbol) = ADDR(array) = 0x601018，那么我们可以根据上节中的算法更新该引用为：\n1 2 3  *refptr = (unsigned) (ADDR(r.symbol) + r.addend) = (unsigned) (0x601018 + 0) = (unsigned) (0x601018)   下图展示了最终生成的可执行目标文件中的 .text 和 .data：\n加载器可以在加载时将这些 Section 中的字节直接拷贝到内存中，无需任何修改就可以运行其中的指令。\n可执行目标文件 ELF 可执行目标文件的结构如下：\nELF 头描述了文件的整体格式，并且包含了程序在运行时要执行的第一条指令的地址。.init 定义了一个名为_init的函数，它将被程序的初始化代码所调用。其余 Section 与可重定位目标文件类似，只不过它们已被重定位到运行时的内存地址。正因如此，该文件中没有 .rel.text 和 .rel.data。\n可执行目标文件的连续块与连续内存段之间存在映射关系，程序头表（Program Header Table）对此进行了描述：\n第一个内存段具有读取和执行权限，从内存地址 0x40000 开始，大小为 0x69c 个字节。该内存段是由可执行目标文件的前 0x69c 个字节（偏移量为 0）初始化得到的，包含了 ELF 头、程序头表、.init、.text 和 .rodata。\n第二个内存段具有读取和写入权限，从内存地址 0x600df8 开始，大小为 0x230 个字节。该内存段对应了可执行目标文件中偏移量为 0xdf8 的 0x228 个字节，包含了 .data 和 .bss（两者大小之差的 8 个字节即保存在 .bss 并将在运行时初始化为 0 的数据）。\n对于每个内存段，链接器必须选择一个起始地址 vaddr，使得：\n$$vaddr\\space mod\\space align = off\\space mod\\space align$$\n其中，off是该内存段中第一个 Section 在目标文件中的偏移量，align是程序头中指定的对齐方式。这种对齐要求是一种优化，它可以使目标文件更加有效地加载到内存中，原因请见第九章虚拟内存。\n加载可执行目标文件 下图展示了 Linux 程序的运行时内存结构：\n加载器首先根据程序头表，将可执行目标文件中的块复制到代码和数据段中。接下来跳转到程序程序入口，即_start_函数（在系统目标文件crt1.o中定义）的地址。该函数随后调用libc.so中定义的系统启动函数__libc_start_main。最后由它初始化执行环境，调用用户级的主函数并处理其返回。\n使用共享库动态链接 静态库需要定期维护和更新，因此程序员需要知晓其变动并将程序与其重新链接。另外，几乎每个 C 程序都会使用一些标准 I/O 函数，例如printf。这些函数的代码会在运行时复制到每个进程的代码段中，从而导致严重的内存浪费。\n共享库（Shared Libraries）可以解决上述静态库的缺点。它是一种可以在加载时或运行时在任意内存地址加载并与程序链接的目标模块，该过程称为动态链接（Dynamic Linking）。共享库在 Linux 系统中以 .so 为后缀，而在 Windows 系统中则被称为 DLL（Dynamic Linking Libraries）。\n在任意文件系统中，每个共享库都只有一个 .so 文件。与静态库不同的是，该文件中的代码和数据可以被引用该库的所有可执行文件共享，而不需要复制到可执行文件中。示例程序 的动态链接过程如下图所示：\n我们使用如下指令将 addvec.c 和 multvec.c 构建为共享库文件libvector.so:\n1  linux\u003e gcc -shared -fpic -o libvector.so addvec.c multvec.c   其中，-fpic指示编译器生成 与位置无关代码（Position-Independent Code），而-shared则指示链接器创建共享目标文件。一旦共享库文件创建成功，我们就可以将其链接到示例程序中：\n1  inux\u003e gcc -o prog2l main2.c ./libvector.so   我们需要明确的是，libvector.so中的任意代码和数据都没有被复制到可执行文件prog2l中。链接器只会复制一些重定位和符号表信息，它们将在加载时用于解析引用了共享库的符号。\n加载器随后读取可执行文件中包含的动态链接器路径，加载并运行它。动态链接器也是一个共享库文件，如 Linux 系统的 ld-linux.so。它通过执行以下重定位操作来完成链接：\n 将libc.so的代码和数据重定位到某个内存段； 将libvector.so中的代码和数据重定位到另一个内存段； 将prog2l中所有引用了共享库的符号重定位。  最终，动态链接器将控制权转移给应用程序，共享库的位置不会在程序执行期间改变。\n从应用程序中加载和链接共享库 应用程序还可以在运行时请求动态链接器加载和链接共享库，其应用场景包括：\n  Windows 应用程序的开发人员经常使用共享库来分发软件更新；\n  现代 Web 服务器使用动态链接有效地更新或添加功能。\n  Linux 系统为应用程序提供了一些简单接口以实现上述功能：\n1 2 3  #include \u003cdlfcn.h\u003evoid *dlopen(const char *filename, int flag); // Returns: pointer to handle if OK, NULL on error   函数dlopen加载并链接共享库文件filename，参数flag可以是 RTLD_GLOBAL、RTLD_NOW 和 RTLD_LAZY 中的一个或多个（详见 dlopen）。\n1 2 3  #include \u003cdlfcn.h\u003evoid *dlsym(void *handle, char *symbol); // Returns: pointer to symbol if OK, NULL on error   类似的接口函数还有 dlsym、dlclose 和 dlerror。示例程序 展示了应用程序是如何调用它们来动态链接共享库的。\n与位置无关代码 现代系统在编译共享库时，会生成一种无需重定位即可加载的代码，称为与位置无关代码（Position-Independent Code，PIC）。这样共享库就能无需链接器修改，被多个正在运行的进程同时引用。\nPIC 数据引用 无论我们在何处加载目标模块（包括共享目标模块），数据段与代码段之间的距离始终相同。编译器会在 PIC 数据段的开头为每个全局变量的引用创建一个全局偏移量表（Global Offset Table，GOT），并且为其中的每个条目生成重定位记录。加载时，动态链接器重定位每个 GOT 条目，使其包含对象的绝对地址。每个引用了全局变量的目标模块都有自己的 GOT。\n下图展示了示例共享库libvector.so中的 GOT：\n函数addvec通过 GOT[3] 间接加载全局变量addcnt的地址，然后令其自增。其关键思想在于下一条指令addl的地址（即 %rip）与 GOT[3] 之间的偏移量是一个运行时常数。如果addcnt是由另一个共享模块定义的，则需要通过 GOT 间接访问。\nPIC 函数调用 PIC 函数调用的运行时地址是在该函数第一次被调用时确定的，这种技术称为延迟绑定（Lazy Binding）。当应用程序导入了一个包含成百上千个函数的共享库（如 libc.so），却只调用其中一小部分的函数时，这种技术可以大大减少加载时不必要的重定位操作。\n延迟绑定是通过 GOT 和过程链接表（Procedure Linkage Table，PLT）共同实现的。只要目标模块调用了共享库中定义的函数，那么它就有自己的 GOT 和 PLT。上文提到，GOT 是数据段的一部分，而 PLT 则是代码段的一部分。\nGOT 和 PLT 在运行时协同工作解析函数地址的过程如下图所示：\n可执行文件中每个对共享库函数的调用都与 PLT 数组中的条目对应。其中，PLT[0] 是跳转到动态链接器的特殊条目，PLT[1] 对应系统启动函数__libc_start_main。从 PLT[2] 开始的条目对应用户代码调用的函数，如图中的addvec。\n当与 PLT 一起使用时，GOT [0] 和 GOT[1] 包含了动态连接器在解析函数地址时所需的信息，GOT[2] 是动态链接器的入口点。其余的每个条目均对应了一个在运行时需要被解析地址的调用函数，以及一个 PLT 条目。例如，GOT[4] 和 PLT[2] 均对应addvec。\n程序第一次调用addvec并解析其地址的过程如上图（a）所示：\n PLT[2] 是该函数的入口，程序首先调用它； PLT[2] 中的第一条指令间接跳转到 GOT[4]。由于最初每个 GOT 条目都指向对应 PLT 条目中的第二条指令，因此控制权将转移到 PLT[2] 中的第二条指令； PLT[2] 中的第二条指令将addvec的 ID 0x1 压入栈中，第三条指令跳转到 PLT[0]； PLT[0] 中的第一条指令将 *GOT[1] 压入栈中，第二条指令通过 GOT[2] 间接跳转到动态链接器。动态链接器根据被压入栈中的两个条目确定addvec的运行时地址，并使用该地址覆盖 GOT[4]，最终将控制权转移给addvec。  程序再次调用addvec的过程如上图（b）所示：\n 程序依然首先调用 PLT[2]； 此时 GOT[4] 指向了addvec，因此控制权直接转移给该函数。  库插入 我们可以使用库插入（Library Interpositioning）来拦截程序对共享库函数的调用，并执行我们自定义的代码。基于这项技术，我们可以计算库函数的调用次数，验证并跟踪其输入和输出的值，甚至将其替换为完全不同的函数。\n库插入的基本思想是创建一个与库函数原型相同的包装函数，然后“欺骗”系统调用包装函数而非库函数。通常，包装函数会执行自己的逻辑，然后调用库函数并将其返回值传递给调用者。\n库插入可以在编译时、链接时以及运行时使用。\n","description":"","tags":["OS"],"title":"CSAPP 读书笔记：链接","uri":"/posts/linking-note/"},{"content":"前言 Thanos 已成为目前 Kubernetes 集群监控的标准解决方案之一。它基于 Prometheus 之上，可以为我们提供：\n 全局的指标查询视图 近乎无限的数据保留期限 包含 Prometheus 在内所有组件的高可用性  在拟定监控方案之前，阅读一些成熟的 用户案例 是十分必要的。这些博文首先分析了各自团队的集群现状以及当前监控方案难以解决的痛点，再对目前流行的几种技术栈进行对比，最后介绍投入生产使用的部署方案，因此非常值得一读。\n不过，由于 Thanos 的组件众多，且每种组件都有较多参数需要配置。对于刚接触 Thanos 的用户来说，可能难以快速上手。考虑到上述博文均未给出组件的具体配置信息，而官方提供的 部署清单 又稍显复杂并缺少详细说明。因此本文将介绍一个使用 Thanos 实现多集群（租户）监控的简单 Demo，希望能对试图尝鲜 Thanos 的用户有所帮助。它实现了以下功能：\n 能够监控多个集群，并提供一个全局的指标查询视图； 每个集群都对应了一个唯一的租户 ID，可以通过租户标签区分不同集群的指标数据； 如果某个租户创建了新的集群，只需在新集群中部署 Prometheus 并配置远程写入； 简单的告警规则和告警消息推送。  Sidecar vs. Receiver Thanos 支持 Sidecar 和 Receiver 两种部署模式。它们各有利弊，需要我们根据实际情况进行取舍。\nSidecar 通常每隔 2 小时才会把 Prometheus 采集到的指标上传到对象存储中，因此 Query 查询近期数据时需要向所有 Sidecar 发起请求并合并返回结果。但这并非是 Thanos 团队引入 Receiver 的决定性因素。\n Receiver is only recommended for uses for whom pushing is the only viable solution, for example, analytics use cases or cases where the data ingestion must be client initiated, such as software as a service type environments.\n 按照文档中的说法，Receiver 只推荐用于多租户以及 Prometheus 配置受限的场景下，比如：\n 租户使用某些 SaaS 服务对集群进行监控，如 Openshift Operator 部署的 Prometheus； 由于安全或权限问题，监控团队无法在被监控集群中配置 Sidecar； 被监控集群部署的是非容器化部署的 Prometheus。  这是因为 Receiver 会暂存多个 Prometheus 实例的 TSDB，当数据量较大时可能会发生 OOM。另外根据 官方文档，开启远程写入还将增加 Prometheus 约 25% 的内存使用。\n我们并非一定要在 Receiver 和 Sidecar 之间做出抉择，比如 Lastpass 就采用了 Sidecar 与 Receiver 混合部署的方式：\n由于我们的 Demo 优先考虑实现的简单性以及对多租户的支持，对资源占用和性能的要求并不高，因此决定选用 Receiver 模式。\n快速开始  Thanos 版本：v0.24.0 Kubernetes 版本：v1.16.9-aliyun.1 环境：阿里云 ACK  1 2 3  git clone https://github.com/koktlzz/thanos-k8s-deployment.git cd thanos-k8s-deployment kubectl apply -k overlays/aliclound/   部署 Prometheus 1 2 3 4 5 6  git clone https://github.com/coreos/kube-prometheus.git cd kube-prometheus kubectl create -f manifests/setup # wait for namespaces and CRDs to become available, then kubectl create -f manifests/    在国内环境拉取 k8s.gcr.io 上的镜像可能会失败，需要将镜像名称改为 bitnami/kube-state-metrics:2.3.0 和 willdockerhub/prometheus-adapter:v0.9.0。\n 为 Prometheus 实例配置远程写入 使用 kubectl edit -n monitoring prometheus k8s命令开启 Prometheus 的远程写入：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # local clusterspec:remoteWrite:- url:http://thanos-receiver.thanos.svc.cluster.local:19291/api/v1/receive # cluster kazusaspec:remoteWrite:- url:http://thanos-receiver.uuid.cn-shanghai.alicontainer.com/api/v1/receiveheaders:THANOS-TENANT:kazusa# cluster setsunaspec:remoteWrite:- url:http://thanos-receiver.uuid.cn-shanghai.alicontainer.com/api/v1/receiveheaders:THANOS-TENANT:setsuna  一段时间后将在 Query UI（kubectl get ingresses -n thanos | grep querier）中看到三个集群（租户）的实例：\n架构说明 指标数据流向 如上图所示，监控集群（Local）以及两个外部集群（Kazusa 和 Setsuna）中的 Prometheus 均将指标数据写入到软租户（soft-tenant）的 Receiver 中。而由于 Kazusa 和 Setsuna 的 Prometheus 还在远程写入中配置了 HTTP 头部，因此软租户 Receiver 会根据其中的租户 ID 将其转发到对应的硬租户 Receiver 中。\n我们可以在 Receiver 容器挂载的 Hashring 配置文件中找到每个租户 Receiver 的 endpoints：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  [root@master1 thanos]# kubectl exec -n thanos thanos-receiver-default-0 -- cat /etc/prometheus/hashring-config/hashrings.json | jq [ { \"hashring\": \"hashring-setsuna\", \"tenants\": [ \"setsuna\" ], \"endpoints\": [ \"thanos-receiver-setsuna-0.thanos-receiver-setsuna.thanos.svc.cluster.local:10901\" ] }, { \"hashring\": \"hashring-kazusa\", \"tenants\": [ \"kazusa\" ], \"endpoints\": [ \"thanos-receiver-kazusa-0.thanos-receiver-kazusa.thanos.svc.cluster.local:10901\", \"thanos-receiver-kazusa-1.thanos-receiver-kazusa.thanos.svc.cluster.local:10901\", \"thanos-receiver-kazusa-2.thanos-receiver-kazusa.thanos.svc.cluster.local:10901\" ] }, { \"hashring\": \"default\", \"endpoints\": [ \"thanos-receiver-default-0.thanos-receiver-default.thanos.svc.cluster.local:10901\" ] } ]   实际上该文件来自于 Configmap thanos-receiver-hashring-generated-config，它是由 Thanos-Receive-Controller 读取当前集群中的 Receiver 实例并根据 Configmap thanos-receiver-hashring-config 动态生成的。\n Receiver 使用一致性哈希作为数据分发策略的原因详见：Thanos Receiver - why does it need consistent hashing?\n Receiver 的可扩展性和高可用性 假设 Kazusa 集群中的工作负载比较多，其指标数据量也比较大。为了防止接收 Kazusa 集群指标数据的 Receiver 发生 OOM，我们增加其 Statefulset 的 副本数。\n上一节提到，指标数据是软租户 Receiver 根据 Hashring 配置分发到硬租户 Receiver 的。这意味着即使我们将 Kazusa 的 Receiver 扩展到三个，数据也不会在某台 Receiver 宕机后分发到其他可用的 Receiver 中。因此我们还要为数据设置 副本，从而实现高可用。Receiver 的副本包含 receiver_replica 标签，Query 可以根据它来对数据 去重。\n 关于 Receiver 的可扩展性和高可用性，详见 Turn It Up to a Million: Ingesting Millions of Metrics with Thanos Receive.\n Headless Service 我们为每个租户的 Receiver 都创建了各自的 Headless Service，这样 Query 就可以通过 DNS 的 SRV 记录动态发现所有的 Receiver 实例：\n1 2 3  - --store=dnssrv+_grpc._tcp.thanos-receiver-default.thanos.svc.cluster.local- --store=dnssrv+_grpc._tcp.thanos-receiver-kazusa.thanos.svc.cluster.local- --store=dnssrv+_grpc._tcp.thanos-receiver-setsuna.thanos.svc.cluster.local  如果将其改为 ClusterIP 类型，Store API 的 grpc 请求将通过轮询发往其中一台 Receiver 实例。这样在 Query UI 中就只能看到一台 Kazusa Receiver：\n值得注意的是，用于处理指标数据的写入请求的 Service 是 ClusterIP 类型的。\nFuture Work 本文介绍的 Demo 仅供读者参考，若想要将其投入生产使用，还需要考虑以下方面：\n 为软租户 Receiver 向集群外部暴露的 Ingress 配置 TLS 证书校验； 上文提到，开启远程写入将增加 Prometheus 的内存使用，我们应当根据 文档 中的建议对其参数进行调优； 持久化 Compactor 和 Storegateway 的数据目录可以减少其重启时间，存储空间的大小可以参考 Slack 中的 讨论； 部署前根据指标数据量评估 Receiver 的 Request、Limit 以及副本数，防止其因数据量过大而导致 OOM； 此 Demo 使用 Ruler 进行全局告警，可能因查询超时而发生 报警失效。  参考文献 Multi cluster monitoring with Thanos · Banzai Cloud\nThanos Remote Write\nAchieve Multi-tenancy in Monitoring with Prometheus \u0026 Thanos Receiver\nAdopting Thanos at Lastpass\n","description":"","tags":["Prometheus","Thanos"],"title":"使用 Thanos 实现多集群（租户）监控","uri":"/posts/thanos-receiver-deployment-demo/"},{"content":"在计算机运行过程中，程序计数器将依次指向一系列的值：$a_0, a_1, …, a_n$。其中，$a_k$ 是其对应指令 $I_k$ 的地址。每个从 $a_k$ 到 $a_{k+1}$ 的转换都称为控制转移（Control Transfer），一系列的控制转移则称为处理器的控制流（Control Flow）。\n最简单的控制流便是程序中指令的顺序执行、跳转、调用以及返回，不过系统还必须对其状态的变化做出一定反应。这些变化无法被内部程序变量捕获，也不一定与程序的执行有关。比如，数据包到达网络适配器后需要被存储到内存中，程序访问磁盘中的数据需要得知其何时准备就绪，父进程必须在其子进程终止时收到通知。\n现代系统通过异常控制流（Exceptional Control Flow，ECF）来处理上述情况，它应用于计算机系统的所有级别中：\n 硬件检测到的事件通过 ECF 将控制转移到异常处理程序（Exception Handler）； ECF 是操作系统实现 I/O、进程和虚拟内存的基本机制； 程序可以通过 ECF 请求一些操作系统服务，如向磁盘写入数据、读取网络中收到的数据以及创建新进程等； 一些编程语言通过 ECF 使程序进行非本地跳转（即违背通常的调用、返回栈规则的跳转）以对错误进行响应。  异常 异常（Exception）是为了响应处理器状态变化而在控制流中发生的突然变化，下图展示了其基本思想：\n处理器状态的变化称为事件（Event），它可能与当前指令（$I_{curr}$）的执行直接相关，比如算术溢出或除数为零。 事件也可能与当前指令的执行无关，比如系统计时器关闭或 I/O 请求完成。\n异常处理 异常处理涉及到软件和硬件的密切合作，因此很容易将不同组件执行的工作相混淆。系统中每种可能的异常都对应了一个唯一的非负整数，即异常数字（Exception Number）。当计算机系统启动时，操作系统会初始化一个跳转表，称为异常表（Exception Table）。异常数字是异常表的索引，而异常表中的每个条目 k 均包含了异常 k 的处理程序地址：\n当处理器检测到事件的发生时，首先将确定异常数字，然后根据异常表调用对应的异常处理程序。\n异常与过程调用类似，但也有一些重要区别：\n 异常的返回地址要么是当前指令（$I_{curr}$），要么是下一条指令（$I_{next}$）； 处理器还会将一些额外的处理器状态压入栈中； 当控制从用户程序转移到内核时，一切都将被压入到内核的栈中； 异常处理程序在内核态运行，因此可以访问所有系统资源。  异常的分类 中断 中断（Interrupt）异步发生，因为它是由处理器外部的 I/O 设备发出的信号产生的。\n当前指令执行完毕后，处理器注意到中断引脚变高，于是从系统总线读取异常数字，然后调用对应的中断处理程序。当处理程序返回时，它将控制权返回给下一条指令。随后程序继续执行，就好像中断从未发生过一样。 其余种类的异常作为当前指令的执行结果同步发生，我们将这类指令称为故障指令（Faulting Instruction）。\n陷阱和系统调用 与中断处理程序一样，陷阱（Trap）处理程序也将控制返回给下一条指令。其最重要的用途是在用户程序和内核之间提供接口，即系统调用（System Call）。\n用户程序通过系统调用向内核请求服务，如读取文件（read）、创建新进程（fork）、加载新程序（execve）和终止当前进程（exit）等。\n在程序员看来，系统调用和常规函数没有什么区别。但常规函数运行在用户态，因此其可执行的指令类型受限，也只能访问用户栈。而系统调用运行在内核态，能够执行特权指令并访问内核栈。\n故障 故障（Faulting）是由一些错误状况引起的异常，而这些错误情况有可能被处理程序修正，否则将返回到内核中的中止例程（图中的abort）：\n中止 与故障相比，导致中止（Abort）发生的错误状况无法挽救，通常是硬件出现问题，如 RAM 位损坏引起的奇偶校验错误。中止处理程序永远不会将控制权返回给应用程序：\nLinux/x86-64 系统中的异常    Exception Number Description Exception Class     0 Divide error Fault   13 General protection fault Fault   14 Page fault Fault   18 Machine check Abort   32-255 OS-defined exception Interrupt or Trap    故障和中止  除法故障：当应用程序尝试除以 0 或除法指令的结果对目标操作数来说太大时，就会发生除法故障。Unix 不会试图纠正除法故障，而是直接中止程序； 一般保护性故障：若程序引用了未定义的虚拟内存区域，或试图写入只读文本段等，可能引起一般保护性故障。Linux 不会试图纠正该故障，Shell 一般称其为分段故障（Segmentation Faults）； 页面故障：当程序引用的虚拟地址对应的页面不在内存而在磁盘上时，将导致页面故障。处理程序将磁盘上合适的虚拟内存页面映射到物理内存页面，然后重新执行故障指令； 机器检查：在执行故障指令期间检测到致命的硬件错误时，会发生机器检查，处理程序永远不会将控制权返回给应用程序。  系统调用 上图中的每个系统调用都有一个唯一的数字，对应了内核中跳转表的偏移量。该跳转表与上文提到的异常表不同。\nC 标准库为大多数系统调用提供了一组包装函数（Wrapper Function），它们比直接使用系统调用更加方便。系统调用及其相关的包装函数统称为系统级函数。举例来说，我们可以使用系统级函数write代替printf：\n1 2 3 4 5  int main() { write(1, \"hello, world\\n\", 13); _exit(0); }   X86-64 系统通过syscall指令使用系统调用，其所有参数均通过寄存器传递。按照惯例，寄存器 %rax 保存系统调用编号，寄存器 %rdi、%rsi、%rdx、%r10、%r8 和 %r9 依次保存各参数的值。系统调用的返回值将写入到寄存器 %rax 中，若为负则表示发生了与负errno相关的错误。因此，上面的程序可以直接用汇编语言表示为：\n进程 进程是正在执行的程序实例，而系统中的每个程序都在进程的上下文中运行。上下文包含了程序正确运行所需的状态，如程序代码和存储在内存中的数据、栈、寄存器、程序计数器、环境变量和打开的文件描述符集合。\n进程为应用提供了两个关键抽象：\n 一个独立的逻辑控制流，让我们产生程序独占处理器的错觉； 一个私有的地址空间，让我们产生程序独占内存的错觉。  逻辑控制流 进程轮流使用处理器。每个进程执行其流程的一部分，然后在其他进程执行时被抢占（即暂时挂起）。\n并发流 执行时间重叠的两个逻辑控制流称为并发流（Concurrent Flow），它们并发运行。如上图 8.12 所示，进程 A 和 进程 B 并发运行，但进程 B 和进程 C 则不是。\n并发流的概念与处理器的核数以及计算机的数量无关，只要两个逻辑控制流在时间上重叠，那么它们便是并发的。如果两个逻辑控制流在不同的处理器内核或计算机上同时运行，我们就称它们为并行流（Parallel Flow）。显然，并行流是并发流的子集。\n私有地址空间 进程为程序提供了私有地址空间。它是程序独享的，与空间内特定地址相关的内存字节通常不能被其他任何进程读取或写入。尽管私有地址空间的内容不同，但其具有相同的组织结构：\n地址空间底部是为用户程序保留的，代码段总是从地址 0x400000 开始。地址空间顶部是为内核保留的，包含了内核为进程执行指令（如程序执行系统调用）时使用的代码、数据和栈。\n用户态和内核态 处理器通过保存在控制寄存器中的模式位（Mode Bit）来识别进程当前享有的特权。当模式位被设置时，进程运行在内核态（Kernel Mode），反之则运行在用户态（User Mode）。在内核态中运行的程序可以执行指令集中的任意指令，并且可以访问系统中的任意位置。而在用户态中运行的程序则受到限制，只能使用系统调用间接地访问内核代码和数据。\n应用程序的进程只能通过异常来从用户态切换到内核态。当异常发生且控制转移到异常处理程序时，处理器切换到内核态。随后异常处理程序在内核态中运行，处理器将在它返回时切换回用户态。\n上下文切换 在进程执行期间，内核可以暂时挂起当前进程并重启先前被抢占的进程，这称为调度（Scheduling）。内核调度新进程是通过上下文切换（Context Switch）机制来实现的，该机制：\n 保存当前进程的上下文； 恢复之前被抢占进程的上下文； 将控制权转移给新进程。  程序使用系统调用时可能会发生上下文切换。比如系统调用read需要访问磁盘中的数据，内核可以通过上下文切换来调度另一个进程，这样就无需等待数据从磁盘加载到内存。\n系统调用错误处理 当执行 Unix 系统级函数遇到错误时，它们会返回 -1 并设置全局整型变量errno的值。因此我们可以在程序中检查调用是否发生错误，如：\n1 2 3 4 5  if ((pid = fork()) \u003c 0) { fprintf(stderr, \"fork error: %s\\n\", strerror(errno)); exit(0); }   其中，strerror函数会根据errno的值返回相关的文本字符串。我们可以定义一个错误报告（Error-reporting）函数，从而将上述代码进行简化：\n1 2 3 4 5 6 7 8  void unix_error(char *msg) /* Unix-style error */ { fprintf(stderr, \"%s: %s\\n\", msg, strerror(errno)); exit(0); } if ((pid = fork()) \u003c 0) unix_error(\"fork error\");   我们再定义一个错误处理（Error-handling）函数，将代码进一步地简化：\n1 2 3 4 5 6 7 8 9  pid_t Fork(void) { pid_t pid; if ((pid = fork()) \u003c 0) unix_error(\"Fork error\"); return pid; } pid = Fork();   这样我们就可以使用包装函数Fork代替fork及其错误检查代码。本书使用的包装函数均定义在 csapp.h 和 csapp.c 中。\n进程控制 获取进程 ID 每个进程都有一个唯一且大于 0 的进程 ID（PID）。函数getpid返回调用进程的 PID，而函数`getppid则返回创建调用进程的进程（父进程） 的 PID。\n1 2 3 4  #include \u003csys/types.h\u003e#include \u003cunistd.h\u003epid_t getpid(void); pid_t getppid(void);   二者返回值的类型为pid_t，它在 Linux 系统的 sys/types.h 中定义为 int。\n创建和中止进程 在程序员看来，进程有三种状态：\n 运行（Running）：该进程要么在 CPU 中执行，要么在等待内核调度； 停止（Stopped）：进程执行暂停，并且不会被调度； 终止（Terminated）：进程永久地停止。  函数exit会以参数status作为退出状态终止进程：\n1 2  #include \u003cstdlib.h\u003evoid exit(int status);   父进程通过调用fork函数创建一个新的子进程：\n1 2 3  #include \u003csys/types.h\u003e#include \u003cunistd.h\u003epid_t fork(void);   子进程将获得一个与父进程相同但独立的用户级虚拟内存空间副本，包括代码、数据、堆、共享库和用户栈等。它还会得到与父进程相同的文件描述符副本，因此可以在调用fork时读写任意父进程打开的文件。父进程和子进程之间最显著的区别便是 PID 不同。\n函数fork执行一次却返回两次：在父进程中返回子进程的 PID，在子进程中返回 0。由于子进程的 PID 始终大于 0 ，因此我们可以通过返回值判断程序在哪个进程中执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  #include \"csapp.h\" int main() { pid_t pid; int x = 1; pid = Fork(); if (pid == 0) { /* Child */ printf(\"child : x=%d\\n\", ++x); exit(0); } /* Parent */ printf(\"parent: x=%d\\n\", --x); exit(0); }   该程序编译后运行的可能结果为：\n1 2 3  linux\u003e ./fork parent: x=0 child : x=2   从结果我们可以看出：父进程和子进程并发执行，我们永远无法预测其执行顺序；子进程的地址空间是父进程的副本，因此当第六行的函数Fork返回时，两进程中的局部变量x均为1；两进程对变量的更改互不影响，所以最终输出的值不同。\n绘制进程图（Process Graph）对理解fork函数很有帮助，如：\n回收子进程 当进程终止时，内核不会立即将其移除。它需要被其父进程回收（Reap），否则将变成僵尸（Zombie）进程。当父进程回收其终止的子进程时，内核会将子进程的退出状态传递给父进程，然后再丢弃它。\n如果父进程终止，内核需要安排init进程（PID 为 1）“收养”孤儿进程。如果父进程在终止前没有回收僵尸子进程，那么init进程会回收它们。\n进程通过调用函数waitpid等待其子进程终止或停止：\n1 2 3 4  #include \u003csys/types.h\u003e#include \u003csys/wait.h\u003epid_t waitpid(pid_t pid, int *statusp, int options); // Returns: PID of child if OK, 0 (if WNOHANG), or −1 on error   默认情况下（参数options为 0 时），函数waitpid会暂停调用进程，直至其等待集（Wait Set）中的某个子进程终止。该函数始终返回导致其返回的子进程 PID。此时，终止的子进程已被回收，内核从系统中删除了它的所有痕迹。\n若参数pid_t大于 0 ，则等待集中只有一个 PID 等于该参数的子进程。若参数pid_t等于 -1，则等待集包含调用进程的所有子进程。\n我们可以通过修改参数options的值来改变函数waitpid的行为：\n WNOHANG：如果等待集中的子进程还未终止，则立即返回 0； WUNTRACED：暂停调用进程执行，直到等待集中的进程终止或停止（默认情况下仅会对终止的子进程返回）； WCONTINUED：暂停调用进程执行，直到等待集中的进程终止或等待集中停止的进程收到 SIGCONT 信号恢复。  若参数statusp不为 NULL，那么waitpid还会将导致其返回的子进程状态信息编码到status中（*statusp = status）。wait.h 文件定义了几个用于解释参数status的宏：\n WIFEXITED(status)：如果子进程正常终止（比如调用exit或返回），则返回 True； WEXITSTATUS(status)：如果 WIFEXITED() 返回 True，则返回终止子进程的退出状态； WIFSIGNALED(status)：如果子进程由于未捕获的信号而终止，则返回 True； WTERMSIG(status)：如果 WIFSIGNALED() 返回 True，则返回导致子进程终止的信号编号； WIFSTOPPED(status)：如果导致返回的子进程当前已停止，则返回 True； WSTOPSIG(status)：如果 WIFSTOPPED() 返回 True，则返回导致子进程停止的信号编号； WIFCONTINUED(status)：如果子进程收到 SIGCONT 信号后恢复，则返回 True。  如果调用进程没有子进程，waitpid将返回 -1 并将全局变量errno设为 ECHILD。而如果waitpid被信号中断，则返回 -1 并将全局变量errno设为 EINTR。\n函数wait是waitpid的简化版本， wait(\u0026status)等效于waitpid(-1, \u0026status, 0)。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  #include \"csapp.h\"#define N 2  int main() { int status, i; pid_t pid; /* Parent creates N children */ for (i = 0; i \u003c N; i++) if ((pid = Fork()) == 0) /* child */ exit(100 + i); /* Parent reaps N children in no particular order */ while ((pid = waitpid(-1, \u0026status, 0)) \u003e 0) { if (WIFEXITED(status)) printf(\"child %d terminated normally with exit status=%d\\n\", pid, WEXITSTATUS(status)); else printf(\"child %d terminated abnormally\\n\", pid); } /* The only normal termination is if there are no more children */ if (errno != ECHILD) unix_error(\"waitpid error\"); exit(0); }   如示例程序所示，父进程首先调用Fork创建了 N 个退出状态唯一的子进程（exit(100+i)）。 随后在 While 循环的测试条件中通过waitpid等待其所有的子进程终止，并打印子进程的退出状态。最终所有的子进程均被回收，waitpid返回 -1 且将全局变量errno设为 ECHILD，函数执行完毕。\n在 Linux 系统上运行该程序时，它会产生以下输出：\n1 2 3  linux\u003e ./waitpid1 child 22966 terminated normally with exit status=100 child 22967 terminated normally with exit status=101   值得注意的是，父进程回收子进程的顺序是随机的。我们可以对上述程序进行一定 修改，从而使其按子进程的 PID 顺序输出。\n让进程休眠 函数sleep可以让进程暂停执行一段时间：\n1 2  #include \u003cunistd.h\u003eunsigned int sleep(unsigned int secs);   如果请求的暂停时间已经过去，则函数返回 0，否则将返回剩余的暂停时间。当该进程被信号中断时，后一种情况便会发生。\n函数pause会使调用进程进入休眠状态，直至收到信号。该函数始终返回 -1:\n1 2  #include \u003cunistd.h\u003eint pause(void);   加载并运行程序 函数execve在当前进程的上下文中加载并运行一个新程序：\n1 2  #include \u003cunistd.h\u003eint execve(const char *filename, const char *argv[], const char *envp[]);   参数filename是加载并运行的可执行文件名称，argv和envp则分别是参数和环境变量列表。函数execve通常没有返回值，仅在出现错误时返回 -1。\n变量argv指向一个以 Null 结尾的指针数组，其中的每个指针都指向一个参数字符串。一般来说，argv[0]是可执行目标文件名称。变量envp也指向一个以 Null 结尾的指针数组，其中的每个指针都指向一个环境变量字符串，而每个字符串都是一个 name=value 形式的键值对。两者的数据结构如下：\nexecve加载文件名后，会调用启动代码。 启动代码设置栈并将控制权传递给新程序的main函数，其原型为：\n1  int main(int argc, char *argv[], char *envp[]);   main函数执行时的用户栈结构如下图所示。其三个参数分别保存在不同的寄存器中：参数argc给出数组argv[]中的非空指针数量，参数argv指向数组argv[]中第一个元素，而参数envp则指向数组envp[]中的第一个元素。\nLinux 提供了几个用于操作环境变量数组的函数：\n1 2 3 4  #include \u003cstdlib.h\u003echar *getenv(const char *name); int setenv(const char *name, const char *newvalue, int overwrite); void unsetenv(const char *name);   如果数组包含格式为 name=value 的字符串，则函数getenv会返回其对应的 value， 函数unsetenv会将其删除，而函数setenv会将 value 替换为参数newvalue（overwrite非零时）。如果 name 不存在，则函数setenv会将 name=newvalue 添加到数组中。\n使用 fork 和 execve 运行程序 Unix shell 和 Web 服务器等程序大量使用了fork和execve函数。本书提供了一个简单的 shell 程序，其缺陷在于没有回收任何后台运行的子进程。我们需要使用下一节介绍的信号来解决这一问题。\n信号 信号（Signal）是一种高级的异常控制流，它允许进程和内核通知其他进程系统中发生了某些类型的事件。Linux 支持的信号类型多达三十种：\n低级别的硬件异常由内核的异常处理程序处理，通常不会对用户进程可见，而信号则可以将此类异常暴露给用户进程。例如一个进程试图除以 0，内核就会向它发送一个 SIGFPE（编号 8）信号。\n信号术语 发送信号到目标进程需要完成两个步骤：\n 发送（传递）信号：内核通过更新目标进程上下文中的某些状态来向目标进程发送信号。发送信号的原因有两种：（1）内核检测到系统事件的发生，如被 0 除错误或子进程终止；（2）进程调用了kill函数（将在下一节介绍）。进程可以向自己发送信号； 接收信号：当内核强制目标进程以某种方式对信号做出响应时，它便会接收到信号。该进程可以通过执行用户级别的信号处理程序（Signal Handler）来忽略、终止或捕获信号。  已发送但还未接收的信号称为待处理信号（Pending Signal）。在任意时间点，相同类型的待处理信号最多只能有一个。这意味着如果一个进程已经有一个类型为 k 的待处理信号，那么后续所有发送给该进程的 k 类型信号都将被丢弃。进程还可以选择性地阻塞（Block）某些信号的接收。\n发送信号 进程组 每个进程都属于一个进程组（Process Group），它由一个正整数的进程组 ID 所标识。getpgrp函数返回当前进程的进程组 ID：\n1 2  #include \u003cunistd.h\u003epid_t getpgrp(void);   默认情况下，子进程与其父进程属于同一个进程组。一个进程可以通过setpgid函数改变自己或另一个进程的进程组：\n1 2  #include \u003cunistd.h\u003eint setpgid(pid_t pid, pid_t pgid);   该函数会将进程pid的进程组更改为pgid。若将参数pid或pgid设为 0，则相当于使用调用进程的 PID 作为参数。举例来说，如果进程 15213 调用函数 setpgid(0, 0)，那么将会创建一个进程组 ID 为 15213 的新进程组，并使该进程加入此组。\n使用 /bin/kill 程序发送信号 命令/bin/kill -9 15213会将编号为 9 的 SIGKILL 信号发送到进程 15213。而 PID 为负则代表信号将发送到进程组 ID 中的所有进程，因此命令/bin/kill -9 -15213会该信号发送到进程组 15213 中的每一个进程。\n从键盘发送信号 Unix Shell 使用任务（Job）表示单个命令行（如ls | sort）创建的进程，同一时间内只能有一个前台任务和多个后台任务。\n在键盘上键入 Ctrl+C 会使内核向前台进程组中的每个进程发送一个 SIGINT 信号，这将终止前台任务。同样，键入 Ctrl+Z 会使内核向前台进程组中的每个进程发送一个 SIGTSTP 信号，这将停止（挂起）前台任务。\n使用 kill 函数发送信号 进程可以通过调用kill函数向其他进程（包括其自身）发送信号：\n1 2 3  #include \u003csys/types.h\u003e#include \u003csignal.h\u003eint kill(pid_t pid, int sig);   如果参数pid大于 0，则该函数将编号为sig的信号发送给进程pid；如果参数pid等于 0，则该函数将信号发送给调用进程所在进程组中的所有进程；如果参数pid小于 0，则该函数将信号发送给进程|pid|所在进程组中的所有进程。\n使用 alarm 函数发送信号 进程可以通过调用alarm函数向自己发送 SIGALRM 信号：\n1 2  #include \u003cunistd.h\u003eunsigned int alarm(unsigned int secs);   内核将在secs秒内向调用进程发送 SIGALRM 信号。该函数会丢弃任何未处理的警告信号，并返回其本应剩余的秒数。\n接收信号 当内核将进程 p 从内核态切换到用户态时，它会检查 p 未阻塞且未处理（Pending \u0026 ~Blocked）的信号集。通常该集合为空，内核将控制权转移给 p 逻辑控制流中的下一条指令。而如果该集合非空，则内核会选择集合中的某个信号 k 并强制 p 接收它。信号将触发进程完成一些动作（Action），预定义的默认动作有：\n 进程终止； 进程终止并转储核心（Dump Core，即将代码和数据内存段的镜像写入磁盘）； 进程停止（暂停），直到接收 SIGCONT 信号重新启动； 进程忽略该信号。  每种信号的默认动作见 图 8.26。除 SIGSTOP 和 SIGKILL 信号外，进程可以通过函数signal修改信号的默认动作：\n1 2 3 4  #include \u003csignal.h\u003etypedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); // Returns: pointer to previous handler if OK, SIG_ERR on error (does not set errno)   如果参数handler为 SIG_IGN，则 signum类型的信号将会被忽略；如果参数handler为 SIG_DFL，则signum类型的信号的动作将恢复为默认；如果参数handler为用户定义的信号处理程序地址，则进程接收到signum类型的信号后会调用该程序，这种方法称为安装处理程序（Installing Handler）。调用处理程序称为捕获信号（Catching Signal），执行处理程序称为处理信号（Handling Signal）。\n如果我们在示例程序运行时按下 Ctrl+C，该进程不会直接终止而是输出一段信息后才终止：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #include \"csapp.h\" void handler(int sig) /* SIGINT handler */ { printf(\"Caught SIGINT\\n\"); exit(0); } int main() { /* Install the SIGINT handler */ if (signal(SIGINT, handler) == SIG_ERR) unix_error(\"signal error\"); pause(); /* Wait for the receipt of a signal */ exit(0); }   信号处理程序还可以被其他处理程序中断（$s \\ne t$）：\n阻塞信号 Linux 为阻塞信号提供了显式和隐式的实现机制：\n 隐式：默认情况下，内核会阻塞任何与处理程序当前正在处理的信号类型相同的未处理信号。比如上图 8.31 中，如果信号 t 的类型与 s 相同，则 t 将在处理程序 S 返回前持续挂起； 显式：应用程序可以通过调用sigprocmask等函数阻塞信号或解除信号的阻塞。  1 2 3 4 5 6 7 8 9 10  #include \u003csignal.h\u003eint sigprocmask(int how, const sigset_t *set, sigset_t *oldset); int sigemptyset(sigset_t *set); int sigfillset(sigset_t *set); int sigaddset(sigset_t *set, int signum); int sigdelset(sigset_t *set, int signum); // Returns: 0 if OK, −1 on error  int sigismember(const sigset_t *set, int signum); // Returns: 1 if member, 0 if not, −1 on error   sigprocmask函数可以改变当前阻塞信号的集合（设为blocked），具体行为取决于参数how的值：\n SIG_BLOCK：将参数set中的信号阻塞（blocked = blocked | set）； SIG_UNBLOCK：为set中的信号解除阻塞（blocked = blocked \u0026 ~set）； SIG_SETMASK：将阻塞信号集合设为set（blocked = set）。  如果参数oldset非空，则先前blocked的值会存储在oldset中。\n函数sigemptyset将set初始化为空集；sigfillset将所有信号加入到set中；sigaddset将编号为signum的信号加入到set中；sigdelset将编号为signum的信号从set中删除；如果signum信号在set中，则函数sigismember返回 1，否则返回 0。\n示例程序暂时阻塞了 SIGINT 信号的接收：\n1 2 3 4 5 6 7 8 9 10  sigset_t mask, prev_mask; Sigemptyset(\u0026mask); Sigaddset(\u0026mask, SIGINT); /* Block SIGINT and save previous blocked set */ Sigprocmask(SIG_BLOCK, \u0026mask, \u0026prev_mask); // Code region that will not be interrupted by SIGINT  /* Restore previous blocked set, unblocking SIGINT */ Sigprocmask(SIG_SETMASK, \u0026prev_mask, NULL);   编写信号处理程序 安全的信号处理 如果处理程序和主程序并发地访问同一个全局数据结构，就会发生不可预知的严重问题。因此我们应当遵循以下守则：\n 使信号处理程序尽可能的简单； 仅调用异步信号安全（Async-Signal-Safe）的函数。这种函数一般只访问局部变量，或者不会被其他信号处理程序中断。值得注意的是，许多常用的函数，如printf、sprintf、malloc和exit等并非异步信号安全。而调用write函数是信号处理程序生成输出的唯一安全方法； 保存并恢复变量errno：许多 Linux 异步信号安全函数返回错误时会设置变量errno的值，可能会干扰程序中其他依赖errno的部分。因此当处理程序有返回时，我们应当在调用前将errno保存到局部变量中，并在返回前恢复其值； 访问全局数据结构时阻塞所有信号； 使用volatile声明全局变量，如volatile int g;； 使用sig_atomic_t类型声明标识（Flag），如volatile sig_atomic_t flag;。  正确的信号处理 上文提到，同一时间内相同类型的未处理信号最多只能有一个。关键在于，未处理信号的存在仅表明至少有一个信号已经到达。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  #include \"csapp.h\" void handler1(int sig) { pid_t pid; if ((pid = waitpid(-1, NULL, 0)) \u003c 0) unix_error(\"waitpid error\"); printf(\"Handler reaped child %d\\n\", (int)pid); Sleep(2); return; } int main() { int i, n; char buf[MAXBUF]; if (signal(SIGCHLD, handler1) == SIG_ERR) unix_error(\"signal error\"); /* Parent creates children */ for (i = 0; i \u003c 3; i++) { if (Fork() == 0) { fprintf(\"Hello from child %d\\n\", (int)getpid()); Sleep(1); exit(0); } } /* Parent waits for terminal input and then processes it */ if ((n = read(STDIN_FILENO, buf, sizeof(buf))) \u003c 0) unix_error(\"read\"); printf(\"Parent processing input\\n\"); while (1) ; exit(0); }   在示例程序中，父进程安装了处理程序handler1并创建三个子进程。它等待来自终端的输入，然后进入 While 循环。每当一个子进程终止时，内核将发送一个 SIGCHLD 通知父进程。父进程捕获信号后回收子进程，输出一段信息然后返回。\n在 Linux 上运行该程序得到的输出结果为：\n1 2 3 4 5 6 7 8  linux\u003e ./signal1 Hello from child 14073 Hello from child 14074 Hello from child 14075 Handler reaped child Handler reaped child CR Parent processing input   我们发现父进程创建了三个子进程，然而却只回收了两个。这是因为信号处理程序在处理第一个信号时，第二个信号到达并添加到未处理信号集中。此时如果第三个信号到达，由于已有一个 SIGCHLD 信号未处理，因此它被直接丢弃。处理程序返回后，内核发现第二个信号还未处理，于是强制父进程接收该信号。父进程再次捕获信号并第二次执行处理程序。当处理程序第二次返回后，不再有任何未处理的信号，第三个子进程成为了僵尸进程。\n适当修改处理程序可以解决这一问题：\n1 2 3 4 5 6 7 8 9 10 11  void handler2(int sig) { pid_t pid; while ((pid = waitpid(-1, NULL, 0)) \u003e 0) printf(\"Handler reaped child %d\\n\", (int)pid); if (errno != ECHILD) unix_error(\"waitpid error\"); Sleep(2); return; }   可移植的信号处理 不同的系统有着不同的信号处理语义，因此 Posix 标准定义了sigaction函数，它允许用户在安装信号处理程序时清楚地指定他们想要的语义：\n1 2 3 4  #include \u003csignal.h\u003eint sigaction(int signum, struct sigaction *act, struct sigaction *oldact); // Returns: 0 if OK, −1 on error   然而，sigaction函数十分笨重，因此我们常使用它的包装函数Signal：\n1 2 3 4 5 6 7 8 9 10 11  handler_t *Signal(int signum, handler_t *handler) { struct sigaction action, old_action; action.sa_handler = handler; sigemptyset(\u0026action.sa_mask); /* Block sigs of type being handled */ action.sa_flags = SA_RESTART; /* Restart syscalls if possible */ if (sigaction(signum, \u0026action, \u0026old_action) \u003c 0) unix_error(\"Signal error\"); return (old_action.sa_handler); }   避免并发错误 上文提到，我们永远无法预测两个同步（并发）运行的函数的调用顺序。如果调用顺序会影响结果的正确性，我们就将这种错误称为竞争（Race）。为此，我们可以通过阻塞相关信号来避免这一问题。\n显式等待信号 有时候主程序需要显式等待某个信号处理程序运行。例如 Linux Shell 创建前台任务后，必须等待任务终止并被 SIGCHLD 处理程序回收，然后才能接收下一条用户命令。示例程序展示了其基本思想：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36  #include \"csapp.h\" volatile sig_atomic_t pid; void sigchld_handler(int s) { int olderrno = errno; pid = waitpid(-1, NULL, 0); errno = olderrno; } void sigint_handler(int s) { } int main(int argc, char **argv) { sigset_t mask, prev; Signal(SIGCHLD, sigchld_handler); Signal(SIGINT, sigint_handler); Sigemptyset(\u0026mask); Sigaddset(\u0026mask, SIGCHLD); while (1) { Sigprocmask(SIG_BLOCK, \u0026mask, \u0026prev); /* Block SIGCHLD */ if (Fork() == 0) /* Child */ exit(0); /* Parent */ pid = 0; Sigprocmask(SIG_SETMASK, \u0026prev, NULL); /* Unblock SIGCHLD */ /* Wait for SIGCHLD to be received (wasteful) */ while (!pid) ; /* Do some work after receiving SIGCHLD */ printf(\".\"); } exit(0); }   父进程首先为信号 SIGCHLD 和 SIGINT 安装处理程序，然后创建子进程并将全局变量pid设为 0，最后进入自旋循环（while (!pid)）。子进程终止后，pid变为非 0，于是父进程退出自旋循环。为了防止父进程进入自旋循环前接收到 SIGCHLD ，我们需要在创建子进程之前阻塞该信号。\n这段代码是正确的，但自旋循环会浪费处理器资源。我们可以将其改为：\n1 2  while (!pid) /* Race! */ pause();   其问题在于：如果父进程在 While 的条件测试之后而pause执行之前接收到 SIGCHLD，那么程序将永远休眠。而如果我们将pause改为sleep：\n1 2  while (!pid) /* Too slow! */ sleep(1);   这样虽然避免了竞争问题，但会增加程序的运行时间。正确的解决方案是使用函数sigsuspend：\n1 2 3  #include \u003csignal.h\u003eint sigsuspend(const sigset_t *mask); // Returns: -1   该函数用参数mask替换当前的阻塞信号集合，然后暂停进程直至其接收信号。如果该信号的动作是终止进程，则进程终止且不从sigsuspend返回；如果该信号的动作是运行一个处理程序，则sigsuspend在处理程序返回后返回，并将阻塞信号集合的状态恢复。\n实际上它等效于下列函数组合的原子性（Atomic，即不可中断）版本：\n1 2 3  sigprocmask(SIG_SETMASK, \u0026mask, \u0026prev); pause(); sigprocmask(SIG_SETMASK, \u0026prev, NULL);   原子性保证了对第一行sigprocmask和第二行pause的调用是同时的，从而消除了如果在调用sigprocmask之后且调用pause之前接收到信号所导致的永久休眠问题。因此我们可以将示例函数修改为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  while (1) { Sigprocmask(SIG_BLOCK, \u0026mask, \u0026prev); /* Block SIGCHLD */ if (Fork() == 0) /* Child */ exit(0); /* Wait for SIGCHLD to be received */ pid = 0; while (!pid) sigsuspend(\u0026prev); /* Optionally unblock SIGCHLD */ Sigprocmask(SIG_SETMASK, \u0026prev, NULL); /* Do some work after receiving SIGCHLD */ printf(\".\"); }   非本地调转 C 提供了一种用户级别的异常控制流，称为非本地跳转（Nonlocal Jump）。它可以无需经过正常的调用和返回序列，就将控制权从一个函数直接转移到另一个当前正在执行的函数。非本地跳转是通过setjmp和longjmp函数实现的：\n1 2 3 4 5 6 7 8  #include \u003csetjmp.h\u003eint setjmp(jmp_buf env); int sigsetjmp(sigjmp_buf env, int savesigs); // Returns: 0 from setjmp, nonzero from longjmps  void longjmp(jmp_buf env, int retval); void siglongjmp(sigjmp_buf env, int retval); // Never returns   setjmp函数将当前调用环境（Calling Environment，包括程序计数器、栈指针和通用寄存器等），保存在参数env指定的缓冲区中。longjmp函数会从env缓冲区恢复调用环境，然后触发最近调用的setjmp函数的返回。此时setjmp会返回一个非零值retval。而在信号处理程序中，我们使用sigsetjmp和siglongjmp代替它们。\n非局部跳转的一个重要应用是可以在检测到某些错误条件时，从深度嵌套的函数调用中立即返回。我们可以使用非本地跳转直接返回到常见的错误处理程序，而无需费力地展开栈（Unwind Stack）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41  #include \"csapp.h\" jmp_buf buf; int error1 = 0; int error2 = 1; void foo(void), bar(void); int main() { switch (setjmp(buf)) { case 0: foo(); break; case 1: printf(\"Detected an error1 condition in foo\\n\"); break; case 2: printf(\"Detected an error2 condition in foo\\n\"); break; default: printf(\"Unknown error condition in foo\\n\"); } exit(0); } /* Deeply nested function foo */ void foo(void) { if (error1) longjmp(buf, 1); bar(); } void bar(void) { if (error2) longjmp(buf, 2); }   示例程序中主函数首先调用setjmp保存当前调用环境，然后依次调用函数foo和bar。 一旦函数执行发生错误，它们会立即通过longjmp从setjmp返回。setjmp的非零返回值表示错误的类型，因此我们可以在代码中的某处对其进行处理。\n非局部跳转的另一个重要应用是从信号处理程序跳转到特定代码位置，而不是像往常那样返回到因信号中断的指令。\n","description":"","tags":["OS"],"title":"CSAPP 读书笔记：异常控制流","uri":"/posts/exception-control-flow-note/"},{"content":"存储系统（Memory System）可以分为多个层级（Hierarchy），每个层级的存储设备（Storage Device）具有不同的容量、成本和访问时间。层级越低，存储设备的容量就越大，成本也越低，但访问速度却越慢。存储系统的层次结构对应用性能有着显著的影响，而 CPU 与主存储器之间的高速缓存则尤为重要，因为它对程序性能的影响最大。\n存储技术 随机存取存储器 随机存取存储器（Random Access Memory，RAM）有两种，分别是静态（static）的 SRAM 和 动态（dynamic）的 DRAM。其中，SRAM 的访问速度比 DRAM 更快，不过其成本明显更高。SRAM 用于 CPU 芯片内外的缓存，而 DRAM 则用于主存储器和图形系统中的帧缓存（Frame Buffer）。\nSRAM SRAM 将每个位存储在一个双稳态（bistable）的存储单元中，每个单元是由一个六晶体管电路实现的，其结构类似于下图中的倒转摆锤：\n该电路将永远处于两种电压配置（称为状态）之一。任何其他的状态都是不稳定的，将会快速地朝向其中一个稳定的状态移动。因此只要 SRAM 通电，就会永久地保存其值而不受一些扰动（如电噪声）的影响。\nDRAM DRAM 将每一位存储为一个电容上的电荷，其存储单元对任何扰动都十分敏感。电流泄露会导致 DRAM 单元在大约 10 到 100 毫秒的时间内失去电荷，因此存储系统必须定期地通过读取并重写来刷新其中的每一位。\n传统的 DRAM DRAM 芯片上的位可以被划分为 $d$ 个超级单元（supercell），每个超级单元包含 $w$ 位。因此，一个 $d \\times w$ 的 DRAM 能够存储 $dw$ 位的信息。下图展示了一个 16 $\\times$ 8 的 DRAM 芯片结构：\n16 个超级单元被排列成一个 4 $\\times$ 4 的矩阵，图中标为阴影的超级单元地址为 (2, 1)。信息通过外部连接器（称为 pin）流入或流出芯片，每个 pin 都可以携带一位信号。上图中包含了能够传输一字节信息的八个data pin，以及携带两位超级单元地址的addr pin。\n每个 DRAM 芯片都连接了一个称为存储控制器（Memory Controller）的电路，它可以一次传输 $w$ 位的数据。为了读取超级单元 (i, j) 中的内容，存储控制器会向 DRAM 发送行地址 i 和 列地址 j 的寻址请求，分别称为 RAS（Row Access Strobe）请求和 CAS（Column Access Strobe）请求。详细的读取过程如下图所示：\n存储控制器首先发送行地址请求，DRAM 会将整个第 2 行拷贝到内部的行缓冲区（Internal Row Buffer）中。然后存储控制器再发送列地址请求，DRAM 将从行缓冲区中拷贝超级单元 (2, 1) 的值返回给存储控制器。\n只要对存储控制器读取 DRAM 的过程有所了解，我们就能明白超级单元的排列方式为什么是矩阵而非一维线性数组。在我们的例子中，DRAM 包含了 16 个超级单元。如果按一维数组排列，则超级单元的地址范围为 0 到 15，因此就需要四个 pin（四位）来进行寻址。当然，矩阵排列方式也有一定缺点。存储控制器读取超级单元需要分为两个步骤，从而增加了访问时间。\n存储模块 DRAM 芯片被封装在存储模块（Memory Modules）中，如下图所示：\n示例的存储模块包含 8 个 8M $\\times$ 8 DRAM 芯片，能够存储 64 MB 的数据。每个超级单元存储主存储器中的一字节（八位）信息，八个超级单元就可以表示一个地址为 A 的 64 位 word。存储控制器首先将地址 A 转换为超级单元地址 (i, j)，然后将其发送到存储模块中。存储模块会向所有的 DRAM 广播地址 i 和 j，从而得到每个 DRAM 中超级单元 (i, j) 的内容。存储模块中的电路会将结果整合成一个 64 位的 word，最终返回给存储控制器。\n改进的 DRAM 我们对传统的 DRAM 进行优化，从而设计出访问速度更快的 DRAM：\n FPM（Fast Page Mode）DRAM：传统 DRAM 将整行数据拷贝到缓冲区中，使用其中一个并丢弃剩余的超级单元。FPM DRAM 可以连续访问同一行的超级单元，因此速度更快。例如读取一行中的四个超级单元，传统 DRAM 需要发送四次 RAS 请求和四次 CAS 请求，而 FPM DRAM 则只需要一次 RAS 请求和四次 CAS 请求； EDO（Extended Data Out） DRAM：通过减少发送 CAS 信号的时间间隔来对 FPM DRAM 进行优化； SDRAM（Synchronous DRAM）：上文提及的所有 DRAM 都是异步（Asynchronous）地向存储控制器发送信号的，而同步的 SDRAM 输出超级单元的速率更快； DDDR（Double Data-Rate Synchronous）DRAM：使用时钟边缘（Clock Edge）作为控制信号来将 SDRAM 的速度提升一倍； VRAM（Video RAM）：常用于图形系统的帧缓存，其本质与 FPM DRAM 类似。区别在于 VRAM 通过依次对内部缓冲区的内容移位来输出数据，并且可以向内存 并行读写。  非易失性存储器 非易失性存储器（Nonvolatile Memory）在断电后还会继续保留其存储的信息。即使有些非易失性存储器是可读写的，但由于历史原因，它们被统一称为 ROM（Read-only Memory）。不同种类 ROM 的区别在于其能够被重新编程（写入）的最大次数，以及写入的实现机制。\n PROM（Programmable ROM）：只能被编程一次； EPROM（Erasable programmable ROM）：可以被擦除和重新编程约 1000 次； EEPROM（Electrically Erasable PROM）：与 EPROM 相比，不需要单独的物理编程设备，但只可以重新编程 105 次； 闪存（Flash Memory）：基于 EEPROM 的一项重要存储技术，SSD 就是通过闪存实现的。  存储在 ROM 中的程序通常称为固件（Firmware）。一些系统会在固件中提供简单的输入/输出功能，比如 PC 的 BIOS（Basic Input/Output System）。\n访问主存储器 数据通过共享的电气管道在 CPU 和 DRAM 主存储器之间流动，这些管道称为总线（Bus）。而数据传输的过程可以分成一系列的总线事务（Bus Transaction），其中读事务将数据从主存加载到 CPU 中，写事务则将数据从 CPU 传输到主存中。\n总线是一组可以传输地址、数据和控制信号的并行线的集合，其中控制线负责同步事务并识别当前正在执行事务的类型。下图展示了计算机系统中的总线结构：\n图中的 I/O Bridge 是一组芯片集合，包含了上文中提到的存储控制器。它会将系统总线的电信号转换为存储器总线的电信号，反之亦然。当 CPU 执行数据读取指令时，其芯片上的总线接口（Bus Interface）电路会在总线上初始化一个读事务，过程如下：\n写事务的过程与之类似：\n磁盘存储 磁盘（Disk）是主力存储设备，可以容纳成百上千 GB 的数据，但其读写速度却远远低于 RAM。本节中的磁盘主要指的是旋转磁盘，并不涉及 SSD。\n磁盘构造 磁盘是由盘片（Platter）组成的，每个盘片都有两面（Surface）。下图 6.9(a) \u0010展示了一个典型磁盘 Surface 的构造：\n每个 Surface 都由一组被称为 Track 的同心环构成，每条 Track 又可以被划分为多个扇区（Section）\u0010。每个扇区中都存储了数量相同的数据位（通常为 512 字节），它们被只保存了扇区标识的 Gap 所分隔。\n如上图 6.9(b) 所示，Cylinder 是所有 Surface 上与主轴等距的 Track 集合。\n磁盘容量 磁盘的容量由以下三个参数决定：\n 记录密度（$bits/in$）：一英寸长度的 Track 中存储的位数； 轨道密度（$tracks/in$）：从主轴中心处延伸一英寸半径内的 Track 数量； 面密度（$bits/in^2$）：记录密度和轨道密度的乘积。  在以前的磁盘中，每条 Track 上的扇区数量是相同的。这样当面密度上升之后，Gap 就会越来越大，从而造成浪费。因此现代大容量磁盘使用多区记录（Multiple Zone Recording）技术，将 Cylinder 分成多个不相交且连续的 Zone。一个 Zone 中每条 Track 所包含的扇区数量均等于该 Zone 中最内侧 Track 的扇区数量。\n磁盘操作 磁盘使用连接在传动臂（Actuator Arm）末端的读/写头来进行读写操作：\n传动臂沿自身旋转轴不断移动，可以将读/写头定位到任意 Track 上，这一操作称为寻道（Seek）。如图 6.10(b) 所示，拥有多个盘片的磁盘的每一个 Surface 都有其对应的读/写头。不过在任何时刻，所有的读/写头均处在相同的 Cylinder 上。传动臂的移动速度非常快，即使是微小的灰尘也会干扰到读/写头的寻道，因此磁盘需要密封包装。\n磁盘以扇区大小的块为单元进行读写，其访问时间受三个主要因素的影响：\n 寻道时间：即移动传动臂所需的时间； 旋转延时：当读/写头定位到目标 Track 上时，磁盘还需要将目标扇区的第一个位旋转到读/写头下，所需时间称为旋转延时； 传输时间：磁盘读写目标扇区内容所需的时间，由磁盘旋转速度和每条 Track 中的扇区数量所决定。  相比其他两个因素，传输时间小到可以忽略。而寻道时间和旋转延时大致相等，因此可以用两倍的寻道时间来估算磁盘的访问时间。\n磁盘的逻辑块 为了向操作系统隐藏磁盘构造的复杂性，现代磁盘将自身简化为一个由 B 个逻辑块组成的序列。每个逻辑块的尺寸与扇区大小相同，编号为 0，1，… ，B - 1。磁盘控制器（Disk Controller）负责维护逻辑块编号与实际物理磁盘扇区之间的映射关系，它会将操作系统读取的目标逻辑块编号转换为唯一标识物理扇区的三元组（Surface，Track 和扇区）。读取到的信息将首先保存在磁盘控制器的缓冲区中，然后再拷贝到主存。\n连接的 I/O 设备 显卡、显示器、鼠标、键盘和磁盘等输入/输出 (I/O) 设备使用 I/O 总线连接到 CPU 和主存储器上：\n访问磁盘 CPU 从磁盘中读取数据的过程如下：\nCPU 使用 Memory-mapped I/O 技术向 I/O 设备发送指令，如上图中 (a) 所示。系统会在地址空间中保留一块地址区用于与 I/O 设备的通信，其中的地址称为 I/O Port。每个连接到总线上的设备都会映射到一个或多个 I/O Port。\nCPU 会发送三个指令到目标 I/O Port 来初始化读取操作：\n 告知磁盘启动读取的命令 目标数据所在逻辑块的编号 数据存储的主存地址  在发出请求之后，CPU 通常会在磁盘读取时进行其他工作。如上图中 (b) 所示，设备自行执行读写总线事务，无需 CPU 的参与，这一过程称为 DMA（Direct Memory Access ）。当数据存储在主存中之后，磁盘控制器便会向 CPU 发送中断信号以通知磁盘读取完成，如上图中 (c) 所示。\nSSD 固态硬盘（Solid State Disk，SSD）是一种基于闪存的存储技术，其基本理念如下：\nSSD 由一个或多个闪存芯片以及闪存转换层（Flash Translation Layer）组成，前者代替了旋转磁盘中的机械驱动器，后者则与磁盘控制器的作用相同。\n闪存中包含了 B 个 Block，每个 Block 又可以分成 P 个 Page。Page 的大小通常在 512 字节到 4 KB 之间，而一个 Block 一般由 32 到 128 个 Page 组成，因此 Block 的大小为 16 KB 到 512 KB。\nSSD 以 Page 为单位进行读写，并且只有当 Page 所在的整个 Block 被擦除（即所有位均置为 1）后才能向其写入。不过，只要一个 Block 被擦除，其中的每一个 Page 都可以被写入一次而无需再次擦除。Block 会在大约 100,000 次重复写入后发生磨损，无法再被使用。\nSSD 的随机写入速度比读取慢，这是因为擦除 Block 需要相对较长的时间。另外，如果写操作尝试修改已有数据的 Page，那么就必须先将同一 Block 中其他任何包含有用数据的 Page 复制到另一个已被擦除的 Block 中。\n相比于旋转磁盘，SSD 的随机访问速度更快，消耗的功率更低，同时也更加坚固。\n局部性 优秀的计算机程序通常表现出良好的局部性（Locality），即程序多次引用相同的数据或最近引用数据附近的数据。前者称为时间局部性（Temporal Locality），后者称为空间局部性（Spatial Locality）。\n局部性在现代计算机系统的多个层级中都有着广泛的应用。如在硬件级别，我们引入高速缓存来加快对主存储器的访问速度。在操作系统级别，主存储器可以缓存磁盘文件系统中最近使用的 Block。在应用程序级别，Web 浏览器会将最近请求的文档缓存到本地磁盘上。\n引用程序数据的局部性 变量sum在每次循环中都被引用一次，因此上图中的两个函数都具有优秀的时间局部性。而对数组元素a[i][j]来说，第一个函数sumarrayrows会按照其在内存中存储的顺序依次读取，所以循环的空间局部性也十分良好。我们将每隔 k 个元素访问连续向量的模式称为 stride-k 引用，空间局部性随着 k 的增加而降低。显然，采用 stride-2 引用模式的函数sumarraycols的空间局部性较差。\n获取指令的局部性 我们还可以评估 CPU 读取内存中程序指令的局部性。循环体中的指令会按照存储顺序依次执行，因此循环具有良好的空间局部性。由于循环体中的指令会被多次迭代执行，所以程序的时间局部性也很优秀。与程序数据相比，指令在运行时很少被修改。\n存储系统层级 我们可以将存储在容量大而访问速度慢的设备中的数据对象暂存到容量相对较小但访问速度更快的区域中，这便是缓存（Cache）。下图展示了存储系统层级中的缓存概念：\n不同层级的设备之间以 Block 为单位传输数据，每个 Block 都有唯一的地址或名称。相邻层级设备之间的 Block 的大小一般是固定的，但也可以是变化的（如存储在 Web 服务器上的 HTML 文件）。不同层级使用的 Block 大小各不相同，如 L0 和 L1 之间的 Block 通常为一个 word，而 L1 和 L2 之间的 Block 则为四到八个 word。一般来说，层级越低的设备访问速度越慢，因此其使用的 Block 也会越大。\n缓存命中与缺失 当程序需要访问存储在第 k + 1 级设备中的数据对象 d 时，会首先在第 k 级设备中查找。如果 d 恰好缓存在第 k 级设备中，那么便称为缓存命中（Cache Hits）。反之则称为缓存缺失（Cache Misses），第 k 级设备会从第 k + 1 级设备中获取 d 所在的 Block。如果此时第 k 级设备已满，则现有的 Block 将会被覆盖（驱逐）。Block 的驱逐策略（Replacement Policy）有多种，比如随机替换，以及 LRU（Least Recently Used）。 LRU 策略会将被访问时间据现在最久远的 Block 驱逐。\n缓存缺失有多种不同类型。如果缓存为空，则访问任何数据对象都将发生缓存缺失，我们称其为强制缺失（Compulsory Misses）或冷缺失（Cold Misses）。\n每当出现缓存缺失时，第 k 级缓存会根据其放置策略（Placement Policy）将第 k + 1 级缓存中的 Block 拷贝到指定位置处。Block 的放置策略同样有多种，最简单的便是随机放置。它的缺点十分明显，因为随机放置的 Block 的寻址成本很高。我们可以将第 k + 1 级缓存中的 Block i 放置在第 k 级缓存中的 Block i mod n 处，该策略称为映射放置。若 n 为 4，则上图中第 k + 1 级缓存中的 Block 0、4、8 和 12 将映射到第 k 级缓存中的 Block 0，而第 k + 1 级缓存中的 Block 1、5、9 和 13 则将映射到第 k 级缓存中的 Block 1。\n映射放置的缺点是容易导致冲突缺失（Conflict Misses）。如果程序依次请求第 k + 1 级缓存中的 Block 0、4、8 和 12，由于它们均映射到第 k 级缓存中的 Block 0，因此即使缓存足以容纳四个 Block，也将连续发生缓存缺失。\n程序通常作为一系列阶段（如循环）运行，其中每个阶段都会访问一些恒定的缓存 Block 集合，称为该阶段的工作集（Working Set）。如果工作集超过了缓存大小，那么就将发生容量缺失（Capacity Misses）。\n缓存管理 不同级别的缓存管理是由硬件、软件或两者的组合实现的，如下图所示：\n缓存的实现细节 假设存在一个简单的存储系统层级结构，在 CPU 和主存储器之间只有一个 L1 级的高速缓存。我们将通过该模型介绍缓存的具体实现细节。\n通用缓存管理 假设我们模型中的每个内存地址都有 $m$ 位，则共有 $M = 2^m$ 个唯一地址。如下图所示，缓存是一个包含了 $S = 2^s$ 个缓存集（Cache Set）的数组，每个缓存集由 E 个缓存行（Cache Line）组成。每个缓存行中都有一个包含了 $B = 2^b$ 字节数据的 Block，一个标识该行是否存储了有效信息的一位有效位，以及一个唯一标识行内 Block 的 $t$ 位（$t = m -(b + s)$）标记位（Tag Bits）。\n一个缓存的结构通常可以用元祖 (S, E, B, m) 来表征，其容量为 $C = S \\times E \\times B$。\n直接映射缓存 缓存可以根据 E 的数量进行分类，E 为 1 的缓存称为直接映射缓存。当 CPU 想要读取内存中的某个 word 时，直接映射缓存将通过以下步骤确定缓存是否命中以及提取请求的 word：\n 集合选择（Set Selection） 行匹配（Line Matching） 字提取（Word Extraction）  我们可以将缓存看成一个以 Set Index 为索引的缓存集数组，示例中的 Set Index 为 $0001_2$，因此将选择缓存集 1。\n如果有效位置为 1 且缓存行与地址中的标记位匹配，则缓存命中。同样，我们可以将 Block 看作一个以 Block Offset 为索引的字节数组。示例中的 Block Offset 为 $100_2$，因此目标 word 的起始字节为 4，即上图中的 $w_0$。\n如果出现缓存缺失，则缓存将从以下一级设备中检索请求的 Block，然后存储在其 Set Index 指定的缓存集中。若当前缓存已满，考虑到直接映射缓存的缓存集中只有一个缓存行，因此对应的缓存行将被替换。\n缓存实现示例 假设一个直接映射缓存有四个缓存集，每个 Block 两个字节，地址均为四位，并且每个 word 都只有一个字节：\n 索引位（Index Bits）和标记位唯一标识了内存中的每个 Block； 不同的 Block 可以映射到相同的缓存集，如 Block 0 和 4； 映射到相同缓存集的 Block 是通过标记位进行区分的，如 Block 0 的标记位为 0，而 Block 1 的标记位为 1。  若 CPU 想要读取内存中的某些 word，上述缓存将会开展一系列的工作。不过在开始时，缓存是空的：\n   Set Valid tag block[0] Block[1]     0 0      1 0      2 0      3 0       当 CPU 读取地址 0 处的 word 时，缓存将从主存储器中获取 Block 0 并把它存储在 Set 0 中：\n   Set Valid tag block[0] Block[1]     0 1 0 m[0] m[1]   1 0      2 0      3 0       此后如果 CPU 想要读取地址 1 处的 word，则显然会发生缓存命中。而当 CPU 读取地址 13 处的 word 时，缓存将从主存储器中获取 Block 6 并把它存储在 Set 2 中：\n   Set Valid tag block[0] Block[1]     0 1 0 m[0] m[1]   1 0      2 0  m[12] m[13]   3 0       当 CPU 读取地址 8 处的 word 时，即使 Set 0 中的有效位为 1，但 tag 并不匹配，因此这是一种缓存缺失。缓存将从主存储器中获取 Block 4 并覆盖之前的缓存行，此后如果 CPU 想要再次读取地址 0 处的 word 就会出现冲突缺失。\n   Set Valid tag block[0] Block[1]     0 1 0 m[8] m[9]   1 0      2 0  m[12] m[13]   3 0       冲突缺失 直接映射缓存中的冲突缺失常发生在程序访问长度为 2 的幂的数组时，如：\n1 2 3 4 5 6 7 8  float dotprod(float x[8], float y[8]) { float sum = 0.0; int i; for (i = 0; i \u003c 8; i++) sum += x[i] * y[i]; return sum; }   该程序具有良好的局部性，但在直接映射缓存中易发生冲突缺失。假设缓存有两个缓存集，每个 Block 可以容纳 16 个字节的数据。因此缓存的容量为 32 字节，可以完整的保存整个数组。数组x[8]和y[8]中的元素与缓存集的映射关系如下：\n由于两数组中索引相同的元素均映射到相同的缓存集，因此每当循环中的指令读取数组元素时就会发生冲突缺失。我们将这种情况称为颠簸（Thrashing），即缓存重复加载并驱逐相同缓存集中的 Block。\n一种简单的解决方案是在每个数组末尾填充若干个字节。如将数组x[8]重新声明为x[12]，则映射关系就变成了：\n显然，此时x[i]与y[i]映射到了不同的缓存集，颠簸将不再出现。\n细心的读者可能会想到，为什么 Set index 在地址的中间位而非高位？如下图所示，若 Set index 位于地址的高位，那么一些连续的 Block 将映射到相同的缓存集。只要程序按顺序读取数组中的元素，缓存就会发生颠簸。\n集关联型缓存 集关联型缓存（Set Associative Caches）中的每个缓存集可以包含多个缓存行。我们通常将包含 E （1 \u003c E \u003c C/B）个缓存行的缓存称为 E-路集关联型缓存，下图则是一个二路集关联型缓存的结构：\n其读取 word 的步骤与直接映射缓存类似，区别主要在于行匹配上。集关联型缓存必须检查多行的有效位和标记位是否与请求的 word 相匹配：\n一旦发生缓存缺失，缓存将首先从主存储器中提取所需的 Block。如果此时缓存中没有空行，则必须根据策略替换已有的缓存行。最简单的方法便是随机替换，但我们应当利用局部性尽可能地减小被替换的行未来再次被使用的可能。除了上文提到的 LRU 以外，我们还可以采用LFU（Least Frequently Used）策略，替换过去某个时间段内被引用次数最少的行。\n全关联型缓存 全关联型缓存（Fully Associative Caches）只有一个缓存集，其中包含多个缓存行（E = C/B）：\n其读取 word 的步骤与其他两种缓存类似，不过 word 的地址将不包含 Set Index：\n对于全关联型缓存，缓存电路必须并行地匹配多行标记位。实现一个容量大且速度快的全关联型缓存是十分困难而又昂贵的，因此它仅适用于小型缓存。\n缓存的写入 相比于读取，写入的情况要更加复杂，因为我们还要考虑如何更新低级别缓存中的副本。最简单的方法便是直接写入（write-through），但这样每次写入操作都会消耗总线的流量。另一种方法称为回写（write-back），即只有当更新的 Block 被驱逐时才向低级别写入。不过回写也会增加系统的复杂性，缓存必须为每个缓存行维护一个脏位（Dirty Bit），标识该 Block 是否已被修改。\n还有一个问题是如何处理写入时的缓存缺失。一种方法会将目标 Block 从低级别加载到缓存中后再进行更新，称为写分配（write- allocate）。该方法试图利用写入的空间局部性，但缺点是每当发生缓存缺失便要传输 Block。另一种方法会绕过缓存直接在低级别中修改目标 Block，称为无写分配（no-write-allocate）。直接写入通常使用无写分配，而回写则使用写分配。\n对于试图编写缓存友好型代码的程序员来说，我们推荐采用回写和写分配的处理方式。尤其是在低级别的缓存（如虚拟内存）中，传输数据的时间较长，因此更容易体现出回写的优势。\n真实缓存层级剖析 上图中的i-cache代表保存指令的缓存，d-cache代表保存程序数据的缓存，而unified cache则代表既存储指令又存储数据的缓存。有趣的是，所有的 SRAM 缓存都包含在 CPU 芯片中。\n缓存参数对性能的影响 缓存的性能指标主要有：\n 缺失率（Miss Rate）：# missed / # references 命中率（Hit Rate）：1 - Miss Rate 命中时间（Hit Time）：CPU 从缓存中读取一个 word 所需的时间 缺失惩罚（Miss Penalty）：因为缓存缺失而增加的读取时间  缓存的不同参数对其性能的影响为：\n 缓存容量：更大的缓存可以增加命中率，但也会增加命中时间； Block 的大小：我们可以利用空间局部性通过增大 Block 而提升命中率。但在缓存容量一定的情况下，Block 越大，缓存行的数量就越少。若程序具有更多的时间局部性，则命中率反而会下降。另外，更大的 Block 还会增加数据传输的时间； 关联性（缓存行的数量）：更多的缓存行可以避免因冲突缺失而导致的颠簸，但维护更多的缓存行也需要大量成本。  编写缓存友好型代码 编写缓存友好型代码的两个基本准则如下：\n 关注核心程序的内部循环并忽略其他部分； 减少每个内部循环中缓存缺失的数量。  其中第二个准则又可以根据局部性分为两方面：\n 最大化空间局部性：采用 stride-1 模式，按数据对象在内存中存储的顺序依次读取； 最大化时间局部性：一旦从内存中读取了某个数据对象，就尽可能多地使用它。  缓存对程序性能的影响 ","description":"","tags":["OS"],"title":"CSAPP 读书笔记：存储系统的层级结构","uri":"/posts/the-memory-hierarchy-note/"},{"content":"处理器支持的指令及其对应的字节编码方式称为指令集架构（Instruction Set Architecture，ISA）。不同的处理器系列，例如 Intel IA32/x86-64，IBM/Freescale Power 和 ARM 等，均使用不同的 ISA。为一种机器编译得到的程序，无法在 ISA 不同的机器上运行。\nISA 在编译器的编写者和处理器的设计者之间提供了一个抽象层。编译器的编写者只需要知道允许使用哪些指令以及它们是如何编码的，而处理器的设计者则需要创建能够执行这些指令的机器。\n在传统的 ISA 模型中，每条指令按顺序执行。而现代处理器则会同时执行多条指令的不同部分，从而获得比一次只执行一条指令更高的性能。不过我们还需要引入一个特殊机制，确保处理器的计算结果与顺序执行相同。\n在本章中，我们仿照 x86-64 创建了一个简单的指令集，称为“Y86-64”。同时使用 HCL（Hardware Control Language）来描述硬件系统的控制部分和处理器设计，它的作用类似于 Verilog HDL（Hardware Description Language）。\nY86-64 指令集架构 程序员可见状态 Y86-64 程序中的每条指令都可以读取和修改处理器状态的某些部分，即程序员可见状态。此处的“程序员”既是用汇编代码写程序的人，又是生成机器代码的编译器。如下图所示：\nY86-64 中的程序员可见状态包括 15 个能够存储 64 位数据的寄存器、3 个单位（singel bit）条件码、程序计数器（PC）、虚拟内存和表示程序执行整体状态的状态码（Stat）。\nY86-64 指令 Y86-64 指令是 x86-64 指令的子集，仅包含了 8 字节的整数运算，以及更少的寻址和运算模式。由于我们的数据均为 8 字节，因此可以无歧义地将它们称为字（word）。\nx86-64 中的movq指令被分成了四个不同的 Y86-64 指令：irmovq、rrmovq、mrmovq和rmmovq。其中，i 代表立即数，r 代表寄存器，m 代表内存。\n上图还缺少了部分信息，比如整型操作指令OPq实际上包含了四个指令：addq、subq、andq和xorq，它们会根据计算结果改变三个条件码的值。另外，该类指令和条件分支指令jXX以及条件移动指令cmovXX编码中的 $f_n$ 将随指令的具体名称变化：\ncall、ret、nop、pushq和popq指令的作用和 x86-64 中的类似，而halt指令则对应了 x86-64 中的hlt。x86-64 不允许程序使用htl指令，因为它会使整个系统暂停操作。不过在 Y86-64 中，halt指令会使处理器停止并将状态码置为 HLT。\n指令编码 上一节的图中还展示了不同指令的字节编码，其长度范围在一到十字节之间，其中的第一个字节标识了其类型。程序寄存器存储在 CPU 中的寄存器文件（一个小型的 RAM）中，下图中的寄存器 ID 就是它们的地址：\n寄存器 ID 将替换指令编码中的 $r_A$ 和 $r_B$。x86-64 中条件分支和跳转指令的目的地址是相对于程序计数器（PC-Relative）的，这样可以使程序编码更加紧凑，同时代码在内存中移动时也无需更改所有分支目标的地址。由于我们更加关心设计的简洁性，因此 Y86-64 采用绝对地址。\n举例来说，在一个小端机器上，指令 rmmovq %rsp, 0x123456789abcd(%rdx)的字节编码为4042cdab896745230100。其中，指令类型rmmovq对应的字节为40，寄存器 %rsp 和 %rdx 分别对应4和2。剩下的立即数将首先填充为八字节的000123456789abcd，然后反向追加到指令尾部。\n任何指令集中的字节编码都必须与指令序列唯一对应。也就是说，只要我们知道了指令字节编码中的首个字节，就能确定完整的指令序列。反之如果我们无法得知字节序列的起始位置，那么也就无法将其拆分为多个单独的指令。\nY86-64 异常 Y86-64 中的状态码 Stat 如下：\n我们没有引入异常处理程序（Exception Handler），只是简单地让处理器在遇到任何异常时停止执行指令。\nY86-64 程序 示例 C 程序如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  long sum(long *start, long count) { long sum = 0; while (count) { sum += *start; start++; count--; } return sum; } int main() { long array[4] = {0x000d000d000d, 0x00c000c000c0, 0x0b000b000b00, 0xa000a000a000}; sum(array, 4); return 0; }   与之对应的 Y86-64 汇编代码为：\n从中我们可以看出它与 x86-64 汇编代码的区别：\n 由于我们的算术运算无法直接使用立即数，因此需要先将其拷贝到寄存器中（第 24-25 行）； 我们需要先从内存中读取值（第 30 行），然后再将其与寄存器中的值相加（第 31 行）。而 x86-64 中只需要使用一个addq指令； 我们的subq指令（第 33 行）在执行减法运算的同时还会修改条件码，因此我们可以在不引入testq的情况下直接使用条件跳转指令jne（第 35 行）。不过为了实现这一点，我们必须在进入循环之前使用andq指令初始化条件码的值（第 27 行）。  汇编器会根据上图中以“.”开头的指令调整其生成的代码地址，如指令.pos 0（第二行）表示代码的起始地址为 0。指令irmovq stack, %rsp会初始化栈指针，其地址是由最后两行指令声明的。运行时栈将从0x200开始向较低的地址处增长，因此我们需要保证它不会覆盖到其他的程序数据。程序的第 8 到 13 行声明了一个四字数组，.align 8指令将使它在 8 字节边界上对齐。\n一些 Y86-64 指令细节 指令pushq %rsp压入栈的值可能是寄存器 %rsp 的原始值，也有可能是栈指针减少后的值。而指令popq %rsp从栈中弹出的值可能是直接从内存中读取的，也有可能是栈指针增加后再读取的。为了避免混淆，我们需要规定上述两个指令均采用前者的方式。\n逻辑设计和 HCL 在硬件设计中，电子电路用于计算位函数（Function on Bits）并将位存储在不同的存储器元素（Memory Element）中。我们可以使用高电压（约 1.0 V）来表示逻辑值 1，使用低电压（约 0.0 V）来表示逻辑值 0。\n数字系统主要由以下三个部分构成：\n 计算位函数的组合逻辑（Combinational Logic） 存储位的存储器元素 控制存储元素更新的时钟信号（Clock Signal）  逻辑门 逻辑门是数字电路的基本计算元素，其输出等于输入的位进行布尔运算后的结果。上图展示了用于布尔函数AND、OR和NOT的标准符号，逻辑门的下方则是对应的 HCL 表达式\u0026\u0026、||和!。逻辑门只对单个位进行操作而非整个字（word），因此我们不使用 C 中的位级运算符\u0026、|和~。\n组合电路和 HCL 布尔表达式 将多个逻辑门组合成一个网络，称为组合电路。其构建方式有以下要求：\n 每个逻辑门的输入必须是：  整个系统的输入之一（即主输入） 某个存储器元素的输出 某个逻辑门的输出   两个或多个逻辑门的输出不能连接到一起 网络不能是一个回路（acyclic）  下图展示了一个多路复用器（multiplexor，MUX）的组合电路：\n输入的数据信号是位 a 和 b，控制信号是位 s。当 s 为 1 时，输出将等于 a。而当 s 为 0 时，输出则为 b。输出信号的 HCL 表达式为：bool out = (s \u0026\u0026 a) || (!s \u0026\u0026 b);\nHCL 表达式和 C 中的逻辑表达式之间存在一些差异：\n 逻辑门和 HCL 的输出会动态地随输入的变化而变化，而 C 中的表达式只会在程序执行过程中运算； C 中的逻辑表达式允许使用任意整型参数，而 HCL 表达式只能对位值 0 和 1 进行运算； C 中的逻辑表达式可能只会执行部分运算。例如(a \u0026\u0026 !a) \u0026\u0026 func(b,c)，由于(a \u0026\u0026 !a)一定为 0，整个表达式的值也为 0，因此不会计算func(b,c)的值。相比之下，HCL 没有任何的部分评估规则。   To be continued …\n ","description":"","tags":["ISA"],"title":"CSAPP 读书笔记：处理器架构","uri":"/posts/processor-architecture-note/"},{"content":" 原文链接：Fabian Reinartz. Writing a Time Series Database from Scratch. fabxc.org, 2017.\n Prometheus 是一个包含了自定义时间序列数据库的监控系统，其查询语言、操作模型以及一些概念性决策使得它易于与 Kubernetes 集成。然而，Kubernetes 集群中的工作负载是动态变化的，有可能给它带来一定的压力。因此，我们致力于提高 Prometheus 在这些运行着高度动态或瞬态服务的环境中的性能。\n过去，单台 Prometheus 服务器每秒能够拉取多达一百万个样本（Sample），并且只占用非常少的磁盘空间。虽然它的性能十分卓越，但仍有改进空间。因此我提出了一种全新的存储系统设计，它可以解决当前方案的痛点，让 Prometheus 具备处理更大规模数据的能力。\n问题，问题和问题空间 首先，我们简要介绍 Prometheus 需要完成的任务及其引发的关键问题。对于每个方面，我们都会讨论当前方案做得好的地方，以及做得不好亟待新方案解决的地方。\n时间序列数据 Prometheus 随时间不断采集数据点：\n1  identifier -\u003e (t0, v0), (t1, v1), (t2, v2), (t3, v3), ....   每个数据点都是一个由时间戳和数值组成的元组。其中，时间戳是一个整型，而数值则是 64 位浮点数。一系列带有严格单调递增时间戳的数据点称为 Series，它可以由含有指标名称和标签字典的标识符（identifier）来寻址。一组典型的 Series 标识符如下：\n1 2 3  requests_total{path=\"/status\", method=\"GET\", instance=”10.0.0.1:80”} requests_total{path=\"/status\", method=\"POST\", instance=”10.0.0.3:80”} requests_total{path=\"/\", method=\"GET\", instance=”10.0.0.2:80”}   指标名称也可以被视为一个标签，如_name_，因此我们可以对这种表达方式进行简化。在查询时它可能会被特殊处理，但存储时则与其他标签无异。\n1 2 3  {__name__=\"requests_total\", path=\"/status\", method=\"GET\", instance=”10.0.0.1:80”} {__name__=\"requests_total\", path=\"/status\", method=\"POST\", instance=”10.0.0.3:80”} {__name__=\"requests_total\", path=\"/\", method=\"GET\", instance=”10.0.0.2:80”}   当查询时间序列数据时，我们通常会根据标签选择所需的 Series。一个最简单的例子，{__name__=\"requests_total\"}会查询属于requests_total指标的所有 Series，Prometheus 将拉取指定时间窗口内的数据点。\n有时候我们还希望一次查询能够选取满足多个标签选择器的 Series，或者在标签匹配中使用比等式更加复杂的条件。例如，否定 (method!=\"GET\") 或正则表达式匹配(method=\"PUT|POST\")。\n本节介绍的内容在很大程度上定义了 Prometheus 存储和调用数据的方式。\n垂直和水平 在简化的视图中，所有数据点都分布在一个二维平面上。其中，水平维度代表时间，垂直维度代表 Series 的标识符：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  series ^ │ . . . . . . . . . . . . . . . . . . . . . . {__name__=\"request_total\", method=\"GET\"} │ . . . . . . . . . . . . . . . . . . . . . . {__name__=\"request_total\", method=\"POST\"} │ . . . . . . . │ . . . . . . . . . . . . . . . . . . . ... │ . . . . . . . . . . . . . . . . . . . . . │ . . . . . . . . . . . . . . . . . . . . . {__name__=\"errors_total\", method=\"POST\"} │ . . . . . . . . . . . . . . . . . {__name__=\"errors_total\", method=\"GET\"} │ . . . . . . . . . . . . . . │ . . . . . . . . . . . . . . . . . . . ... │ . . . . . . . . . . . . . . . . . . . . v \u003c-------------------- time ---------------------\u003e   这些数据点是由 Prometheus 周期性地拉取一组 Series 的当前值而得到的。由于该操作对每个数据源实体（称为 Target）均独立完成，因此 Prometheus 的写入模式是完全垂直且高度并发的。\n假设我们的数据规模为：单个 Prometheus 实例从数万个 Target 中采集数据点，而每个 Target 又暴露成百上千个不同的 Series。在这种数据量达到百万级别的情况下，批量写入是必需的。\n对于旋转磁盘来说，随机写入数据非常缓慢，因为它需要不断移动磁头来寻址。而对于 SSD，尽管其随机写操作很快，但是它只能以 4KiB 或更大的 Page 为单位写入。也就是说，写入一个 16 字节的样本其实相当于写入一个完整的 4KiB Page。这种现象属于写放大（Write Amplification）的一部分，将会导致 SSD 的磨损和性能下降，甚至在几天或几周内摧毁你的磁盘。有关该问题的详细信息，可以参考系列文章 Coding for SSDs。综上所述，无论对于旋转磁盘还是 SSD，顺序写入和批量写入均是最理想的模式，这是我们必须坚持的原则。\n查询模式则与写入模式完全不同。我们可以查询一个 Series 中的单个数据点，一万个 Series 中的单个数据点，一个 Series 中一周内的数据点以及一万个 Series 中一周内的数据点等等。在二维数据平面中，查询的数据点既不是完全垂直或完全水平的，而是两者的矩形组合。\n我们可以使用 Recording rules 来改善执行常用查询语句时遇到的性能问题，但它对临时性的查询并不起作用。\n 译者注：关于 Recording rules，除了原文给出的文档链接外，还可以参阅 Today I Learned: Prometheus Recording Rules 一文。\n 当 Prometheus 批量写入时，每个批次（Batch）的数据点分布在垂直方向上的多个 Series 中。而如果我们查询某个时间窗口内的某个 Series 的数据点时，不仅很难找出每个点在磁盘上的位置，还必须从磁盘上随机读取数据。每次查询可能涉及到数百万个样本，即使在最快的 SSD 上进行也很慢。虽然每次查询请求的样本大小可能只有 16 字节，但读取操作会从磁盘上检索更多的数据。对于 SSD 是一个 Page，而对于 HDD 则是整个扇区。无论如何，我们都在浪费宝贵的吞吐量。\n因此在理想情况下，同一个 Series 中的样本按顺序存储。这样只要我们知道某个 Series 的起始位置，就可以快速访问它所有的数据点，从而减少读取操作的次数。\n将数据写入磁盘的理想模式和能够显著提升查询效率的设计之间显然存在着强烈的矛盾，这是我们的时序数据库必须解决的根本问题。\n 译者注：原文为：“There’s obviously a strong tension between the ideal pattern for writing collected data to disk and the layout that would be significantly more efficient for serving queries. It is the fundamental problem our TSDB has to solve.” 译者不确定此处的 tension 该如何翻译，猜测原文作者可能是想表达一种类似 trade-off 的概念。因为上文提到，在理想的写入模式中，数据点是垂直分布的。而通常查询得到数据点却是水平，甚至是矩形的。\n 当前解决方案 是时候看看 Prometheus 当前的存储系统（我们称之为 “V2”）是如何解决这一问题的。我们为每个 Series 创建一个文件，其中所有的样本均按时间顺序排列。由于每隔几秒就将样本追加写入到这些文件末尾的成本很高，我们先将样本缓存到内存中的 Chunk。每个 Series 都有一个对应的 Chunk，待 Chunk 被写满（即大小达到 1 KiB）之后再添加到文件尾部。这种方案既实现了批量写入，又将样本按顺序存储，解决了很多问题。一般来说，同一个 Series 中相邻样本的数值变化较小，因此可以使用非常高效的数据压缩格式。一篇关于 Gorilla TSDB 的 论文 介绍了一种类似基于 Chunk 的方法和压缩格式，能够将 16 字节的样本数据压缩到平均 1.37 字节。V2 版本的存储系统使用多种压缩格式，其中就包括 Gorilla 的变体。\n尽管基于 Chunk 的方法很棒，但为每个 Series 维护一个独立文件将给 V2 存储系统带来很多麻烦：\n 实际上我们所需的文件数量远比当前收集到的 Series 多得多，原因参见 Series Churn 章节。上百万个文件迟早会耗尽我们文件系统上所有的 inodes，只能通过重新格式化磁盘来恢复。 即使我们引入了 Chunk，每秒也会有数千个 Chunk 被写满并准备持久化，这意味着每秒发生数千次独立的磁盘写入。虽然可以通过将一个 Series 中几个已完成的 Chunk 批量落盘来改善这一问题，但这样做反而增加了等待持久化的 Chunk 数量，因此会占用更多的内存。 为了读写而保持所有文件均处于打开状态是不可行的。尽管约 99%的数据在 24 小时后就不再被查询，但只要查询到已持久化的样本，就必须打开数千个文件然后将结果读入内存中，最后再关闭它们。因为这种操作极大地提高了查询的延时，Prometheus 将缓存更多的 Chunk ，从而导致 资源消耗 章节中提到的问题。 最后我们必须删除旧数据。它们存储在数百万个文件的头部中，因此删除其实是一种写入密集型操作。此外，遍历数百万个文件并对其进行分析通常需要数个小时，可能刚一完成就不得不重新开始。没错，删除旧文件还将进一步地导致 SSD 的写放大。 当前累积的 Chunk 均存储在内存中，如果 Prometheus 发生崩溃数据就会丢失。为了避免这一情况，内存状态将周期性地保存（Checkpoint）到磁盘中。然而，完成 Checkpoint 的所需时间可能远比我们能够接受的数据丢失时间窗口还长。同时恢复 Checkpoint 一般长达几分钟，使 Prometheus 的重启周期变得非常漫长。  现有设计的关键概念是 Chunk，我们当然希望保留它。将最新的 Chunk 始终保持在内存中也是一个很好的设计，毕竟近期的数据查询频率最高。不过，为每个 Series 都创建一个文件的方案看起来并不合理，我们希望能够找到新的方案代替它。\nSeries Churn 在 Prometheus 中，Series Churn 表示一组 Series 变得不活跃。即新的数据点不再由它们接收，而是关联到一组新出现的 Series。例如，一个微服务暴露的所有 Series 都有一个对应的“实例”标签。如果我们对该微服务进行滚动更新并将每个实例替换为新版本，Series Churn 就会发生。在更加动态的环境中，这种现象甚至可能每小时就出现一次。集群编排系统（如 Kubernetes）允许应用连续地自动扩展和频繁地滚动更新，因此每天可能将有上万个实例以及相关的 Series 被创建。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  series ^ │ . . . . . . │ . . . . . . │ . . . . . . │ . . . . . . . │ . . . . . . . │ . . . . . . . │ . . . . . . │ . . . . . . │ . . . . . │ . . . . . │ . . . . . v \u003c-------------------- time ---------------------\u003e   由于 Series Churn 的存在，即使整个基础架构的规模保持不变，Series 数量也会随时间线性增长。虽然 Prometheus 能够收集多达 1000 万个 Series 的数据，但要让它从十亿个 Series 中查找数据还是十分困难的。\n当前解决方案 V2 存储系统为当前存储的所有 Series 分配了一个基于 LevelDB 的索引。它允许查询包含给定标签对的 Series，但缺少一种可扩展的方式来对不同标签选择的结果进行组合。\n例如，通过标签__name__=\"requests_total\"选取所有的 Series 十分高效，但使用instance=\"A\" AND __name__=\"requests_total\"就会遇到扩展能力的问题。稍后我们将再次讨论导致这种现象的原因，以及想要改善查询性能所必须做出的调整。\n这个问题实际上是我们寻找更好的存储系统的最初原因，Prometheus 需要一种更加完善的索引方法以从数亿个 Series 中快速搜索数据。\n资源消耗 当尝试扩展 Prometheus（或其他任何东西）时，资源消耗是贯穿始终的主题之一。但是，真正困扰用户的并非是绝对的资源不足。实际上，Prometheus 通常能够满足用户所需的吞吐量，问题在于其面对变化时的不稳定性和不可预知性。V2 存储系统为样本数据缓慢地创建 Chunk，内存使用量随时间的推移而持续上升。待 Chunk 写满后，它们会被写入磁盘并从内存中驱逐。最终，Prometheus 的内存使用量将达到一个稳定的状态。但是一旦监控的环境发生变化——扩展应用或滚动更新时，Series Churn 就会增加内存、CPU 和磁盘的使用。\n如果变化持续进行，资源消耗将再次达到一个稳定的状态，不过明显要比静态环境中的高。过渡周期通常长达数个小时，因此我们很难确定最大的资源使用量。\n为每个 Series 维护一个文件的方案也会使单个查询操作很容易终止 Prometheus 的进程。当查询未被缓存的数据时，与之关联的 Series 文件需要被打开并将包含相关数据点的 Chunk 读入内存。如果数据量超过了内存配额，Prometheus 就会因 OOM 而相当不雅地退出。加载的数据可以在查询结束后释放，但为了后续能够更快地查询相同数据，通常要缓存更长的时间。\n我们在上文中提到了 SSD 的写放大，以及 Prometheus 是如何通过批量写入来解决这一问题的。尽管如此，在一些场景中——如写入的批次（Batch）太小或者数据没有与 Page 的边界精确对齐，写放大还是会产生。我们已经在一些规模较大的 Prometheus 服务器上观测到了硬件寿命缩短的现象，虽然这对写入吞吐量较高的数据库应用来说是正常的，但还是应当思考如何缓解它。\n重新开始 到目前为止，我们已经对 Prometheus 需要解决的问题、V2 存储系统的设计方案及其缺点有了充分的了解。许多 V2 存储系统存在的不足可以通过一些优化和部分重新设计来改善，但为了让事情变得更有趣（当然也经过了仔细评估），我决定从零开始编写一个完整的时序数据库。\n存储模式将直接影响到 Prometheus 的性能和资源消耗等关键问题，因此我们必须为数据找到正确的算法集和磁盘设计方案。这就是我能够免走弯路而直接找到解决方案的原因。\nV3——宏观设计 当我们在 Prometheus 的数据目录下使用tree命令时，就可以看到 V3 存储系统的宏观层级结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  $ tree ./data ./data ├── b-000001 │ ├── chunks │ │ ├── 000001 │ │ ├── 000002 │ │ └── 000003 │ ├── index │ └── meta.json ├── b-000004 │ ├── chunks │ │ └── 000001 │ ├── index │ └── meta.json ├── b-000005 │ ├── chunks │ │ └── 000001 │ ├── index │ └── meta.json └── b-000006 ├── meta.json └── wal ├── 000001 ├── 000002 └── 000003   最上层目录是一系列经过编号的 Block，其前缀均为b-。每个 Block 中都有一个索引文件index和一个包含若干编号文件的chunks目录，该目录中保存了多个 Series 数据点的原始 Chunk。和 V2 存储系统一样，这种设计可以减少读取一个时间窗口内的 Series 数据的性能开销，并支持使用相同的高效压缩算法。基于 Chunk 的理念已经被证明是行之有效的，因此我们将沿用下去。不过，现在的存储系统不再是每个文件对应一个 Series，而是由几个文件保存多个 Series 的 Chunk。\n索引文件index的存在不足为奇。让我们假设它拥有诸多黑魔法，可以用于查找标签及其可能的值、整个时间序列以及存储数据点的 Chunk。\n但为什么要设计成若干个包含索引和多个 Chunk 文件的目录？为什么最后一个 Block 中还存在一个wal目录？只要理解了这两点，就能解决我们 90%的问题。\n多个小型数据库 我们将二维数据平面在水平维度（即时间）上划分为多个互不重叠的 Block，每个 Block 均表现为一个包含其时间窗口内所有 Series 数据的独立数据库。因此，Block 中存在自身的索引和多个 Chunk 文件。\n每个已持久化的 Block 都是不可改变的，但我们还需要一个可变的 Block（即上图中的 Mutable Block）来接收新的样本数据。该 Block 是一个能够高效更新数据结构的内存数据库，并有着与已持久化的 Block 相同的查询特性。为了防止数据丢失，所有刚采集到的数据都会另外写入临时的预写日志（Write Ahead Log）中。它实际上就是wal目录中的一组文件，可以在 Prometheus 重启时恢复原有的内存数据库。\n现在我们可以根据每个 Block 对应的时间范围将查询请求分发到各个 Block 中，最终的查询结果是由每个 Block 的返回值合并而成的。\n这种设计的优势在于：\n 当查询某一时间范围内的数据时，我们可以轻易地忽略该时间范围外的所有 Block。随着查询开始时检索的数据集数量的减少， Series Churn 带来的问题迎刃而解； 当一个 Block 写满后，我们可以通过顺序写多个大文件的方式从内存数据库中持久化数据。这样 Prometheus 就避免了任何的写放大问题，并且能在 SSD 和 HDD 上表现得同样出色； 我们保留了 V2 存储系统中的优点，即查询最为频繁的 Chunk 始终保存在内存中； 我们不再需要为了数据对齐而限制 Chunk 的大小为固定的 1KiB，现在可以选择对于单个数据点和所选压缩格式最合适的任意大小； 删除旧数据的开销变得很小并且能够快速完成，因为我们只需要简单地删除一个目录。要知道在 V2 存储系统中，我们必须分析和重写多达数亿个文件，可能需要花费数个小时。  每个 Block 中还存有一个meta.json文件，它只保存一些与 Block 相关的可读信息，因此我们便可以轻松了解存储状态以及 Block 中包含的数据。\n 译者注：从 Prometheus v2.19.0 开始，Mutable Block 便不再全部存储在内存中，详见：Prometheus TSDB (Part 1): The Head Block。\n mmap 既然已持久化的数据从数百万个小文件变成了若干个大文件，那么我们就能够以很低的开销保持所有文件均处于打开状态。在这种情况下，我们可以引入系统调用 mmap，将文件内容透明地映射到虚拟内存区域中。mmap 有些类似于交换分区（Swap Space），只不过我们所有的数据都已经在磁盘上了，在数据换出内存时不会发生写入。\n这意味着我们可以把数据库中的所有内容都视为在内存中，而实际上却没有占用任何物理内存（RAM）。只有我们访问数据库文件中指定的字节范围时，操作系统才会从磁盘中懒加载 Page。我们让操作系统来负责所有已持久化数据的内存管理，因为它对整个机器和其中的进程有着全面的了解。为了处理查询请求，更多的数据将被缓存到内存中，但它们会在内存压力较大时被操作系统驱逐。如果机器中还有内存未被使用，Prometheus 甚至可以把整个数据库缓存到内存中。而一旦另一个应用需要使用这部分内存，Prometheus 便会立刻返还给它。\n因此，查询比 RAM 容量更多的已持久化数据很容易导致进程的 OOM。内存中缓存的大小变得完全自适应，只有在需要响应查询请求时才会加载数据。\n据我所知，如今大多数的数据库均采用这种设计。如果磁盘格式允许，这是一种理想的工作模式——除非有人有信心在管理进程方面胜过操作系统。\n压缩 存储系统必须定期“切割”出一个新的 Block，然后将前一个 Block 写入到磁盘中。只有当它成功持久化后，对应的预写日志文件才能被删除。\n每个 Block 覆盖的时间范围不能太长（默认设置为两小时），否则将占用过多内存。但查询多个 Block 时，我们必须把每个返回结果合并到一起。这种操作显然会消耗性能，因此 Block 覆盖的时间范围也不能太短。一般来说，查询一周内数据所需要合并的 Block 数量不应超过 80 个。\n为了同时实现这两点，我们对 Block 进行压缩，即将一个或多个 Block 中的数据写入到一个有可能更大的 Block 中。Prometheus 在压缩过程中还会修改现有数据，比如删除已被标记为即将删除的数据，或者为了提高查询性能而重新构建样本的 Chunk。\n在上图中，几个 Block 被顺序编号为 [1, 2, 3, 4]。Block 1、2 和 3 可以被压缩到一起，变成 [1, 4]。也可以将它们两两压缩，变成 [1, 3]。所有时间序列数据仍然存在，但现在 Block 的总数减少了。这样查询时需要被合并的返回结果就更少，从而显着地降低了合并操作的开销。\n保留 在 V2 存储系统中，删除旧数据是一个非常缓慢的过程，并且会花费大量的 CPU、内存和磁盘资源。而现在，我们只需要删除那些在指定保留时间窗口内没有数据的 Block 目录即可。下方示例中，Block 1 可以被安全地删除，Block 2 则必须等到它完全超出保留边界（Retention Boundary）后才能被删除：\n随着旧数据的不断积累，压缩后的 Block 会变得越来越大。我们必须为其最大值设置上限，否则它将增长到包含整个数据库，从而很难被删除。对于像上图中 Block 2 这样的跨越保留边界的 Block，这种做法也限制了维护它们所需的磁盘开销。当 Block 的最大尺寸被设为保留时间窗口的 10%时，维护 Block 2 的成本也会受到同样的限制。\n总之，删除旧数据的开销从非常昂贵变成了几乎免费。\n 如果你已经读到这里并掌握一些数据库知识，可能会有这样的疑问：“这些设计是全新的吗？”——实际上并非如此，甚至可能做得更好。\n在内存中批量处理数据、在预写日志中记录操作以及周期性地将数据落盘的设计模式在如今无处不在，使用这种方案的知名开源项目有 LevelDB、Cassandra、InfluxDB 以及 HBase 等。关键在于不要发明劣质的轮子，而是对已被证明有效的方法深入研究，并以正确的方式应用它们。\n当然，我们还有机会在 Prometheus 中加入自己的“魔法”。\n 索引 我们改进存储系统的初衷是希望解决 Series Churn 带来的问题，而基于 Block 的设计则减少了查询时涉及的 Series 数量（设为 n）。然而对于时间复杂度为 $O(n^2)$ 的查询操作，减少 n 的数量没有什么意义。如果以前查询性能很差，那么现在依然会很糟糕。\n实际上，大多数查询操作的响应都很快。可一旦时间跨度较大，即使只查询几个 Series 也会很慢。在开展所有工作前，我最初的想法便是为这个问题找到一个解决方案。我们需要一个更加强大的 倒排索引。\n倒排索引可以基于数据内容的子集来为我们提供快速查找的能力。简而言之，我们可以在不遍历全部 Series 的情况下，找到所有包含标签app=\"nginx\"的 Series。\n为此，我们需要为每个 Series 分配一个独有的 ID。它可以在常量时间，即 $O(1)$ 内被检索出来。在这种情况下，ID 是我们的正排索引（Forward Index）。\n 示例：如果包含标签app=\"nginx\"的 Series ID 为 10、29 和 10，那么标签“nginx”的倒排索引就是一个简单的数组 [10, 29, 9]，我们可以用它来快速检索所有包含该标签的 Series。即使还有其他 200 亿个 Series，也不会影响这次查询的速度。\n 简单来说，如果 n 是 Series 总数，m 是给定查询结果的大小，那么查询的时间复杂度就从 $O(n)$ 变成了 $O(m)$。通常 m 要比 n 小很多，因此查询性能将显著提升。\n实际上，这一设计与 V2 存储系统中所采用的倒排索引几乎相同，也是在数百万个 Series 中实现高性能查询的最低要求。敏锐的读者可能已经发现，在最坏的情况下，一个标签存在于所有的 Series 中，那么复杂度又变成了 $O(n)$。其实这是合理的，因为如果查询涉及到全部数据，自然需要更长的时间。不过，一旦我们使用更加复杂的查询语句，就会面临一些新的问题。\n标签的组合 一个标签与数百万个 Series 关联是很常见的。假设一个微服务\"foo\"水平扩展为数百个实例，其中每个实例又有数千个包含标签app=\"foo\" 的 Series。通常我们不会查询所有的 Series，而是使用多个标签的组合来对返回结果进行一定的限制。例如，我们可以通过__name__=\"requests_total\" AND app=\"foo\"来获取服务实例接收的请求数。\n为了找到满足两个标签选择器的所有 Series，我们需要计算两个标签对应的倒排索引数组的交集，其结果通常比单独的倒排索引数组小几个数量级。在最坏的情况下，每个倒排索引数组的长度均为 n，那么在两个数组中通过嵌套迭代取交集的时间复杂度就为 $O(n^2)$。其他类似的查询操作，如app=\"foo\" OR app=\"bar\"，也会花费同样的开销。当我们向查询语句中添加更多的标签选择器时，时间复杂度会呈指数增长：$O(n^3)$、$O(n^4)$、$O(n^5)$ … $O(n^k)$。\n幸运的是，只需要一个小小的改动就可以解决我们的问题。如果倒排索引数组中的 ID 都已被排序，那么会发生什么呢？\n1 2 3 4  __name__=\"requests_total\" -\u003e [ 9999, 1000, 1001, 2000000, 2000001, 2000002, 2000003 ] app=\"foo\" -\u003e [ 1, 3, 10, 11, 12, 100, 311, 320, 1000, 1001, 10002 ] intersection =\u003e [ 1000, 1001 ]   我们在每个数组的起始元素处放置一个游标，其中数字较小的会不断前进。当两个游标对应的数字相等时，我们就把它添加到结果中并同时推进两个游标。由于游标只会在其所在的数组中移动，因此其遍历全部数组的时间复杂度为 $O(2n) = O(n)$。\n对两个以上的倒排索引数组取交集的过程与之类似。当标签增长到 k 个时，时间复杂度只会变为 $O(n * k)$，而不是 $O(n^k)$。这是一个非常大的改进。\n本文介绍的是典型搜索索引的一个简化版本，几乎所有的 全文搜索引擎 都在使用它。每个 Series 的标识符都被视为一个简短的“document”，而每个标签（名称加上固定的值）则是其中的一个“word”。我们可以忽略一些搜索引擎中常用的的索引附加数据，比如 word 的位置和频率。\n实际上还有很多技术可以对倒排索引进行压缩，但它们各有优缺点。考虑到我们的 document 很小，并且 word 在各个 Series 中的重复率很高，因此压缩其实无关紧要。例如，一个真实世界的数据集中大约有 440 万个 Series，每个 Series 大约有 12 个标签，但其中唯一的标签却不超过 5000 个。最初版本的存储系统没有采用压缩，只是做了一些简单优化以跳过大范围没有交集的 ID。\n让 ID 始终按顺序排列并非看起来那么简单。例如，V2 存储系统将哈希值作为 ID 分配给新的 Series，这样我们就无法有效地对倒排索引进行排序。\n另一个艰巨的任务是在数据被删除或更新时修改索引。通常最简单的方法就是在保证数据库可查询且一致的同时，重新计算并重写它们。V3 存储系统为每个 Block 都分配了一个独立的索引。对于已持久化的 Block，其索引只有在压缩时才会被重写。而对于内存中可变的 Block，其索引则需要被持续更新。\n基准测试 我们使用 测试工具 将 Prometheus 部署在 AWS 上的 Kubernetes 集群中，其中包含两个 1.5.2 版本（V2 存储系统）和两个 2.0 版本（V3 存储系统）的实例。为了模拟 Series Churn，微服务会定期移除旧的 Pod 并创建新的 Pod 以生成更多新的 Series。服务扩展频率和查询负载远远超过了如今生产环境中的真实情况，因此可以确保 V3 存储系统能够应对未来的数据规模。例如，在我们的测试环境中微服务每隔 15 分钟就要更换自身 60% 的实例。而在实际的生产环境中，这种情况每天只会发生一到五次。Prometheus 每秒从 850 个 Target 中采集约 110000 个样本，每次涉及多达 50 万个 Series。基准测试的结果如下：\n$$Heap \\space memory \\space usage \\space in \\space GB$$\n我们可以发现被查询的 Prometheus 实例消耗更多的内存，并且 2.0 版本的堆内存使用量比 1.5 版本减少了三到四倍之多。在测试开始后 6 小时左右，1.5 版本的 Prometheus 实例达到了峰值。这是因为我们将数据的保留期限设置为了 6 小时，而 V2 存储系统中删除数据的巨大开销导致了资源消耗的上升。\n$$CPU \\space usage \\space in \\space cores/second$$\nCPU 使用率的状况与内存类似，只不过查询操作对其的影响更大。总体来看，新的存储系统中 CPU 的使用率比原来减少了三到十倍。\n$$Disk \\space writes \\space in \\space MB/second$$\n我们可以通过这张图清楚地看到 Prometheus 1.5 是如何导致 SSD 磨损的。每当一个 Chunk 被持久化到 Series 对应的文件中，或删除旧数据并重写文件时，磁盘的写入速率就会大幅上升。而 Prometheus 2.0 每秒只会向预写日志中写入约 1MB 字节，写入速率只有在 Block 被持久化时才会出现峰值。新的设计方案成功地减少了约 97%～99%的磁盘写入。\n$$Disk \\space size \\space in \\space GB$$\n虽然两个版本的 Prometheus 使用的压缩算法近乎相同，但 Series Churn 的存在导致了两者占用磁盘空间的巨大差异。\n$$99th \\space percentile \\space query \\space latency \\space in \\space seconds$$\n在 Prometheus 1.5 中，查询延迟随着 Series 数量的不断上升越来越高。当数据达到保留期限并开始删除旧的 Series 时，查询延迟便又会趋于平稳。相比之下，Prometheus 2.0 的查询延迟从一开始就保持不变。\n$$Ingested \\space samples/second$$\n两个 Prometheus 2.0 实例的样本采集率完全吻合，并在数小时后开始变得不稳定。这并非是 Prometheus 自身的问题，而是集群中节点的负载过高而导致的。对于 Prometheus 1.5，即使节点仍有可用的 CPU 和内存资源，它的样本采集率也会因 Series Churn 而大大降低。\n基准测试的结果表明，Prometheus 2.0 在云服务器上的表现远远超出了最初设计时的预期。不过其成功与否还是要取决于用户的反馈，而非基准测试中的数字。\n总结 对于 Prometheus 来说，处理高基数（High Cardinality）的 Series 和独立样本的吞吐量是一项颇为艰巨的任务。不过，新的存储系统似乎已经准备好应对未来的挑战。\n使用 V3 存储系统的 Prometheus 2.0 的第一个 Alpha 版本 已经可供测试，预计会出现一些崩溃、死锁和其他 Bug。\n存储系统自身的代码可以在一个独立的 项目 中找到。它其实与 Prometheus 无关，因此可以广泛用于其他需要高效本地存储的时序数据库应用中。\n 译者注：上述项目已于 2019 年合并到 Prometheus 主仓库中，原因详见：https://github.com/prometheus/prometheus/pull/5805\n  译者注：本文从宏观的角度介绍了 Prometheus 需要解决的问题，以及 1.X 版本（V2 存储系统）和 2.X 版本（V3 存储系统）的设计理念。想要了解其实现细节，除了阅读源码外还可以参考以下内容：\n  关于 Promtheus 中的内存数据库：Prometheus TSDB (Part 1): The Head Block；\n  关于预写日志和 Checkpoint：Prometheus TSDB (Part 2): WAL and Checkpoint；Write-Ahead Log；\n  关于 mmap：Prometheus TSDB (Part 3): Memory Mapping of Head Chunks from Disk；Why mmap is faster than system calls；\n  关于索引：Prometheus TSDB (Part 4): Persistent Block and its Index；\n  关于查询：Prometheus TSDB (Part 5): Queries；\n  关于压缩和保留：Prometheus TSDB (Part 6): Compaction and Retention；Time-series compression algorithms, explained；\n  一些视频：PromCon 2017: Storing 16 Bytes at Scale - Fabian Reinartz；技术分享：Prometheus 是怎么存储数据的（陈皓）；\n   ","description":"","tags":["Prometheus","TSDB"],"title":"【译】从零开始编写一个时序数据库","uri":"/posts/writing-a-time-series-database-from-scratch/"},{"content":"前言 OpenShift 4.X 版本要求安装在操作系统为 CoreOS 的机器上，因此 官方文档 给出了使用 PXE 或 IPXE 引导 CoreOS 系统的方法。我们可以参考其操作流程，将一台 CentOS 7.X 的机器改写为 CoreOS 系统，步骤如下：\n  从 镜像下载页 下载安装所需版本的 kernel、initramfs 和 rootfs 文件，并将 rootfs 和点火文件（*.ign）上传到自建的 HTTP 服务器上；\n  将 kernel 和 initramfs 文件拷贝到 CentOS 7.X 机器的 /boot 目录下；\n  根据需求修改 /boot/grub2 目录下的 grub.cfg 文件；\n  重启机器。\n  对于操作系统初学者（比如我）来说，很难想象仅依靠添加和修改文件就能改变一台计算机的操作系统。为了解其实现原理，我们将对 Linux 的启动流程进行讨论，并从中说明上述操作是如何影响操作系统的。\nLinux 启动流程 启动一台 Linux 机器的过程可以分为两个部分：Boot 和 Startup。其中，Boot 起始于计算机启动，在内核初始化完成且 systemd 进程开始加载后结束。紧接着， Startup 接管任务，使计算机达到一个用户可操作的状态。\nBoot 阶段 如上图所示，Boot 阶段又可以细分为三个部分：\n BIOS POST Boot Loader 内核初始化  BIOS POST 开机自检（Power On Self Test，POST）是 基本输入输出系统（Basic I/O System，BIOS）的一部分，也是启动 Linux 机器的第一个步骤。其工作对象是计算机硬件，因此对于任何操作系统都是相同的。POST 检查硬件的基本可操作性，若失败则 Boot 过程将会被终止。\nPOST 检查完毕后会发出一个 BIOS 中断调用 INT 13H，它将在任何可连接且可引导的磁盘上搜索含有有效引导记录的引导扇区（Boot Sector），通常是 主引导扇区。引导扇区中的主引导记录（Master Boot Record，MBR）将被加载到 RAM 中，然后控制权就会转移到其手中。\nBoot Loader 大多数 Linux 发行版使用三种 Boot Loader 程序：GRUB1、GRUB2 和 LILO，其中 GRUB2 是最新且使用最为广泛的。GRUB2 代表“GRand Unified Bootloader, version 2”，它能够定位操作系统内核并将其加载到内存中。GRUB2 还允许用户选择从几种不同的内核中引导计算机，如果更新的内核版本出现兼容性问题，我们就可以恢复到先前内核版本。\nGRUB1 的引导过程可以分为三个阶段：stage 1、stage 1.5 和 stage 2。虽然 GRUB2 中并没有 stage 的概念，但两者的工作方式基本相同。为了方便说明，我们在讨论 GRUB2 时将沿用 GRUB1 中 stage 的说法。\nstage 1 上文提到，BIOS 中断调用会定位主引导扇区，其结构如下图所示：\n主引导记录首部的引导代码便是 stage 1 文件 boot.img，它和 stage 1.5 文件 core.img 均位于 /boot/grub2/i386-pc 目录下：\n1 2 3  [root@bastion ~]# du -b /boot/grub2/i386-pc/*.img  512 /boot/grub2/i386-pc/boot.img 26664 /boot/grub2/i386-pc/core.img   它的作用是检查分区表是否正确，然后定位和加载 stage 1.5 文件。446 字节的 boot.img 放不下能够识别文件系统的代码，只能通过计算扇区的偏移量来寻找，因此 core.img 必须位于主引导记录和驱动器的第一个分区（partition）之间。第一个分区从扇区 63 开始，与位于扇区 0 的主引导记录之间有 62 个扇区（每个 512 字节），有足够的空间存储大小不足 30000 字节的 core.img 文件。当 core.img 文件加载到 RAM 后，控制权也随之转移。\nstage 1.5 相比于只能读取原始扇区的 LILO，GRUB1 和 GRUB2 均可识别文件系统，这依赖于 stage 1.5 文件中内置的文件系统驱动程序。如果你拥有一台仍然使用 GRUB1 引导的 CentOS 6.X 机器，那么便可以在 /boot/grub/ 目录下找到这些适配不同文件系统的 stage 1.5 文件：\n1 2 3 4 5 6 7 8 9 10 11  [root@centos6.5 ~]# du -b /boot/grub/* | grep stage1_5 13380 /boot/grub/e2fs_stage1_5 12620 /boot/grub/fat_stage1_5 11748 /boot/grub/ffs_stage1_5 11756 /boot/grub/iso9660_stage1_5 13268 /boot/grub/jfs_stage1_5 11956 /boot/grub/minix_stage1_5 14412 /boot/grub/reiserfs_stage1_5 12024 /boot/grub/ufs2_stage1_5 11364 /boot/grub/vstafs_stage1_5 13964 /boot/grub/xfs_stage1_5   GRUB2 中的 core.img 不仅整合了上述文件系统驱动，还新增了菜单处理等模块，这也是其优于 GRUB1 的地方。我们可以在 GNU GRUB Manual 2.06: Images 中找到对各种 GRUB 镜像文件的详细介绍。\n既然 core.img 文件可以识别文件系统，那么它就能够根据安装时确定的系统路径定位和加载 stage 2 文件。同样，当 stage 2 文件加载到 RAM 后，控制权也随之转移。\nstage 2 stage 2 文件并非是一个 .img 的镜像，而是一些运行时内核模块：\n1 2 3 4 5 6 7 8 9 10 11  [root@bastion ~]# ls /boot/grub2/i386-pc/ | grep .mod | head acpi.mod adler32.mod affs.mod afs.mod ahci.mod all_video.mod aout.mod appendedsig.mod appended_signature_test.mod archelp.mod   它们的任务是根据 grub.cfg 文件的配置定位和加载内核文件，然后将控制权转交给 Linux 内核。grub.cfg 文件存放在 /boot/grub2 目录下：\n1 2 3 4 5 6  [root@bastion ~]# head /boot/grub2/grub.cfg -n 5 # # DO NOT EDIT THIS FILE # # It is automatically generated by grub2-mkconfig using templates # from /etc/grub.d and settings from /etc/default/grub   通过该文件的注释我们可以知道，它实际上是由 grub2-mkconfig 命令使用 /etc/grub.d 目录下的一些模板文件并根据 /etc/default/grub 文件中的设置生成的：\n1 2  [root@bastion ~]# ls /etc/grub.d/ 00_header 00_tuned 01_users 10_linux 20_linux_xen 20_ppc_terminfo 30_os-prober 40_custom 41_custom README   40_custom 和 41_custom 文件常用于用户对 GRUB2 配置的修改，实际上我们对机器的操作也是从这里开始的。为了让 GRUB2 在机器启动时选择 CoreOS 系统内核而非默认的 CentOS，需要在原始 40_custom 文件末尾添加如下内容：\n1 2 3 4 5  menuentry 'coreos' { set root='hd0,msdos1' linux16 /rhcos-live-kernel-x86_64 coreos.inst=yes coreos.inst.install_dev=vda rd.neednet=1 console=tty0 console=ttyS0 coreos.live.rootfs_url=http://{{HTTP-Server-Path}}/rhcos-live-rootfs.x86_64.img coreos.inst.ignition_url=http://{{HTTP-Server-Path}}/master.ign ip=dhcp initrd16 /rhcos-live-initramfs.x86_64.img }   所示的 Menuentry 由三条 Shell 命令组成：\n set root='hd0,msdos1' linux16 /rhcos-live-kernel-x86_64 ... initrd16 /rhcos-live-initramfs.x86_64.img  第一条命令指定了 GRUB2 的根目录，也就是 /boot 所在分区在计算机硬件上的位置。既然我们已经将内核文件拷贝到了 /boot 目录下，那么能够识别文件系统的 GRUB2 便可以定位和加载它。本例中hd代表硬盘（hard drive），0代表第一块硬盘，mosdos代表分区格式，1 代表第一个分区。详细的硬件命名规范见 Naming Convention。\n第二条命令将从rhcos-live-kernel-x86_64（CoreOS 系统的内核文件）中以 16 位模式加载 Linux 内核映像，并通过coreos.live.rootfs_url和coreos.inst.ignition_url参数指定根文件系统（rootfs）的镜像文件和点火文件的下载链接。ip=dhcp代表该计算机网络将由 DHCP 服务器动态配置，也可以按ip={{HostIP}}::{{Gateway}}:{{Genmask}}:{{Hostname}}::none nameserver={{DNSServer}}的格式写入静态配置。\n第三条命令将从rhcos-live-initramfs.x86_64.img中加载 RAM Filesystem。GRUB2 读取的内核文件实际上只包含了内核的核心模块，缺少硬件驱动模块的它无法完成 rootfs 的挂载。然而这些硬件驱动模块位于 /lib/modules/$(uname -r)/kernel/ 目录下，必须在 rootfs 挂载完毕后才能被识别和加载。为了解决这一问题，initramfs（前身为 initrd）应运而生。它是一个包含了必要驱动模块的临时 rootfs，内核可以从中加载所需的驱动程序。待真正的 rootfs 挂载完毕后，它便会从内存中移除。\n除此之外我们还需要将 /etc/default/grub 文件中的 GRUB_DEFAULT=saved 修改为 GRUB_DEFAULT=“coreos”，使其与 40_custom 文件中的menuentry 'coreos'对应。最后使用命令grub2-mkconfig -o /boot/grub2/grub.cfg来重新生成一份 grub.cfg 文件，这样计算机重启后 GRUB2 就会根据我们的配置去加载 CoreOS 系统的内核了。\n至此我们已经明白了为什么“仅依靠添加和修改文件就能改变一台计算机的操作系统”，但计算机想要达到用户可操作状态还远不止于此。让我们再来看看内核被加载到内存后发生了什么。\n内核初始化 不同内核及其相关文件位于 /boot 目录中，均以 vmlinuz 开头：\n1 2 3 4  [root@bastion ~]# ls /boot/ | grep vmlinuz vmlinuz-0-rescue-20210623110808105647395700239158 vmlinuz-4.18.0-305.12.1.el8_4.x86_64 vmlinuz-4.18.0-305.3.1.el8.x86_64   内核通过压缩自身来节省存储空间，所以当选定的内核被加载到内存中后，它首先需要进行解压缩（extracting）。一旦解压完成，内核便会开始加载 systemd 并将控制权移交给它。\nStartup 阶段 systemd 是所有进程之父，它负责使计算机达到可以完成生产工作的状态。其功能比过去的 init 程序要丰富得多，包括挂载文件系统、启动和管理计算机所需的系统服务。当然你也可以将一些应用（如 Docker）以 systemd 的方式启动，但它们与 Linux 的启动无关，因此不在本文的讨论范围之内。\n首先，systemd 根据 /etc/fstab 文件中的配置挂载文件系统。然后读取 /etc 目录下的配置文件，包括其自身的配置文件 /etc/systemd/system/default.target。该文件指定了 systemd 需要引导计算机到达的最终目标和状态，实际上是一个软链接：\n1 2  [root@bastion ~]# ls /etc/systemd/system/default.target -l lrwxrwxrwx. 1 root root 37 Oct 17 2019 /etc/systemd/system/default.target -\u003e /lib/systemd/system/multi-user.target   在我使用的 bastion 服务器上，它指向的是 multi-user.target；对于带有图形化界面的桌面工作站，它通常指向 graphics.target；而对于单用户模式的机器，它将指向 emergency.target。target 等效于过去 SystemV 中的 运行级别（Runlevel），它提供了别名以实现向后兼容性：\n   SystemV Runlevel systemd target systemd target alias Description      halt.target  在不关闭电源的情况下中止系统。   0 poweroff.target runlevel0.target 中止系统并关闭电源。   s emergency.target  单用户模式。 没有服务正在运行，也未挂载文件系统。仅在主控制台上运行一个紧急 Shell，供用户与系统交互。   1 rescue.target runlevel1.target 一个基本系统。文件系统已挂载，只运行最基本的服务和主控制台上的紧急 Shell。   2  runlevel2.target 多用户模式。虽然还没有网络连接，但不依赖网络的所有非 GUI 服务都已运行。   3 multi-user.target runlevel3.target 所有服务都在运行，但只能使用命令行界面（CLI）。   4  runlevel4.target 用户自定义   5 graphical.target runlevel5.target 所有服务都在运行，并且可以使用图形化界面（GUI）。   6 reboot.target runlevel6.target 重启系统    每个 target 都在其配置文件中指定了一组依赖，由 systemd 负责启动。这些依赖是 Linux 达到某个运行级别所必须的服务（service）。换句话说，当一个 target 配置文件中的所有 service 都已成功加载，那么系统就达到了该 target 对应的运行级别。\n下图展示了 systemd 启动过程中各 target 和 service 实现的一般顺序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  cryptsetup-pre.target veritysetup-pre.target | (various low-level v API VFS mounts: (various cryptsetup/veritysetup devices...) mqueue, configfs, | | debugfs, ...) v | | cryptsetup.target | | (various swap | | remote-fs-pre.target | devices...) | | | | | | | | | v | v local-fs-pre.target | | | (network file systems) | swap.target | | v v | | | v | remote-cryptsetup.target | | | (various low-level (various mounts and | remote-veritysetup.target | | | services: udevd, fsck services...) | | remote-fs.target | | tmpfiles, random | | | / | | seed, sysctl, ...) v | | / | | | local-fs.target | | / | | | | | | / \\____|______|_______________ ______|___________/ | / \\ / | / v | / sysinit.target | / | | / ______________________/|\\_____________________ | / / | | | \\  | / | | | | | | / v v | v | | / (various (various | (various | |/ timers...) paths...) | sockets...) | | | | | | | | v v | v | | timers.target paths.target | sockets.target | | | | | | v | v \\_______ | _____/ rescue.service | \\|/ | | v v | basic.target rescue.target | | | ________v____________________ | / | \\  | | | | | v v v | display- (various system (various system | manager.service services services) | | required for | | | graphical UIs) v v | | multi-user.target emergency.service | | | | \\_____________ | _____________/ v \\|/ emergency.target v graphical.target   如上图所示，想要到达到某个 target，其依赖的所有 target 和 service 就必须已完成加载。如实现 sysinit.target，需要先挂载文件系统（local-fs.target）、设置交换文件（swap.target）、初始化 udev （various low-level services）和设置加密服务（cryptsetup.target）等。不过，同一个 target 的不同依赖项可以并行执行。\n当计算机达到 multi-user.target 或 graphical.target 时，它的漫漫启动之路就走到了尽头。但为了满足用户多样的需求，它所面临的挑战其实才刚刚开始。\nFuture Work  前言提到 RedHat 官方给出了 IPXE/PXE 引导 CoreOS 系统的方法，那么这项技术又是如何实现的呢？ MBR 只有 446 个字节，可为什么 boot.img 文件却有 512 个字节？ 目前已经有越来越多的计算机使用 UEFI 和 GPT 来代替 BIOS 和 MBR，其优势体现在哪？ 我们该如何理解 systemd 的配置文件？如何使用 systemd 部署我们的应用？  参考文献 Creating Red Hat Enterprise Linux CoreOS (RHCOS) machines by PXE or iPXE Booting\nBIOS - Wikipedia\nINT 13H - Wikipedia\n主引导记录 - Wikipedia\nAn Introduction To the Linux Boot and Startup Processes\nGNU GRUB Manual 2.06\nBootup(7) - Linux manual page\n","description":"","tags":["OS","Linux","CoreOs"],"title":"通过安装 CoreOS 系统了解 Linux 启动流程","uri":"/posts/understand-linux-boot-process-by-installing-coreos/"},{"content":"在使用高级语言，如 C、Java 编程时，我们无法了解程序具体的机器级实现。相比之下，使用汇编语言编写程序时，程序员必须指定程序使用的低级指令来执行计算。编译器提供的类型检查有助于检测许多程序错误，确保我们以一致的方式来引用和操作数据。最重要的是，用高级语言编写的程序可以在多种不同的机器上编译运行，而汇编语言则与机器特性高度相关。\n尽管编译器完成了生成汇编代码的大部分工作，但阅读和理解汇编语言对于程序员来说是一项重要的技能：\n Those who say “I understand the general principles, I don’t want to bother learning the details” are deluding themselves.\n Intel 处理器历史 程序编码 机器级代码 首先，机器级程序的格式和行为是由指令集架构（instruction set architecture，ISA）定义的，包括处理器状态、指令格式以及每条指令对状态的影响。大多数 ISA，包括 x86-64，都将程序的行为描述为每条指令按顺序执行，且一条指令在下一条指令开始之前完成。虽然处理器硬件要复杂得多，可以同时执行许多指令，但它采用了安全措施来确保其整体行为与 ISA 规定的操作顺序相匹配。其次，机器级程序使用的内存地址是虚拟地址，提供了一个看似非常大的字节数组的内存模型。\n汇编代码表示非常接近机器代码，与机器代码的二进制格式相比，它采用更具可读性的文本格式。一些对程序员隐藏的处理器状态在汇编代码中是可见的：\n 程序计数器（PC）：在 x86-64 中称为 %rip，代表即将执行的下一条指令在存储器中的地址； 包含 16 个位置（location）的整数寄存器文件（register file）：这些位置均被命名，每个都能存储 64 位的值。该寄存器可以保存地址（与 C 中的指针对应）和整数数据。一些寄存器用于记录程序状态的关键部分，而其他寄存器则用于保存临时数据，例如过程中的参数、局部变量和函数返回值； 条件码寄存器（condition code registers）：保存了最新执行的算术或逻辑指令的状态信息，用于实现控制流或数据流中条件的改变，例如 if 语句和 while 语句； 一组向量寄存器（vector registers）：每个都可以保存一个或多个整数或浮点数值。  虽然 C 提供了一个模型，让我们可以在内存中声明和分配不同数据类型的对象。但机器级代码只会简单地将内存视为一个按字节寻址的数组，因此 C 中的聚合数据类型（如数组和结构体）在机器级代码中会表示为连续的字节集合。甚至对于标量数据类型（如 int、char、float 和 bool 等），汇编代码也不会区分有符号和无符号整数、不同类型的指针以及指针和整数。\n程序的可执行机器级代码、操作系统所需的一些信息、用于管理过程调用和返回（procedure calls and returns）的运行时栈以及用户分配的内存块（如使用库函数malloc）共同构成了程序内存，它使用虚拟地址寻址，不过只有部分虚拟地址的范围有效。例如，x86-64 机器的虚拟地址必须将前 16 位设置为 0，因此其有效范围包含 $2^{48}$ （64 TB）字节。操作系统负责管理该虚拟地址空间，并将其转换为实际处理器内存（processer memory）中的物理地址。\n单个机器指令仅执行一些基本的操作，如将存储在寄存器中的两个数字相加、在内存和寄存器之间传输数据以及有条件地跳转到新的指令地址。编译器必须生成这样的指令序列来实现程序结构，例如算术表达式求值、循环或过程调用和返回。\n代码示例 C 程序文件 mstore.c 中包含如下代码：\n1 2 3 4 5  long mult2(long, long); void multstore(long x, long y, long *dest) { long t = mult2(x, y); *dest = t; }   使用gcc -Og -S mstore.c命令即可生成汇编代码文件 mstore.s，其中的-Og代表对代码进行优化。汇编代码中有多种声明，包括：\n每一行缩进的代码都对应着一条机器指令，图中的蓝色注解则标示了指令的作用，如pushq代表将寄存器 %rbx 的内容压入程序栈中。原程序中局部变量名称以及数据类型的所有信息都已被删除，随后我们可以在 Linux 系统中执行下列命令：\n1 2  gcc -Og -c mstore.c objdump -d mstore.o   第一条命令将生成二进制格式的目标代码文件（object-code file）mstore.o，第二条命令则是进行反汇编，即将机器级代码转换为一种与汇编语言格式类似的代码。图中的行号和斜体注释是为了方便说明加入的，左侧有 6 组十六进制字节序列，每个都是一条指令，右侧则显示了等效的汇编代码：\n x86-64 指令的长度范围为 1 到 15 个字节，常用的和操作数较少的指令比不太常用或操作数较多的指令需要更少的字节数； 从给定的起始位置开始，将字节唯一地解码为机器指令。例如，只有指令 pushq %rbx 以字节值 53 开头； 反汇编程序只是根据机器级代码文件中的字节序列来确定汇编代码，不需要访问源代码或汇编代码； 反汇编程序使用的指令命名规则与 gcc 生成的汇编代码略有不同，如许多指令中的后缀q被省略了。这些后缀是尺寸指示符，在大多数情况下可以省略。而反汇编器在call和ret指令中添加了后缀q，它们同样是可以省略的。  想要生成实际的可执行代码还需要在一组包含main函数的目标代码文件上运行链接器（Linker），假设 main.c 文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12  #include \u003cstdio.h\u003evoid multstore(long, long, long *); int main() { long d; multstore(2, 3, \u0026d); printf(\"2 * 3 --\u003e %ld\\n\", d); return 0; } long mult2(long a, long b) { long s = a * b; return s; }   那么通过gcc -Og -o prog main.c mstore.c命令生成的可执行文件 prog 的大小就超过了 mstore.o，因为它还包含了用于启动和终止程序以及与操作系统交互的代码。同样对 prog 文件使用objdump命令进行反汇编，其输出结果包含如下代码：\n与第一次反汇编的结果相比，区别主要在：\n 链接器将代码的地址（Offset）移动到了不同的地址范围内，因此左侧的地址不同； 链接器的一项任务是将函数调用与这些函数的可执行代码的位置进行匹配。因此在结果第 4 行，链接器填充了callq指令在调用函数mult2时应该使用的地址； 结果第 8，9 行填充的代码对结果没有任何影响，nop 意为 no operation。插入它们是为了将函数的代码增加到 16 字节，这样可以更好地放置下一个代码块，从而提升存储器的系统性能。  数据格式 Intel 用术语“字”（word）来表示 16 位数据类型，因此 32 位数称为“双字”（double words），64 位数称为“四字”（quad words）。在 x86-64 机器上 C 的原始数据类型大小如下：\n   C declaration Intel data type Assembly-code suffix Size(bytes)     char Byte b 1   short Word w 2   int Double word l 4   long Quad word q 8   char * Quad word q 8   float Single precision s 4   double Double precision l 8    大多数由 gcc 生成的汇编代码都有一个表示操作数大小的单字符后缀，如数据移动指令有四种变体：movb（移动字节）、movw（移动字）、movl（移动双字）和movq（移动四字）。值得一提是，后缀“l”即可以表示 4 字节整数又表示 8 字节双精度浮点数。这并不会引起歧义，因为涉及到浮点数的代码使用一组与整数完全不同的指令和寄存器。\n访问信息 上文提到，x86-64 机器的 CPU 中包含 16 个通用寄存器（general-purpose registers），均可以存储 64 位的整数或指针数据：\n图中所有寄存器的名称均以 %r 开头，它们的演变顺序是从右往左的。前 8 个寄存器，即 %ax 到 %sp，是最初的 8086 机器使用的 8 个 16 位寄存器。随着 IA32 的出现，它们被扩展位 32 位，即 %eax 到 %esp。目前的 x86-64 机器将其进一步地扩展为 64 位，即 %rax 到 %rsp。同时还新添加了 8 个寄存器，即 %r8 到 %r15。图中右侧的注释说明了各个寄存器在典型程序中扮演的角色，其中最独特的是栈指针 %rsp，它用于指示运行时栈的结束位置。\n指令可以对存储在寄存器低位字节中的不同大小的数据进行操作。字节级操作可以访问最低有效字节，16 位操作可以访问最低有效 2 个字节，32 位操作可以访问最低有效 4 个字节，64 位操作则可以访问整个寄存器。\n操作数 大多数指令都有一个或多个操作数（operand），指定了执行操作时使用的源数据和结果放置的位置。x86-64 机器支持的操作数格式如下：\n源数据既可以以常数形式给出，也可以从寄存器或内存中读取，结果则存储在寄存器或内存中。因此操作数有三种可能的类型：\n 立即数（immediate）：即常数值，书写方式为 $ 符号后面跟一个整数； 寄存器（register）：寄存器中的内容。我们使用 $r_a$ 表示任意寄存器 $a$，使用 $R[r_a]$ 来表示它的值； 内存（memory）引用：根据计算出的地址（称为有效地址）来访问某个内存位置。使用 $M_b[Addr]$ 表示在内存中从地址 $Addr$ 开始 $b$ 字节的引用，下角标 $b$ 可以省略。  最通用的内存引用方式在上表底部：$M[Imm + R[r_b] + R[r_i]\\times s]$，常用于引用数组元素。\n假设下列值存储在内存或寄存器中指定的地址处：\n那么以下操作数存储的值分别为：\n %rax：0x100 0x104：地址为 0x104，值为 0xAB $0x108：0x108（立即数） (%rax)：地址为 0x100，值为 0xFF 9(%rax,%rdx)：地址为 9 + 0x100 + 0x3 = 0x10C，值为 0x11 260(%rcx,%rdx)：260 即十六进制数 0x104，因此地址为 0x104 + 0x1 + 0x3 = 0x108，值为 0x13 0xFC(,%rcx,4)：地址为 0xFC + 0x1 * 4 = 0x100，值为 0xFF %rax,%rdx,4：地址为：0x100 + 0x3 * 4 = 0x10C，值为 0x11  数据移动指令 在所有机器指令中使用最为频繁的便是数据移动指令，它们负责将数据从一处移动到另一处。我们将操作相同但操作数尺寸不同的指令划分为同一个指令类（instruction classes），下表列出的便是 MOV 指令类中的各种操作：\n上表中的 S 代表 源地址（Source），D 代表目的地址（Destination），I 代表立即数（Immediate），R 代表寄存器（Register）。其中，移动指令不能将一个位于内存中的数据直接移动到内存中的另一个位置，必须经过一个寄存器中转。另外，当movl指令的目的地址为一个寄存器时，它不仅会把目的寄存器的较低四位更新为源数据，还会将较高四位的字节全部置 0。最后，movabsq指令只能将寄存器作为数据的目的地址（R \u003c- I）。下面的例子中，左边为顺序执行的移动指令，右边为寄存器 %rax 中字节的变化情况：\nmovabsq $0x0011223344556677, %rax %rax = 0011223344556677 movb $-1, %al %rax = 00112233445566FF movw $-1, %ax %rax = 001122334455FFFF movl $-1, %eax %rax = 00000000FFFFFFFF movq $-1, %rax %rax = FFFFFFFFFFFFFFFF 还有两类数据移动指令可以将较小尺寸的源数据移动到较大尺寸的目的寄存器，如下表所示：\n两者不同的是，movz指令将目的寄存器的剩余字节均填充为 0（零扩展），movs指令则将其填充为源操作数的最高有效位（符号扩展）。相比于符号扩展，零扩展缺少了指令movzlq。这是因为上文提到，使用movl指令移动数据到寄存器时，会将高位全部置为 0，其效果与零扩展无异。另外，符号扩展还多了一个指令cltq。它没有操作数，且实际上等效于movslq %eax, %rax。\n在 C 中，引用指针（dereference pointer，*p）会将指针拷贝到寄存器中，然后将该寄存器作为内存地址的引用。如下面的一个简单的 C 程序：\n1 2 3 4 5 6  long exchange(long *xp, long y) { long x = *xp; *xp = y; return x; }   与之等效的汇编代码如下：\n; xp in %rdi, y in %rsi exchange: movq (%rdi), %rax movq %rsi, (%rdi) ret 最后两个数据移动指令分别是将数据压入程序栈（Stack）的push和将数据从程序栈中弹出的pop，如下表所示：\n在 x86-64 中，程序栈存储在内存中的某些区域中，其特性为后进先出（last-in, first-out）。习惯上我们将栈顶画在底端，而栈顶元素的地址（由栈指针 %rsp 保存）是整个栈中最小的：\n如上图所示，想要将一个四字数据压入栈中，首先要把栈指针减 8，然后令源数据值成为新的栈顶元素。因此指令pushq %rax就等效于：\n; subq 为减法运算，下一节会进行介绍 subq $8, \u0026rsp movq %rax, (%rsp) 同样，若想将栈顶的四字数据弹出栈，首先要把栈顶元素的值拷贝到寄存器中，然后再把栈指针加 8。因此指令popq %rdx就等效于：\nmovq (%rsp), %rdx ; addq 为加法运算，下一节会进行介绍 addq $8, %rsp 算术和逻辑操作 下图列出了一些 x86-64 中的整数算数和逻辑操作，其中除了leaq外给出的都是指令类：\n上述操作可分为四类：加载有效地址（load effective address）、一元（unary）、二元（binary）和移位（shifts）。一元操作只有一个操作数，而二元操作则有两个。接下来我们将分别介绍它们。\n加载有效地址 加载有效地址的指令名为leaq，其实质是movq指令的一种变体。它会从内存中读取源操作数的地址拷贝到寄存器中，有时候也实现一些简单的运算。如寄存器 %rdx 中存储的值为 x，那么指令leaq 7(%rdx, %rdx, 4), %rax的作用便是将寄存器 %rax 的值设为 5x+7。虽然leaq指令在执行运算方面的能力是有限的，但是与加法（ADD）或乘法（IMUL）指令相比，它的性能更加出色。\n一元和二元操作 一元操作只有一个操作数，因此源地址和目的地址相同。如操作incq (%rsp)可以将程序栈顶增加 8 个字节的元素，类似于 C 中的自增运算符++。\n二元操作类似于 C 中的赋值运算符（assignment operator），如 x -= y。举例来说，操作subq %rax, %rdx会将寄存器 %rdx 中的值减去寄存器 %rax 中的值。第一个操作数可以是立即数、寄存器或内存中的位置，第二个操作数则只能是寄存器或内存中的位置。参考MOV类指令，二元操作的两个操作数不能同时为内存中的位置。\n移位操作 移位操作的第一个操作符为位数，可以是立即数，也可以是单字节的寄存器（通常使用寄存器 %cl）。第二个操作数为移位的值。可以是寄存器或内存中的位置。对于右移运算来说，sar代表算术右移，shr则代表逻辑右移。\n特殊算数操作 两个 64 位整型的积需要用 128 位来表示，因此 Intel 引入了 16 字节单位“八字”（ oct word）来解决这一问题。下表展示了一些支持运算结果为“八字”的操作：\n我们在普通算数操作和特殊算数操作中均发现了imulq指令。第一种属于 IMUL类，有两个操作数。其计算结果为 64 位（若超过 64 位，则截断高位），等效于第二章介绍的 无符号乘法 和 二进制补码乘法。第二种即是上表中的特殊算数操作，只有一个操作数，因此编译器可以根据操作数的数量区分它们。\n用于有符号乘法的操作称为imulq，用于无符号乘法的为mulq。两者的第一个参数为寄存器 %rax，第二个参数为源操作数。计算结果中较高 64 位将存储在寄存器 %rdx 中，较低 64 位则存储在寄存器 %rax 中。\n1 2 3 4 5  #include \u003cinttypes.h\u003etypedef unsigned __int128 uint128_t void store_uprod(uint128_t *dest, uint64_t x, uint64_t y){ *dest = x * (uint128_t)y; }   所示的 C 程序在小端（little-endian）机器上可转化为如下的汇编代码，结果中的较高位存储在高地址处 8(%rdi)：\n; dest in %rdi, x in %rsi, y in %rdx store_uprod movq %rsi, %rax mulq %rdx movq %rax, (%rdi) movq %rdx, 8(%rdi) 普通算数操作中没有提供除法或余数运算，因此需要使用特殊算数操作idivq和divq。与乘法类似，被除数的较高 64 位将存储在寄存器 %rdx 中，较低 64 位存储在寄存器 %rax 中。除数为操作数，商保存在寄存器 %rax 中，余数则保存在寄存器 %rdx 中。如果被除数只有 64 位，那么就应当将寄存器 %rdx 全部置为 0（无符号运算）或符号位（有符号运算）。后者可以使用cqto操作实现，它会从寄存器 %rax 中读取符号位，然后将其拷贝到寄存器 %rdx 的每一位中。\n控制 上文中介绍的操作只考虑了顺序执行的代码，对于 C 中的 if、for、while 和 switch 语句，它们需要根据数据检验的结果来决定代码执行的顺序。机器级代码提供了两种基本机制来实现这种条件行为（conditional behavior），一种根据检验结果改变控制流（control flow），另一种则改变数据流（data flow）\n条件码 CPU 维护了一组单字节的条件码（condition code）寄存器，它们记录了最近一次算数或逻辑操作结果的某些属性：\n CF（carry flag）：进位标识，记录最高有效位是否发生进位，用于检测无符号数操作的溢出； ZF（zero flag）：零标识，记录结果是否为 0； SF（sign flag）：符号标识，记录结果是否为负值； OF（overflow flag）：溢出标识，记录是否发生二进制补码溢出。  下列两个指令类可以在不改变任何其他寄存器的情况下设置条件码，如指令testq %rax, %rax可以检测寄存器 %rax 存储的值是正数、负数还是零：\n相比于直接读取，我们更常用以下三种方式使用条件码：\n 根据条件码的组合将单个字节设置为 0 或 1； 有条件地跳转到程序的其他部分； 有条件地传输数据。  下列操作指令便可以实现上述第一种方式。注意，此处的指令后缀代表的并非是不同的操作数大小，而是不同的条件判断。如指令setl和setb分别代表 set less 和 set below：\n下面示例的 C 程序中，首先比较了变量 a 和 b 的大小，然后根据结果把寄存器 %eax 的最低字节（即寄存器 %al）置为 0 或 1。最后一条指令的作用是将寄存器 %eax 的高位三个字节以及寄存器 %rax 的高位四个字节全部清零：\n; int comp(data_t a, data_t b) ; a in rdi%, b in rsi% comp: cmpq %rsi, %rdi setl %al movzbl %al, %eax ret 跳转指令 跳转指令可以让程序转到一个全新的位置继续执行，该位置在汇编代码中使用标签（label）来指定。\nmovq $0, %rax jmp .L1 movq (rax%), %rdx .L1: popq %rdx 指令jmp .L1将使程序跳过movq指令，开始执行popq操作。 下图展示了不同的跳转指令：\n​\njmp指令既可以是直接跳转，也可以是间接跳转。直接跳转的目标使用标签指定，而间接跳转的目标则需要从寄存器或内存中读取。其余的指令均为条件跳转，它们根据条件判断的结果来决定是否执行跳转操作。注意，条件跳转均为直接跳转。\n在生成机器代码的过程中，汇编器和链接器会确定跳转目标（即目标指令的地址），并编码为跳转指令的一部分。其使用的编码方式有多种，但大多数与程序计数器（PC）相关，即比较目标指令的地址和紧挨着跳转指令的下一条指令的地址之间的差异。这样的说法有些绕口，我们以一个简单的例子来说明：\nmovq %rdi%, %rax jmp .L2 .L3 sarq %rax .L2 testq %rax, %rax jq .L3 rep; ret 该汇编代码包含了两个跳转指令，第一个跳转到了更高的地址处，第二个则相反。而下图是对上述代码汇编然后再进行反汇编后的结果：\n在右侧注释中，第一个跳转指令为 +0x8，第二个跳转指令为 +0x5。再看左侧指令的字节编码，第一个指令的目标被编码为 0x03，将其加上下一条指令的地址 0x5，就得到了跳转目标指令的地址，即第四行指令的地址 0x8。同样，第二个指令的目标是使用单字节二进制补码表示的 0xf8（即十进制数 -8），将其加上下一条指令的地址 0xd，就得到了第三行指令的地址 0x5。\n当目标代码文件经过链接器处理后，这些指令会被重新分配地址。不过第二行和第五行跳转目标的编码依然不变，这种方式能够让指令编码更加紧凑：\n使用条件控制实现条件分支 若想将 C 中的条件表达式转化为机器代码，通常使用条件跳转和无条件跳转的组合。一个简单的 C 程序及其编译得到的汇编代码分别如下：\n1 2 3 4 5 6 7 8  long absdiff(long x, long y) { long result; if (x \u003c y) result = y - x; else result = x - y; }   ; long absdiff(long x, long y) ; x in %rdi, y in %rsi absdiff: cmpq %rsi, %rdi jge .L2 movq %rsi, %rax subq %rdi, %rax ret .L2 movq %rdi, %rax subq %rsi, %rax ret 实际上该汇编代码的控制流（control flow）更像是使用 goto 语句将示例 C 程序改写后得到的结果，即先比较两数大小，然后根据结果决定是否跳转：\n1 2 3 4 5 6 7 8 9 10 11  long gotodiff(long x, long y) { long result; if (x \u003e= y) goto x_ge_y; result = y - x; return result; x_ge_y: result = x - y; return result; }   让我们推广到一般情况。假设 C 中的 if-else 语句模板为：\n1 2 3 4  if (test-expr) then-statement else else-statement   那么编译生成的汇编代码的控制流便可以用如下 C 程序描述：\n1 2 3 4 5 6 7 8  t = test-expr; if (!t) goto false; then-statement goto done; false: else-statement done:   使用条件移动实现条件分支 使用条件控制实现条件分支虽然简单有效，但在现代处理器上使用可能十分低效，我们更倾向于使用一些简单的条件移动指令。上一节中的 C 程序可以改写为：\n1 2 3 4 5 6 7 8 9 10 11  long cmovdiff(long x, long y) { long rval = y - x; long eval = x - y; long ntest = x \u003e= y; /* Line below requires single instruction: */ if (ntest) rval = eval; return rval; }   这段代码首先计算 y-x 和 x-y，分别命名为变量 rval 和变量 eval。然后测试 x 是否大于或等于 y，如果是，则在返回 rval 之前将 eval 赋值给 rval。编译器可以参考这种控制流生成汇编代码：\n; long absdiff(long x, long y) ; x in %rdi, y in %rsi absdiff: movq %rsi, %rax subq %rdi, %rax movq %rdi, %rdx subq %rsi, %rdx cmpq %rsi, %rdi cmovge %rdx, %rax ret 上述代码中的comovge就是一个条件移动指令，只有 cmpq 指令判断一个值大于或等于（如后缀 ge 所示）另一个值时，它才会将数据从源寄存器传输到目标寄存器。\n想要理解为什么基于条件移动的代码效率胜过条件控制，我们必须了解现代处理器的运行方式。一条指令需要处理器通过一系列的步骤进行处理，每个步骤只执行所该指令的一小部分（例如，从内存中获取指令，确定指令类型、从内存读取、执行算术运算、写入内存和更新程序计数器等）。为了实现高性能，这条流水线（pipelining）需要将指令的步骤重叠。例如，在执行前一条指令的算术运算的同时提取下一条指令。想要做到这一点，处理器需要能够提前确定即将执行的指令序列，以保证流水线上充满指令。\n当处理器遇到条件跳转（即分支）时，它将采用复杂的逻辑来预测跳转指令是否触发。如果预测结果足够可靠（现代微处理器试图达到 90% 的成功率），流水线就可以保持指令充满。但是，错误的预测将导致处理器不得不丢弃它在未来指令上已经完成的大部分工作，使程序性能严重下降。\n示例代码中 x \u003e= y 的判断结果显然是难以预测的，这种情况下使用条件控制代码的效率很低。而条件移动代码先计算出所有可能的结果，再根据条件判断决定返回值。这样控制流便不再依赖数据，处理器也就更容易保持其流水线的完整性，从而提升执行效率。全部的条件移动指令如下：\n编译器可以从目标寄存器的名称推断条件移动指令的操作数长度，因此相同的指令名称可用于不同的操作数长度。我们同样把条件分支推广到一般情况，使用 C 程序来描述条件移动的控制流：\n1 2 3 4 5  v = then - expr; ve = else - expr; t = test - expr; if (!t) v = ve;   当然，使用条件移动来实现条件分支是也有一些弊端的。因为无论判断结果如何，我们都会全部执行 then-expr 和 else-expr。一旦某一个分支中的指令发生错误，将影响整个程序的可用性。另外，如果 then-expr 或 else-expr 需要大量计算，而对应的条件又不成立时，处理器所做的工作就完全白费了。不过总体来说，条件移动的性能还是高于条件控制的，这也是 GCC 编译器使用它的原因。\n循环 Do-While 假设 C 中的 Do-While 语句模板为：\n1 2 3  do body-statement while (text-expr);   我们可以将其转化为 goto 语句和 if-else 语句的组合：\n1 2 3 4 5  loop: body-statement t = text-expr; if (t) goto loop;   实际上汇编代码正是用这种方式来实现 Do-While 语句的控制流 。示例函数fact_do及其编译得到的汇编代码分别如下：\n1 2 3 4 5 6 7 8 9 10  long fact_do(long n) { long result = 1; do { result *= n; n = n - 1; } while (n \u003e 1); return result; }   ; long fact_do(long n) ; n in %rdi fact_do: movl $1, %eax .L2: imulq %rdi, %rax subq $1, %rdi cmpq $1, %rdi jq .L2 rep; ret While 假设 C 中的 While 语句模板为：\n1 2  while (text-expr); body-statement   我们有两种方法将其转化为 goto 语句和 if-else 语句的组合。第一种称为 jump-to-middle，通过一个非条件跳转在循环的结束执行条件判断：\n1 2 3 4 5 6 7  goto test; loop: body-statement test: t = text-expr; if (t) goto loop;   当 GCC 的优化参数指定为 -Og 时，汇编代码就会用这种方法来实现 While 语句的控制流 。示例函数fact_while及其编译得到的汇编代码分别如下：\n1 2 3 4 5 6 7 8 9 10  long fact_while(long n) { long result = 1; while (n \u003e 1) { result *= n; n = n - 1; } return result; }   ; long fact_while(long n) ; n in %rdi fact_while: movl $1, %eax jmp .L5 .L6: imulq %rdi, %rax subq $1, %rdi .L5: cmpq $1, %rdi jg .L6 rep; ret 第二种方法称为 guarded-do，即首先将代码转化为 Do-While 循环，如果条件判断失败则通过条件移动指令跳过循环：\n1 2 3 4 5 6 7 8 9  t = test-expr; if (!t) goto done; loop: body-statement t = test-expr; if (t) goto loop; done:   当 GCC 的优化参数指定为更高级别的 -O1 时，汇编代码就会用这种方法来实现 While 语句的控制流 。上面提到的函数fact_while使用 guarded-do 编译得到的汇编代码如下：\n; long fact_while(long n) ; n in %rdi fact_while cmpq $1, %rdi jle .L7 movl $1, %eax .L6: imulq %rdi, %rax subq $1, %rdi cmpq $1, %rdi jne .L6 rep; ret .L7 movl $1, %eax ret For 假设 C 中的 For 语句模板为：\n1 2  for (init-expr; test-expr; update-expr) body-statement   显然可以将其转化为等效的 While 语句：\n1 2 3 4 5  init-expr; while (test-expr) { body-statement update-expr; }   因此，我们依然可以使用两种方法来将其转化为 goto 语句和 if-else 语句的组合。\njump-to-middle：\n1 2 3 4 5 6 7 8 9  init-expr; goto test; loop: body-statement update-expr; test: t = test-expr; if (t) goto loop;   guarded-do：\n1 2 3 4 5 6 7 8 9 10 11  init-expr; t = test-expr; if (!t) goto done; loop: body-statement update-expr; t = test-expr; if (t) goto loop; done:   同样地，编译器会根据给定的优化参数使用对应的控制流来生成汇编代码。\nSwitch GCC 会根据 Switch 语句中 Case 的数量和 Case 值的稀疏性（sparsity）决定编译方法。当存在多种 Case（例如四个或更多），且它们跨越的值范围较小时会使用一种名为跳转表（jump table）的数据结构来实现。跳转表是一个数组，数组元素分别是 Switch 语句中每个 Case 对应的代码块地址。\n一个简单的 C 程序及其通过 GCC 编译后得到的汇编代码分别如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  void switch_eg(long x, long n, long *dest) { long val = x; switch (n) { case 100: val *= 13; break; case 102: val += 10; /* Fall through */ case 103: val += 11; break; case 104: case 106: val *= val; break; default: val = 0; break; } *dest = val; }   ; void switch_eg(long x, long n, long *dest) ; x in %rdi, n in %rsi, dest in %rdx switch_eg: subq $100, %rsi cmpq $6, %rsi ja .L8 jmp *.L4(,%rsi,8) .L3: leaq (%rdi,%rdi,2), %rax leaq (%rdi,%rax,4), %rdi jmp .L2 .L5: addq $10, %rdi .L6: addq $11, %rdi jmp .L2 .L7: imulq %rdi, %rdi jmp .L2 .L8: movl $0, %edi .L2: movq %rdi, (%rdx) ret 为了便于理解，我们用 C 来描述其实现（运算符\u0026\u0026会为代码块的位置创建指针）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  void switch_eg_impl(long x, long n, long *dest) { /* Table of code pointers */ static void *jt[7] = { \u0026\u0026loc_A, \u0026\u0026loc_def, \u0026\u0026loc_B, \u0026\u0026loc_C, \u0026\u0026loc_D, \u0026\u0026loc_def, \u0026\u0026loc_D}; unsigned long index = n - 100; long val; if (index \u003e 6) goto loc_def; /* Multiway branch */ goto *jt[index]; loc_A: /* Case 100 */ val = x * 13; goto done; loc_B: /* Case 102 */ val = x + 10; loc_C: /* Case 103 */ val = x + 11; goto done; loc_D: /* Case 104, 106 */ val = x * x; goto done; loc_def: /* Default case */ val = 0; done: *dest = val; }   其中的goto *jt[index]就相当于汇编代码中第五行的jmp *.L4(,%rsi,8)。它是一个间接跳转指令，其操作数.L4(,%rsi,8)指定了由寄存器 %rsi 索引的内存地址（我们将在后续章节讨论数组是如何转化为机器代码的）。\n在汇编代码中，跳转表将被表示为：\n在名为.rodata的只读目标代码段中，包含了由七个quad（八字节）组成的序列，每个quad的值为汇编代码标签（如.L3）所对应的指令地址。\n过程 过程（Procedure）在不同的编程语言中有不同的叫法，如函数（Function）、方法（Method）、子程序（Subroutine）和 Handler 等。不过它们都提供了一种打包代码的方法，该代码使用一组参数和可选的返回值来实现某些功能，并可以在程序的不同位置调用。\n假设过程 P 调用了过程 Q，Q 执行完毕后返回 P。那么：\n 传递控制：在调用 Q 时程序计数器必须设置为其代码地址，同时在返回 P 时也要置为 P 的代码地址； 传递参数：P 必须能向 Q 提供一个或多个参数，反之 Q 必须能将值返回给 P； 分配和释放内存：在调用 Q 之前可能需要为其局部变量分配内存空间，并在返回时释放。  运行时栈 当过程所需的存储空间超过寄存器所能容纳的范围时，它便会在运行时栈上分配空间。上图为运行时栈的一般结构，主要由执行过程 Q 和调用过程 P 所需的帧（Frame）组成。每个过程可以在其栈帧内保存寄存器的值（图中的 Saved registers），为局部变量分配空间（图中的 Local Variables），以及为其调用的过程设置参数（图中的 Argument）等。当 P 调用 Q 时，它会将返回地址（图中 Return Address）压入栈中。这样当 Q 返回时，程序便知道应该在哪里恢复执行 P。\n不过出于对时间和空间效率的考虑，程序只会为过程分配它们必须的栈帧。例如某个过程的参数不足 6 个，那么它们将全部存储在寄存器中而非运行时栈（图中的参数区是从 Argument 7 开始的）。许多过程的局部变量很少且不调用其他过程，那么就不需要运行时栈。\n接下来我们会对运行时栈中的各个区域进行更为深入地讨论。\n传递控制 在汇编代码中，过程间调用是通过指令call和ret来实现的。其中，指令call Q会将程序计数器设置为过程 Q 的代码起始地址，并将返回地址 A 压入栈中。指令ret则会将地址 A 从栈中弹出，然后将程序计数器设置为 A。实际上，地址 A 就是紧跟在call指令之后的指令地址。\n下图说明了 代码示例 中主函数调用 multstore 后返回的过程中运行时栈的变化情况：\n其对应的反汇编代码如下：\n当主函数调用函数 multstore 时，程序计数器 %rip 中的值为call指令的地址 0x400563。该指令将返回地址 0x400568 压入栈中并跳转到函数 multstore 中的第一条指令，其地址为 0x0400540。 随后函数 multstore 继续执行，直到遇到地址 0x40054d 处的ret指令。 该指令将返回地址 0x400568 从栈中弹出并跳转到该地址对应的指令，主函数得以继续执行。\n传递参数 在 x86-64 机器上，最多可以通过寄存器传递六个参数：\n参数从第七个开始将存储在运行时栈中。一个简单的 C 程序及其通过 GCC 编译后得到的汇编代码分别如下：\n1 2 3 4 5 6 7 8 9 10 11  void proc(long a1, long *a1p, int a2, int *a2p, short a3, short *a3p, char a4, char *a4p) { *a1p += a1; *a2p += a2; *a3p += a3; *a4p += a4; }   虽然参数a4的类型为 char，但程序分别通过8(%rsp)和16(%rsp)来对它和指针类型的a4p进行寻址，说明两者均占用了栈中 8 个字节的空间。参数的栈帧结构如下图所示：\n局部变量 某些情况下，局部变量必须存储在运行时栈中：\n 没有足够的寄存器来存储局部变量； 程序需要对局部变量进行取地址（\u0026）操作； 局部变量的类型为数组或结构体（我们将在后续的章节中讨论这种情况）。  示例函数call_proc的代码如下，其中调用的函数proc在上一节中已有介绍：\n1 2 3 4 5 6 7 8 9  long call_proc() { long x1 = 1; int x2 = 2; short x3 = 3; char x4 = 4; proc(x1, \u0026x1, x2, \u0026x2, x3, \u0026x3, x4, \u0026x4); return (x1 + x2) * (x3 - x4); }   该 C 程序生成的汇编代码十分冗长，但值得我们认真阅读和研究：\n图中Set up arguments to proc阶段对应的栈帧结构如下图所示：\n我们可以看到，局部变量x1到x4存储在栈中且有着不同的大小，分别占用字节 24-31、20-23、18-19 和 17。指向这些参数的指针均通过指令leaq生成，分别对应汇编代码中的第 7、10、12 和 14 行。前六个参数通过寄存器传递，而第七个和第八个参数则存储在栈中，相对于栈指针 %rsp 的偏移量为 0 和 8。\n当程序执行到Call proc阶段时，由于调用了函数proc，因此返回地址需要被压入到栈顶。此时的栈帧结构和上一节展示的相同，第七个参数和第八个参数相对于栈指针 %rsp 的偏移量分别变为了 8 和 16。\n被保存的寄存器 当一个过程（调用者）在调用另一个过程（被调用者）时，我们必须保证被调用者的执行不会影响到调用者后续计划使用的寄存器值。因此，x86-64 规定寄存器 %rbx、%rbp 和 %r12–%r15 为被调用者保存（callee-saved）寄存器。被调用者通过将上述寄存器中的值压入栈中，然后在返回时将原始值弹出以实现调用前后寄存器的值不变。除栈指针 %rsp 以外的其他寄存器则为调用者保存（caller-saved）寄存器，由于被调用者可以随意修改其中的值，因此调用者有责任保存调用之前的数据。\n递归（Recursive） x86-64 允许过程以递归的方式调用自身，这是因为每个过程在运行时栈上的空间是私有的，因此多个未完成调用的局部变量不会互相干扰。递归调用实质上和调用一个其他过程没有区别。\n数组 对于长度为 L 的数据类型 T 和整型常量 N，声明T A[N]代表：\n 内存中将为其分配 L * N 大小的空间； 数组名称 A 为指向数组头部（设为$x_A$）的指针，任意数组元素 i 的地址为 $x_A$ + L * i。  在 x86-64 中，内存引用指令的设计旨在简化对数组元素的访问。例如 int 类型的数组 E[i]，E 的地址存储在寄存器 %rdx 中，i 则存储在寄存器 %rcx 中。那么我们就可以通过指令movl(%rdx, %rcx, 4), %eax来将目标数组元素拷贝到寄存器 %eax 中。\nC 允许我们对指针进行计算。例如 p 是一个指向长度为 L 的数据类型为 T 的指针且 p 的值为 $x_p$，则表达式 p + i 的值为 $x_p$ + L * i。进一步地，任意数组元素 A[i] 就等效于表达式 *(A + i)。\n还是以数组 E[i] 为例，一些指针算数表达式的结果和对应的汇编指令如下：\n最后一个例子表明，我们可以计算同一数据结构中两个指针的差值。其结果的数据类型为 long， 值为两个地址的差值除以数据类型的长度。\n多维数组 多维数组可以转化为一般数组的形式，例如声明int A[5][3]就等效为：\n1 2  typedef int row3_t[3]; row3_t A[5];   数据类型 row3_t 是一个包含三个整型的数组，而数组 A 则包含五个这样的元素。我们将其推广到一般情况，若一个数组声明为T D[R][C]，则数组元素D[i][j]在内存中的地址为：\n$$\\tag{3.1} \\And D[i][j] = x_D + L(C * i + j)$$\n其中，$x_D$ 为数组地址，L 为数组元素的长度。\n一个 5 X 3 的整型数组 A，其任意数组元素 A[i][j] 的地址用汇编代码的表示结果为：\n; A in %rdi, i in % rsi and j in %rdx ; Compute 3i leaq (%rsi, %rsi, 2), %rax ; Compute A + 12i leaq (%rdi, %rax, 4), %rax ; Read from M[A + 12i + 4j] movl (%rax, %rdx, 4), %rax 定长数组 编译器可以对一些操作定长多维数组的代码进行优化。例如一个进行矩阵运算的 C 程序：\n1 2 3 4 5 6 7 8 9 10 11  #define N 16 typedef int fix_matrix[N][N]; /* Compute i,k of fixed matrix product */ int fix_prod_ele(fix_matrix A, fix_matrix B, long i, long k) { long j; int result = 0; for (j = 0; j \u003c N; j++) result += A[i][j] * B[j][k]; return result; }   它可以被优化为下列代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  #define N 16 typedef int fix_matrix[N][N]; /* Compute i,k of fixed matrix product */ fix_prod_ele_opt(fix_matrix A, fix_matrix B, long i, long k) { int *Aptr = \u0026A[i][0]; /* Points to elements in row i of A */ int *Bptr = \u0026B[0][k]; /* Points to elements in column k of B */ int *Bend = \u0026B[N][k]; /* Marks stopping point for Bptr */ int result = 0; do { /* No need for initial test */ result += *Aptr * *Bptr; /* Add next product to sum */ Aptr++; /* Move Aptr to next column */ Bptr += N; /* Move Bptr to next row */ } while (Bptr != Bend); /* Test for stopping point */ return result; }   比较两者我们可以发现，优化代码没有使用索引 j，并将所有的数组引用都转换为了指针引用。其中，Aptr指向矩阵 A 中第 i 行中的连续元素，Bptr指向矩阵 B 中第 k 列 中的连续元素。Bend则指向矩阵 B 中第 k 列中的第 N + 1 个元素，它就等于循环结束时Bptr的值。\n变长数组 C 只支持可以在编译时确定长度的多维数组(一维可能除外），声明可变大小的数组时必须使用malloc或calloc等函数来分配数组的存储空间，并且需要通过行主索引（row-major indexing）将多维数组的映射显式编码为一维数组（就像 公式 3.1 做的那样）。\n我们可以编写一个函数来访问 n × n 数组的元素 i, j，如下所示：\n1 2 3 4  int var_ele(long n, int A[n][n], long i, long j) { return A[i][j]; }   参数n必须在参数A[n][n]之前，这样函数在处理数组时才能够明确其维度。该程序经 GCC 编译得到的汇编代码如下：\n; int var_ele(long n, int A[n][n], long i, long j) ; n in %rdi, A in %rsi, i in %rdx, j in %rcx var_ele: ； Compute n * i imulq %rdx, %rdi ; Compute A + 4(n * i) leaq (%rsi,%rdi,4), %rax ; Read from M[A + 4(n * i) + j] movl (%rax,%rcx,4), %eax ret 数组元素 A[i][j] 地址的计算方式与定长多维数组类似，即 $x_A + 4(n * i) + 4j = x_A + 4(n * i + j)$。区别之处在于：\n 由于添加了变量n，寄存器的使用方式不同； 使用了乘法指令imulq而不是leaq来计算 n * i，因此将导致程序性能的损失。  无论数组的长度是否为常量，编译器都会对操作多维数组的代码进行一定优化。虽然二者实现的细节有所差异，但其宗旨是一致的：避免直接使用 公式 3.1 而导致的乘法运算。\n异构数据结构 结构体 结构体（Structure）的每个部分都存储在连续的内存空间中，指向结构体的指针是其第一个字节的地址。一个简单的结构体声明如下：\n1 2 3 4 5 6 7  struct rec { int i; int j; int a[2]; int *p; };   该结构体包含了 4 个 字段：2 个 4 字节的 int 变量，1 个两元素 int 型数组和一个 8 字节的 int 型指针，共占用 24 字节的内存空间：\n汇编代码可以通过在结构体地址上添加适当的偏移量来访问结构体中的任意字段。假设structure rec *类型的变量r存储在寄存器 %rdi 中，那么下列代码将元素r -\u003e i，即(*r).i，拷贝到r -\u003e j：\nmovl (%rdi), %eax movl %eax, 4(%rdi) 同样，如果我们要获取结构体中数组元素rec.a[i]的地址\u0026(r -\u003e a[i])，只需：\n; r in %rdi, i in %rdi leaq 8(%rdi, %rsi, 4), %rax 如上述汇编代码所示，程序对结构体字段的选择是在编译时完成的，因此机器代码中不含有任何有关字段声明或字段名称的信息。\n联合体 联合体（Unions）的声明和语法与结构体相同，但其不同字段均引用相同的内存空间。例如一个结构体和一个联合体的声明如下：\n1 2 3 4 5 6 7 8 9 10 11 12  struct S3 { char c; int i[2]; double v; }; union U3 { char c; int i[2]; double v; };   那么S3和U3各个字段的偏移量和总数据长度为：\n   Type c i v Size     S3 0 4 16 24   U3 0 0 0 8    对于类型为union U3 *的指针p，表达式p -\u003e c，p -\u003e i[0]和p -\u003e v都将引用联合体的起始地址。联合体的长度等于其所有字段中最大的数据长度，而结构体则等于其所有字段长度的和。我们将在下一节 数据对齐 中介绍为什么i在S3中的偏移量是 4 而不是 1，以及v的偏移量是 16，而不是 9 或 12。\n如果一个数据结构中的两个字段是互斥（mutually exclusive）的，我们就可以使用联合体来减少空间浪费。假如存在一个二叉树，其每个叶节点（leaf node）都有两个双精度浮点值，每个内部节点（internal node）都有两个指向子节点的指针。那么使用结构体实现该数据结构的代码为：\n1 2 3 4 5 6 7 8  struct node_s { // for internal node  struct node_s *left; struct node_s *right; // for leaf node  double data[2]; };   这样每个节点都需要占用 32 字节的空间，其中只有一半会被节点真正使用。而如果我们使用联合体来实现：\n1 2 3 4 5 6 7 8 9  union node_u { struct { union node_u *left; union node_u *right; } internal; double data[2]; };   则每个节点就只需占用 16 个字节的空间。对于类型为union node_u *的指针n，叶节点可以用n -\u003e data[0]和n -\u003e data[1]来表示，而内部节点指向的子节点可以用n -\u003e internal.left和n -\u003e internal.right来表示。\n联合体还可用于访问不同数据类型的位模式（bit pattern）。假设我们需要将一个 double 类型的变量强制转换为 unsigned long，则可以用如下方法实现：\n1 2 3 4 5 6 7 8 9 10  unsigned long double2bits(double d) { union { double d; unsigned long u; } temp; temp.d = d; return temp.u; };   该代码将两种数据类型均存储在联合体temp中，使其有着相同的位级表达，从而实现了类型转换。\n数据对齐 许多计算机系统要求某些对象的地址必须是某个值（通常为 2，4或8）的倍数，其目的是简化处理器和内存系统之间的硬件接口设计。假设一个处理器每次都从内存中读取 8 个字节，而一个 double 类型的数据地址为 8 的倍数，那么它就可以被一次内存访问操作读取或写入。否则，它会被拆分为两个 8 字节的内存块，导致处理器操作次数的增加。\n无论数据对齐是否实现，x86-64 的硬件都将正常工作。但是 Intel 建议对齐数据以提高内存系统性能，其规则为：任何 K 字节对象的地址必须为 K 的倍数：\n   K Types     1 char   2 short   4 int, float   8 long, double, char *    编译器会在汇编代码中放置指令，指示全局数据所需的对齐方式。比如我们在介绍跳转表时，示例代码中的.align 8就代表该指令后面的数据地址均为 8 的倍数。对于涉及到结构体的代码，编译器可能还需要在字段分配空间时插入间隙以实现数据对齐。一个简单结构体的声明如下：\n1 2 3 4 5 6  struct S1 { int i; char c; int j; };   编译器实际为其分配的内存空间为：\n即然各字段的偏移量均为 4 的倍数，那么只要起始地址也为 4 的倍数，该结构体就实现了数据对齐。\n机器级代码中控制和数据的组合 理解指针  指针不是机器代码的一部分，而是 C 提供的一种用来帮助程序员避免寻址错误的抽象； 每个指针都有一个关联的类型。特殊的指针类型void *代表通用（范型）指针，可以显式或隐式地转换为有关联类型的指针； 每个指针都有一个值，其值为指定类型的某个对象的地址。若指针的值为NULL(0)，则代表它没有指向任何地方； 指针是用操作符\u0026创建的，在机器代码中常用leaq指令实现； 使用操作符*来引用指针； 数组和指针之间关系密切； 指针可以被强制类型转换，但不会改变其值； 指针也可以指向函数，使程序可以在其他地方调用代码，其值为函数的机器代码中第一条指令的地址。  例如一个函数的原型为：\n1  int fun(int x, int *p);   我们可以声明一个指针来指向它：\n1 2  int (*fp)(int, int *) fp = fun;   这样便可以使用指针来调用该函数：\n1 2  int y =1; int result = fp(3, \u0026y);   内存引用越界和缓冲区溢出 C 不会对数组引用做任何的边界检查，这可能导致存储在栈中的数据因写入越界（out-of-bounds）的数组元素而损坏。如果程序随后以这种状态重新加载寄存器或执行ret指令，事情可能会变得十分严重。\n一种常见的状态损坏原因称为缓存区溢出（buffer overflow），比较典型的例子是字符串的长度超过了栈中为其分配的字符数组空间：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  /* Implementation of library function gets() */ char *gets(char *s) { int c; char *dest = s; while ((c = getchar()) != '\\n' \u0026\u0026 c != EOF) *dest++ = c; if (c == EOF \u0026\u0026 dest == s) /* No characters read */ return NULL; *dest++ = '\\0'; /* Terminate string */ return s; } /* Read input line and write it back */ void echo() { char buf[8]; /* Way too small! */ gets(buf); puts(buf); }   上半部分代码展示了库函数gets的实现过程：它首先从标准输入中读取一行字符串，当遇到换行符或某些错误条件时停止；然后它将读取内容复制到参数s指向的位置上，并以字符null结尾。gets的问题在于其无法确定是否分配了足够的空间来保存读取到的字符串。而在下半部分的函数echo中，我们故意将缓冲区做得非常小（只有 8 个字节），因此任何超过 7 个字符的字符串都会导致越界写入。随着字符串长度的增加，受到影响的栈区域会越来越多：\n   Character typed Additional corrupted state     0-7 None   9-23 Unused stack space   24-31 Return Address   32+ Saved state in caller    编译器为函数echo分配了 24 个字节的空间，因此读取少于 23 个字符的内容不会发生严重后果。但一旦超出这个范围，返回指针的值和其他保存的状态就会被破坏，ret指令将导致程序跳转到一个完全不可预知的位置。\n阻止缓冲区溢出攻击 缓冲区溢出甚至会导致计算机受到网络攻击，危害系统安全。受到攻击的程序会接受一个字符串，包含一些可执行代码的字节编码（称为漏洞利用代码），以及一些额外字节。只要额外字节能够将返回地址覆盖为指向漏洞利用代码的指针，程序执行ret指令时就会跳转到漏洞利用代码。现代编译器和操作系统已经开始使用一些机制来抵御这种攻击。\n随机化栈 只要我们在程序开始运行时在栈上分配 0 到 n 个字节之间的随机空间（例如使用分配函数alloca），就可以让相同的代码在多次运行中使用不同的栈地址。分配的空间范围 n 需要足够大，以便栈地址能够发生足够的变化。但又要足够小，否则程序将浪费太多的内存空间。\n不过攻击者可以在实际的漏洞利用代码前加入一长串的nop指令（即 no operation，不做任何操作），从而暴力攻克随机化栈。因此随机化栈可以增加成功攻击系统所需的工作量，但不能提供可靠完善的保障。\n栈损坏检测 我们还可以在局部缓冲区和栈的剩余部分之间插入一个金丝雀值（Canary）来检测栈是否损坏：\n限制可执行代码区域 最后，我们可以限制只有保存了编译器生成的代码的那部分内存区域是可执行的，其他部分被限制为只读或只写。\n支持可变大小的栈帧 在之前我们介绍的汇编代码中，编译器为程序分配的运行时栈大小都是确定的。而有些程序则需要大小可变的运行时栈，例如：\n1 2 3 4 5 6 7 8 9  long vframe(long n, long idx, long *q) { long i; long *p[n]; p[0] = \u0026i; for (i = 1; i \u003c n; i++) p[i] = q; return *p[idx]; }   函数vframe声明了一个变长的指针数组*p[n]，将在栈中占用 8n 个字节的空间。由于 n 的值是由函数第一个参数给出的，因此编译器无法确定要为该函数的运行时栈分配多少空间。另外，程序涉及了对局部变量i地址的引用，所以该变量必须存储在栈中。当函数返回时，运行时栈需要被回收，栈指针将指向存储返回地址的位置。\n为了管理可变大小的栈帧，x86-64 使用寄存器 %rbp 作为帧指针（又称为基指针，base pointer）。栈帧结构如下图所示：\n上文提到，%rbp 是一个被调用者保存寄存器，因此其原值将被保存在栈中（图中的 Saved %rbp）。在程序的执行过程中，%rbp 会一直指向这个位置。一些固定长度的局部变量，比如i，就可以根据其相对于 %rbp 的偏移量来引用。函数编译后生成的部分汇编代码如下：\n函数首先将 %rbp 压入栈中，然后将其置为当前运行时栈的地址（汇编代码第 2 到 3 行）。接下来在栈上分配了 16 个字节的空间，栈指针指向图中 s1 的位置。前 8 个字节用于存储局部变量i，后 8 个字节并未使用。然后它开始为数组*p[n]分配空间，栈指针指向图中 s2 的位置。其相对于 s1 的偏移量是通过汇编代码第 5 到 7 行计算得到的：\n$$ (8n + 22) \\And (-16)= \\begin{cases} 8n + 8 \u0026\\text n 为奇数 \\cr 8n + 16 \u0026\\text n 为偶数 \\end{cases} $$\n由于该结果是 16 的倍数，因此实现了数据对齐。汇编代码第 8 到 10 行将图中 p 的值置为最接近 s2 的 8 的倍数，最终由寄存器 %rcx 中的值作为*p[n]的起始地址。假设 n 为 5 或 6，s1 的值为 2065 或 2064，则图中各值为：\n   n s1 s2 p e1 e2     5 2065 2017 2024 1 7   6 2064 2000 2000 16 0    在函数执行结束前，汇编代码第 20 行的leave指令将帧指针恢复原值。相当于执行了以下两条指令：\n; Set stack pointer to beginning of frame movq %rbp, %rsp ; Restore saved %rbp and set stack ptr ; to end of caller’s frame popq %rbp 浮点代码 处理器的浮点架构由操作浮点数的程序如何映射到机器级代码的不同方面组成，包括：\n 浮点数是被如何被存储和访问的； 操作浮点数的指令； 浮点数如何作为参数传递给函数以及如何作为结果返回； 在函数调用时如何保存寄存器中的值。  我们的内容基于 AVX2（Adcanced Vector Extensions2）扩展，可以在 GCC 编译时加入-mavx2参数以生成这种架构的代码。\n如上图所示，AVX 架构允许浮点数存储在 16 个 YMM 寄存器中，每个长度均为 256 位（32 字节）。在对标量数据进行操作时，这些寄存器只会保存浮点型数据。而对于 float 类型和 double 类型，分别只有较低的 32 位和 64 位被使用。汇编代码通过 XMM 寄存器（即图中的 %xmm0–%xmm15）的名称来引用它们，每个 XMM 寄存器是其对应的 YMM 寄存器的低 128 位（16 字节）。\n浮点数的移动和转换操作 下图展示了一组在内存和 XMM 寄存器之间和在两个 XMM 寄存器之间不进行类型转换的传输数据浮点数指令：\n前四个指令涉及到对内存的引用，我们称其为 Scalar 指令。它们操作的对象是单独的数值，只会改变目的寄存器低位的四字节或八字节。而后两个指令则属于 Packed 指令，它们会更新目的寄存器中全部的内容。\n浮点数和整型之间进行转换的操作指令为：\n在 C 和大多数编程语言中，浮点数转换为整型时会先进行截断操作，将数值向零舍入。而整型转化为浮点数时，我们一般会忽略第二个源操作数，因为它只影响结果的高位字节。因此，通常情况下第二个源操作数和目的操作数是相同的。\n最后则是两种浮点数（float 和 double）之间的类型转换。假设寄存器 %xmm0 的低位四字节保存了一个单精度浮点值，那么我们很容易使用指令vcvtss2sd %xmm0, %xmm0, %xmm0来把它转换为双精度浮点值然后再存储于寄存器 %xmm0 的低位八字节中。然而实际上 GCC 生成的汇编代码为：\n; Replicate first vector element vunpcklps %xmm0, %xmm0, %xmm0 ; Convert two vector elements to double vcvtps2pd %xmm0, %xmm0 指令vunpcklps会交错两个 XMM 寄存器中的值，并将它们存储在第三个寄存器中。举例来说，如果两个源寄存器中的值分别为$[s_3, s_2, s_1, s_0]$和$[d_3, d_2, d_1, d_0]$，那么目标寄存器的值就是$[s_1, d_1, s_0, d_0 ]$。在上述代码中，三个操作数使用相同的寄存器，因此如果寄存器 %xmm0 中的原始值为$[x_3, x_2, x_1, x_0]$，则该指令将更新其值为$[x_1, x_1, x_0, x_0]$。\n指令vcvtps2pd将源 XMM 寄存器中的两个低位单精度值扩展为目的 XMM 寄存器中的两个双精度值。设 $dx_0$ 为将 $x_0$ 转换为双精度的结果，那么该指令将得出值$[dx_0, dx_0]$。综上，这两条指令的最终效果是将寄存器 %xmm0 中低位四字节中的原始单精度值转换为双精度值，并将它的两个副本存储在寄存器 %xmm0 中。\n同样，GCC 为双精度值转换为单精度生成类似的汇编代码：\n; Replicate first vector element vmovddup %xmm0, %xmm0 ; Convert two vector elements to single vcvtpd2psx %xmm0, %xmm0 假设寄存器 %xmm0 中包含了两个双精度值$[x_1, x_0]$，则执行上述代码的结果为$[0.0, 0.0, x_0, x_0]$。\n过程中的浮点代码 观察图 3.45 中最右侧的注释，我们可以发现以下准则：\n 最多可以使用 XMM 寄存器传递八个参数（%xmm0-%xmm7），其余的则需要通过栈； 和 %rax 类似，%xmm0 通常作为浮点数的返回寄存器； 所有 XMM 寄存器均为调用者保存（caller-saved），被调用者可以覆盖其中任意一个； 当函数参数是指针、整数和浮点数的组合时，指针和整数在通用寄存器中传递，而浮点值则在 XMM 寄存器中传递。  浮点数的算数操作 浮点数算数操作指令如下，每个指令都有一到两个源操作数和一个目的操作数。第一个源操作数可以是 XMM 寄存器或内存中的位置，第二个源操作数和目的操作数只能是 XMM 寄存器；\n浮点常量的定义和使用 与整数算术运算不同，AVX 浮点运算不能将立即数作为操作数。编译器必须为常量分配和初始化存储空间，再由代码从内存中读取值。\n浮点代码中的位级运算 下图展示了两个用于 XMM 寄存器的位级运算指令，其操作对象均为 Packed 数值（XMM 寄存器中全部的 128 位）：\n浮点数的比较操作 AVX 2 为浮点数值的比较运算提供两种操作指令：\n   Instruction Based on Description     ucomiss $S_1$, $S_2$ $S_2 - S_1$ Compare single precision   ucomisd $S_1$, $S_2$ $S_2 - S_1$ Compare double precision    上述指令与 条件码 中介绍的 CMP 指令类相似。参数 $S_2$ 必须是 XMM 寄存器，而参数 $S_1$ 则既可以是 XMM 寄存器，又可以是内存中的位置。\n浮点数比较操作会改变以下条件码的值，其中 PF 意为 Parity Flag：\n   Ordering $S_2$:$S_1$ CF ZF PF     Unordered 1 1 1   $S_2$ \u003c $S_1$ 1 0 0   $S_2$ = $S_1$ 0 1 0   $S_2$ \u003e $S_1$ 0 0 0    当任意操作数为 NaN 时，Unordered 的情况就会出现。PF 的值将被置为 1，对应的跳转指令为jp。其余三种情况则和整型的 跳转指令 相同，分别为jb、je和ja。\n","description":"","tags":["OS"],"title":"CSAPP 读书笔记：程序的机器级表示","uri":"/posts/machine-level-representation-of-programs-note/"},{"content":"信息的存储 大多数机器使用字节（8 位的块）作为存储器中的最小寻址单元，而非访问单独的位。内存中的每一个字节都对应一个唯一的数字，即它的地址，所有可能的地址集合构成了 虚拟内存。它是由 DRAM、闪存（flash memory) 和磁盘存储共同实现的，而在程序看来则只是一个统一的字节数组。\n编译器和运行时系统负责将这个内存空间划分为更加可管理的单元，来存储不同的程序对象。例如，C 中指针的值代表存储块中第一个字节的虚拟地址。C 编译器还将每个指针与其类型信息联系起来，这样就可以根据指针值的类型生成不同的机器级代码来访问指针所指向的值。不过机器级代码中并没有任何有关类型的信息，而是简单的把每个程序对象都视为一个字节块。\n十六进制表示法 使用二进制表示位模式（bit pattern）会非常冗长，因为一个字节就包含了 8 位。而如果使用十进制，则不方便与位模式进行互相转化，因此我们采用十六进制（Hexadecimal）来书写位模式。一个十六进制数占 4 位，因此一个字节的取值范围就是 $00_{16}$ ~ $FF_{16}$ 。\n将一个二进制数字转化为十六进制数字，需要首先将其分为多个 4 位的组，然后再将每组数转化为十六进制。如果总位数不为 4 的倍数，那么最左边的一组可以少于四位，然后在首位补 0。如 $111100_2$ 可以分成 $0011_2$ 和 $1100_2$，转化结果为 $3C_{16}$ 。在 C 中，若一个常数以 0x 或 0X 作为前缀，则代表它是一个十六进制数字。\n数据大小 每台计算机都有一个字长（word size），它指定了指针数据的标准大小。如果一台机器的字长为 $w$ 位，那么虚拟地址的范围为 $ 0～2^w -1 $，程序最多访问 $2^w$ 字节。32 位机器的虚拟地址大小约为 4GB，而 64 位机器则能达到 16EB。\nC 中几个基本数据类型的大小如下表所示：\n   Signed Unsigned 32-bit 64-bit     [signed] char unsigned char 1 1   short unsigned short 2 2   int unsigned 4 4   long unsigned long 4 8   int32_t uint_32t 4 4   int64_t uint_64t 8 8   char *  4 8   float  4 4   double  8 8    除 char 外，若不添加前缀 unsigned，则默认使用有符号类型。指针类型的数据使用机器的全字长，如 char *。由于某些数据类型的大小在不同机器上有所不同，因此开发人员应使程序对不同数据类型的确切大小不敏感，从而保证程序的可移植性（portable）。\n寻址和字节顺序 对于多字节的程序对象，我们必须建立两个准则：这个对象的地址是什么和这些字节在内存中的排列顺序是怎样的。某些机器选择按照从最低有效字节到最高有效字节的顺序存储对象，称为小端法（little endian）；而某些则与之相反，称为大端法（big endian）。如一个 int 类型的变量 x 地址为 0x100，其值为十六进制的 0x1234567，那么上述两种机器存储该变量的方式分别如下：\nlittle endian\n    0x100 0x101 0x102 0x103      … 67 45 23 01 …    big endian\n    0x100 0x101 0x102 0x103      … 01 23 45 67 …    有些时候不同的存储顺序可能导致一些问题：\n 不同类型的机器通过网络传递二进制数据，如小端法机器产生的数据发送到大端机器上，得到的字节序列是反的。这就要求发送方的机器需要将代码转换为网络标准，接收方再将其转化为其内部的表达方式； 对于小端法机器，书写字节序列与书写数字的顺序相反； 某些使用强制类型转换（cast）的程序，在不同类型的机器上编译运行的结果不同。  对于上述第三种情况，我们以一个程序为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  #include \u003cstdio.h\u003e typedef unsigned char *byte_pointer; void show_bytes(byte_pointer start, size_t len) { size_t i; for (i = 0; i \u003c len; i++) { // %.2x 代表整数会被打印为至少两位（digits）的十六进制数字  printf(\"%.2x\", start[i]); } printf(\"\\n\"); } void show_int(int x) { show_bytes((byte_pointer)\u0026x, sizeof(int)); } void show_float(float x) { show_bytes((byte_pointer)\u0026x, sizeof(float)); } void show_pointer(void *x) { show_bytes((byte_pointer)\u0026x, sizeof(void *)); } int main() { int val = 12345; float fval = (float) val; int *pval = \u0026val; show_int(val); show_float(fval); show_pointer(pval); }   不同类型机器的输出结果如下：\n   Machine val fval pval     Linux32 0x39300000 0x00e44046 0xe4f9ffbf   Windows32 0x39300000 0x00e44046 0xb4cc2200   Sun 0x00003039 0x4640e400 0xeffffa0c   Linux64 0x39300000 0x00e44046 0xb811e5ffff7f0000    这是因为 Sun 系统采用大端法，其他三者采用小端法。对于指针类型的变量，不同操作系统在存储分配上有着不同的准则。同时，64 位系统使用 8 字节地址，32 位系统使用 4 字节地址，这导致了指针类型的变量 pval 输出结果的不同。\n字符串 字符串在 C 中被编码为一个以 null 字符结尾（其值为 0）的字符数组，每个字符都由某种标准编码组成，如 ASCII 码。因此，如果我们执行上面的程序show_bytes(\"12345\", 6)，将得到 31 32 33 34 35 00。\n由于字符串中各字符的排列顺序是由字符串本身决定的，因此字符串不会受到大端法/小端法的影响，除非字符使用 16 位两字节的 Unicode 进行编码。\n代码 不同类型的机器使用不同且不兼容的指令和编码方式，因此二进制代码很少能在不同机器和操作系统组合之间移植。\n布尔代数简介 几种布尔运算符的定义如下：\n ～：非，相当于逻辑运算的 NOT； \u0026：与，相当于逻辑运算的 AND； ｜：或，相当于逻辑运算的 OR； ^：异或，相当于逻辑运算的 EXCLUSIVE-OR。若 $p = 0$，$q = 1$ 或 $p = 1$，$q = 0$ 时，$p \\text{\\textasciicircum} q = 1$。  布尔运算符可以应用于位向量，即固定长度的 0、1 序列。举例来说，若 a 为 [0110]，b 为 [0101]，那么：\n   Operation Result     ~a [1001]   a \u0026 b [0100]   a | b [0111]   a ^ b [0011]    C 中的位级运算 C 中支持按位布尔运算，上面提到的布尔运算符其实就是在 C 中使用的。一个使用布尔运算符的经典程序如下：\n1 2 3 4 5 6 7 8  #include \u003cstdio.h\u003e void inplace_swap(int *x, int *y) { *y = *x ^ *y; *x = *x ^ *y; *y = *x ^ *y; }   利用对于任意数 $a$ ，$a \\text{\\textasciicircum} a$ = 0 以及 $0 \\text{\\textasciicircum} a = a$ 这一性质，该程序不使用中间变量便完成了变量值的交换。另外，上述方法的实现还建立在异或运算满足交换律和结合律的基础之上。\nC 中的逻辑运算 C 还提供了逻辑运算符，即 || 、\u0026\u0026 和 !。它们和位级运算的一个区别就是，如果对第一个参数求值就能确定表达式的结果，那么就不会对第二个参数进行求值。如表达式 a \u0026\u0026 5 / a 不会导致除数为 0 的异常，而 p \u0026\u0026 *p++ 也不会导致简介引用空指针。\nC 中的移位运算 C 中的移位运算有左移和右移两种，均不会改变位向量的长度。左移运算 $x « k$ 就是 x 向左移动 k 位，丢弃 k 个高位，并在右端补充 k 个 0。通常可以使用 $x « 1$ 和 $x « 2$ 分别代替 $x \\times 2$ 和 $x \\times 4$，因为位级运算拥有相比乘法更快的运算速度。\n右移运算分为两种形式，逻辑和算术。无符号数据必须使用逻辑右移，$x » k$ 将 x 的左端补充 k 个 0 ，并丢弃 k 个低位。而大多数机器使用算术右移处理有符号数据，$x » k$ 将 x 的左端补充 k 个最高有效位的拷贝，并丢弃 k 个低位。\n加减乘除运算符的优先级大于移位运算符，因此 $1 « 2 + 3 « 4$ 等效于 $1 « (2 + 3) « 4$。\n整数的表示 无符号编码 一个长度为 $w$ 的位向量 $\\vec{x} = [x_{w-1},x_{w-2},…,x_0]$，它从二进制（Binary）转化为无符号编码（Unsigned）的公式为：\n$$ B2U_w(\\vec{x}) \\doteq \\displaystyle\\sum_{i=0}^{w - 1}x_i2^i $$\n其中， $\\doteq$ 符号表示等式的左手边被定义为右手边。无符号编码的最小值为位向量 [00…0]，即整数值 0。最大值为位向量 [11…1]，即整数值 $U\\max_w \\doteq \\displaystyle\\sum_{i=0}^{w - 1}2^i = 2^w -1$。函数 $B2U_w$ 是一个双射（bijection）：对于每个长度为 $w$ 的位向量，都有唯一的整数值与之对应，反之亦然。\n二进制补码 二进制补码（Two’s-Complement Encoding）将位向量的符号位（即最高有效位）作为负权重（negative weight），符号位为 1 代表值为负，符号位为 0 代表值为正。如一个长度为 $w$ 的位向量 $\\vec{x} = [x_{w-1},x_{w-2},…,x_0]$，它从二进制转化为二进制补码的公式为：\n$$ B2T_w(\\vec{x}) \\doteq -x_{w - 1}2^{w - 1} + \\displaystyle\\sum_{i=0}^{w - 2}x_i2^i $$\n二进制补码的最小值为位向量 [10…0]，即整数值 $T\\min_w \\doteq -2^{w-1}$。最大值为位向量 [01…1]，即整数值 $T\\max_w \\doteq \\displaystyle\\sum_{i=0}^{w - 2}2^i = 2^{w-1} -1$。与二进制转化无符号编码类似，函数 $B2T_w$ 也是一个双射。而它们的最值之间有着如下性质：\n$$ |T\\min| =|T\\max| + 1$$ $$ U\\max = 2T\\max + 1$$\n特别地，整数 -1 和 $U\\max_w$的位级表示均为全 1：[11…1]，而整数 0 在两种表达方式中均为全 0：[00…0]。虽然 C 的标准并没有限制有符号整数的二进制表达，但大多数机器都采用了二进制补码的方式。\n有符号数和无符号数的转换 在 C 中，有符号数和无符号数之间的转换是基于位级视角的，而非数字。例如：\n1 2 3  short int v = -12345; unsigned short uv = (unsigned short) v; printf(\"v = %d, uv = %u\\n\", v, uv);   输出结果为 v = -12345，uv = 53191。这意味着在类型转换过程中位向量不变，但位向量转换到整数值的方式不同。根据推导，二进制补码转换为无符号数的公式如下：\n$$T2U_w(x) = \\begin{cases} x + 2^w \u0026\\text x \u003c 0 \\cr x \u0026\\text x \\geq 0 \\end{cases}$$\n如图所示，非负数转换前后保持不变，负数则变成了一个较大的正数：\n而无符号数转换为二进制补码的公式则为：\n$$U2T_w(u) = \\begin{cases} u \u0026\\text u \\leq T\\max_w \\cr u - 2^w \u0026\\text u \u003e T\\max_w \\end{cases}$$\n如图所示，小于 $2 ^ {w - 1}$ （最高有效位为 0）的数转换前后保持不变，大于等于 $2 ^ {w - 1}$ （最高有效位为 1）的数将被转换为一个负数：\n通过上面的讨论我们发现，大于等于 0 且小于 $2 ^ {w - 1}$ 的值有相同的无符号和二进制补码表示。而这个范围之外的数，在转换过程中需要加上或减去 $2 ^ w$。\nC 中的有符号数和无符号数 通常大多数数字默认都是有符号的，除非加上后缀 U 或 u，如 12345U 和 0x1A2Bu 等。除了显式地使用强制类型转换（如上一节中的程序）以外，也可以将一种类型的表达式赋值给另一种变量，即隐式转换：\n1 2 3 4  int tx, ty; unsigned ux, uy; tx = ux; /* Cast to signed*/ uy = ty; /* Cast to usigned*/   如果参与运算的两个数一个是有符号的，一个是无符号的，那么 C 会隐式地将有符号数转换为无符号数并假定两数均为非负后再进行计算。比如表达式 -1 \u003c 0U 的值为 false，因为 -1 会先转换为无符号数再与 0 进行比较。\n扩展一个数的位级表示 要将一个无符号数转换为更大的数据类型，只需简单地在头部添加 0，这种运算被称为零扩展（zero extension）。而对于二进制补码，则需要在头部添加最高有效位（符号位），称为符号扩展（sign extension）。\n当类型转换既改变数据类型的大小又改变符号类型时，则先改变大小，再改变有无符号。例如将一个 short 类型的变量转换为 unsigned 类型，实际上先将它转换为了 int 类型，再从 int 转换为了 unsigned。\n截断数字 将一个数转换为更小的数据类型时，截断数字的位数是不可避免的。如一个 $w$ 位的位向量 $\\vec{x} = [x_{w-1},x_{w-2},…,x_0]$截断为 $k$ 位时，我们会丢弃 $w-k$ 个高位，得到 ${\\vec{x} = [x_{k-1},x_{k-2},…,x_0]}$。截断数字可能会导致值的变化：\n$$B2U_k([x_{k-1},x_{k-2},…,x_0]) = B2U_w([x_{w-1},x_{w-2},…,x_0]) \\bmod 2^k $$ $$B2T_k([x_{k-1},x_{k-2},…,x_0])= U2T_k(B2U_w([x_{w-1},x_{w-2},…,x_0]) \\bmod 2^k)$$\n通过上面几小节的讨论，我们发现无符号数与有符号数之间的隐式转换导致了一些与常识相悖的运算结果，这将导致一些很难发现的程序错误。因此很多编程语言，如 Java，不支持无符号数的使用。\n整数的运算 无符号加法 两个可用 $w$ 位无符号编码表示的非负整数 $x$ 和 $y$，其范围：$0 \\leq x, y \\leq 2^w -1$，那么它们的和：$0 \\leq x+ y \\leq 2^{w + 1} -2$ 就有可能需要用 $w+1$ 位来表示。如果出现溢出便丢弃高位，因此无符号加法（$+_w^u$）等价于计算 $(x+y) \\bmod 2^w$，即：\n$$x + _w^uy = \\begin{cases} x + y \u0026\\text x + y \u003c 2^w \u0026\\text Normal \\cr x + y - 2^w \u0026\\text 2^w \\leq x + y \u003c 2^{w+1} \u0026\\text \\space \\space \\space Overflow \\end{cases}$$\n模数加法构成了一种数学结构，称为阿贝尔群，它是可交换的和可结合的。$w$ 位无符号数的集合执行 $+_w^u$ 运算，对于每个值 $x$，必然有某个值 $-_w^ux$ 满足 $-_w^ux +_w^ux = 0$，该值称为 $x$ 的逆元。当 $x = 0$ 时，其逆元自然为 0。当 $x \u003e 0$ 时，显然 $ (x + 2^w - x)\\bmod 2^w = 0$。由于 $0 \u003c 2^w -x \u003c 2^w$，因此 $2^w -x$ 便是 $x$ 的逆元。上述两种情况总结如下：\n$$ -_w^ux = \\begin{cases} x \u0026\\text x = 0 \\cr 2^w -x \u0026\\text x \u003e 0 \\end{cases}$$\n二进制补码加法 两个数的 $w$ 位二进制补码之和（$+_w^t$）与无符号之和有着完全相同的位级表示，因此对于 $-2^{w-1} \\leq x, y \\leq 2^{w-1} -1$ ，有：\n$$\\begin{split} x +_w^ty \u0026=U2T_w(T2U_w(x) +_w^uT2U_w(y))\\cr \u0026=U2T_w[(x+y)\\bmod 2^w] \\end{split}$$\n进一步地，我们根据两数之和的范围，将上述结果分情况讨论，从而得到：\n$$x + _w^ty = \\begin{cases} x + y + 2^w \u0026\\text x + y \u003c -2^{w-1} \u0026\\text \\space \\space \\space Negative \\space \\space Overflow \\cr x + y \u0026\\text -2^{w-1} \\leq x + y \u003c 2^{w-1} \u0026\\text Normal \\cr x + y - 2^w \u0026\\text x + y \\geq 2^{w-1} \u0026\\text \\space \\space \\space Postive \\space \\space Overflow \\end{cases}$$\n因此，若 $x\u003e0, y\u003e0, x+_w^ty\\leq 0$，那么结果便出现了正溢出；若 $x\u003c0, y\u003c0, x+_w^ty\\geq 0$，结果便出现了负溢出。\n二进制补码的逆元计算公式如下：\n$$ -_w^tx = \\begin{cases} T\\min_w \u0026\\text x = T\\min_w \\cr -x \u0026\\text x \u003e T\\min_w \\end{cases}$$\n无符号乘法 无符号乘法运算（$\\times_w^u$）与加法类似，都可以转换为对 $2^w$ 的模运算：\n$$x\\times_w^uy=(x \\times y)\\bmod2^w$$\n二进制补码乘法 同样与加法类似，两个数的 $w$ 位二进制补码之积（$\\times_w^t$）与无符号之积有着完全相同的位级表示，因此：\n$$\\begin{split} x \\times_w^ty \u0026=U2T_w(T2U_w(x)\\times_w^uT2U_w(y))\\cr \u0026=U2T_w[(x \\times y)\\bmod 2^w] \\end{split}$$\n乘以常数 整数乘法运算在许多机器上运行缓慢，一个重要的优化便是使用移位运算和加法运算来代替它。我们首先考虑乘以 2 的幂的情况，然后推广到任意常数。若存在位向量 $[x_{w-1},x_{w-2},…,x_0]$，$x \\times 2^k$ 可以表示为在其右端添加 $k$ 个 0，即 $[x_{w-1},x_{w-2},…,x_0,0,…,0]$。因此在 C 中，对于整数 $x$ （无论是无符号数还是二进制补码）和 无符号数 $k$，$x \\times _w^t2^k$ 就等于 $x«k$。\n如果一个常数可以拆分为 2 的幂的和，那么我们便可以使用左移运算和加（减）法运算来替换与相关的乘法运算。如 $14=2^3+2^2+2^1$，那么 $x\\times14=(x«3)+(x«2)+(x«1)$。\n除以 2 的幂 我们使用右移运算来代替除法运算，而逻辑右移和算术右移分别适用于无符号数和二进制补码。由于结果为整数，因此很可能需要进行舍入（round）。我们定义 $⌊ ⌋$ 为向下取整，$⌈ ⌉$ 为向上取整。如 $⌊3.14⌋=3$，$⌊-3.14⌋=-4$，而 $⌈3.14⌉=4$，$⌈-3.14⌉=-3$。\n在 C 中，对于无符号数 $x, k$， $x»k=⌊x/2^k⌋$；对于二进制补码 $x$ 和无符号数 $k$，则有 $x»k=⌊x/2^k⌋$。前者为逻辑右移，后者为算术右移。\n考虑到若 $x\u003c0$，$x/y$ 的结果应为 $⌈x/y⌉$，而非 $⌊x/y⌋$。我们可以利用性质：$⌈x/y⌉=⌊(x+y-1)/y⌋$，来修正这种不合适的舍入。因此，对于 $x\u003c0$ 的二进制补码除法，应使用：$(x+(1«k)-1)»k=⌈x/2^k⌉$。综上，二进制补码除以 2 的幂 $x/2^k$ 可以用三元运算符表示为：\n1  (x\u003c0 ? x+(1\u003c\u003ck)-1 : x) \u003e\u003e k   与乘法不同，除法无法推广到任意常数。\n浮点 二进制小数 十进制小数的表示方法为 $d_md_{m-1}…d_1d_0.d_{-1}d_{-2}…d_{-n}$，其中 $d_i$ 为 0～9 的整数。那么该数的大小为：\n$$d=\\displaystyle\\sum_{i=-n}^{m}10^id_i$$\n小数点左边的数的权值为 10 的正幂，右边的则为 10 的负幂。类似地，我们可以得出二进制小数的表示方法。$b_i$ 为 0 或 1，则二进制数 $b_mb_{m-1}…b_1b_0.b_{-1}b_{-2}…b{-n}$ 的值为：\n$$b=\\displaystyle\\sum_{i=-n}^{m}2^ib_i$$\n二进制小数点向左移动一位，相当于数字除以 2。向右移动一位，则相当于数字乘以 2。这种方法只能表示可转化为 $x \\times 2^y$ 形式的数，无法精确表示如 $\\frac{1}{3}$、$\\frac{5}{7}$ 这样的数。\nIEEE 浮点数表示 二进制小数的表示方法难以表示很大的数，我们更希望通过给定 $x, y$ 的值来表示形如 $x \\times 2^y$ 的数。IEEE 浮点数标准使用 $V=(-1)^s\\times M \\times 2^E$ 的形式来表示小数：\n 符号 $s$：为 1 代表负值，为 0 代表正值； 有效数 $M$：一个二进制小数，范围在 1 到 $2-\\varepsilon$ 或 0 到 $1-\\varepsilon$ 之间； 指数 $E$：2 的幂指数，有可能是负数。  因此，浮点数的位级表达分为了三个部分：\n 一个符号位 $s$; $k$ 位的指数域 $exp=e_{k-1}…e_1e_0$ 编码指数 $E$； $n$ 位的小数域 $frac=f_{n-1}…f_1f_0$ 编码有效数 $M$。  C 中的 float 类型，$s=1, k=8, n=23$。而对于 double 类型，$s=1, k=11, n=52$。IEEE 浮点数表示法有三种情况，如下图所示：\n第一种情况是最常见的，即指数域不全为 0，也不全为 1。在这种情况下，指数域的值为 $E=e-Bias$，其中 $e$ 是一个位级表达为 $e_{k-1}…e_1e_0$ 的无符号数，$Bias$ 则是一个 $2^{k-1}-1$ 的常数。小数域的值为 $M=1+f$，其中，$f$ 是一个二进制小数 $0.f_{n-1}…f_1f_0$。\n第二张情况是指数域全为 0，这样所表示的数就是非标准化形式的。在这种情况下，$E=1-Bias, M=f$。非标准化数可以表示第一种情况无法表示的 0 以及非常接近 0 的数字。\n第三种情况是指数域全为 1 时出现的。若小数域全为 0，得到的值则为 $\\pm \\infin$。若小数域不全为 0，则结果为 NaN，即不是一个数字。比如计算 $\\infin -\\infin$ 和 $\\sqrt{-1}$，就会得到这样的结果。\n以 8 位浮点数为例，$s=1, k=4, n=3$，此时偏移量 $Bias=2^{4-1}-1=7$。最靠近 0 的是非标准化数，$E=1-Bias=-6$，$2^E=\\frac{1}{64}$，$M=f=0,\\frac{1}{8},…,\\frac{7}{8}$，因此浮点数 $V$ 的范围就是 0 ～$\\frac{7}{8\\times64}=\\frac{7}{512}$。而对于最小的标准数来说，指数域为 [0001]，因此 $E=e-Bias=-6$，小数域 $M=1+f=1,\\frac{9}{8},…\\frac{15}{8}$，浮点数 $V$ 的范围为 $\\frac{8}{512}=\\frac{1}{64}$ ~ $\\frac{15}{512}$。\n我们可以观察到最大非标准数和最小标准数分别为 $\\frac{7}{512}$ 和 $\\frac{8}{512}$，这种平滑的过渡得益于我们将非标准数的 $E$ 使用 $1-Bias$ 来计算，而非 $-Bias$。\n在这种条件下，当指数域为 [1110]，$E=e-Bias=7, 2^E=128$，小数域 $M=1+0.111_2=\\frac{15}{8}$ 时，$V$ 取到最大值 240，超出这个值就会溢出到 $+\\infin$。值得一提的是，IEEE 浮点数可以使用整数排序函数来进行排序。\n舍入 对浮点数的表示限制了其范围和精度，因此浮点计算只能近似地表示实数计算。IEEE 浮点数格式定义了四种不同的舍入方式：\n向偶数舍入（Round-to-even），也称向最接近的数舍入（Round-to-nearest），是默认的方法。它试图找到一个最接近的匹配值，对于中间值（如表中的 1.5），则使结果的最低有效位为偶数（舍入为 2）。其他三种方法用来确定值的上下界。\n即使在舍入到小数的情况下，也可以使用整数舍入，只需简单地考虑最低有效数字是偶数还是奇数。如保留两位小数，我们把十进制小数 1.234999 舍入到 1.23，把 1.235001 舍入到 1.24，而 1.235 和 1.245 均舍入到 1.24。这种方法同样可以推广到二进制小数，此时应将中间值舍入到最低有效位等于 0 的数。\n浮点运算 浮点数的加法和乘法是实际运算后进行舍入后的结果，即对于实数 $x, y$，以及运算 $\\odot$，结果为 $Round(x\\odot y)$。IEEE 标准规定了浮点数运算的行为，这意味着它不依赖于任何具体的硬件或软件，从而实现了可移植性。\n上文提到整数的加法和乘法形成了阿贝尔群，而实数亦如此，但浮点数还要考虑舍入对其特性的影响。浮点数 $\\infin$ 和 NaN 没有逆元，加法也只满足交换律但不满足结合律。类似地，浮点数乘法只满足交换律，而不满足结合律和分配律。这几种特性的缺乏，对程序员有着非常重要的影响。例如下面的简单程序：\n1 2  x = a + b + c; y = b + c + d;   编译器可能试图产生如下代码来省去一个浮点加法从而提升运算效率：\n1 2 3  t = b + c; x = t + a; y = t + d;   由于使用了加法结合律，计算的结果就有可能产生不同。因此大多数编译器倾向于非常保守，避免任何可能对功能造成影响的优化。\n","description":"","tags":["OS"],"title":"CSAPP 读书笔记：信息的表示和处理","uri":"/posts/representing-and-manipulating-information-note/"},{"content":"信息就是 Bits + Context 一堆 bit 可以表示系统中的所有信息，包括磁盘中的文件、内存中的程序和用户数据以及网络中传输的数据，区分它们的唯一方式便是我们查看这些数据对象时所处的上下文（Context）。例如，相同的一串 bit 在不同的 Context 中可能代表一个整数，也可能代表一个浮点数，甚至字符串。\n程序的转化过程 一个简单的 C 程序 hello.c 如下：\n1 2 3 4 5 6 7  #include \u003cstdio.h\u003e int main() { printf(\"hello world\\n\"); return 0; }   高级的 C 程序文件 hello.c 被转化为一系列低级的机器语言指令，最后以二进制可执行文件存储在磁盘中。\n 预处理阶段（Preprocessor）：预处理器修改 C 程序文件中以 # 号开头的命令。如 hello.c 中的#include \u003cstdio.h\u003e命令将会告诉预处理器系统头文件 stdio.h 的内容，然后将其直接插入到程序文本中。生成的新程序文件为 hello.i； 编译阶段（Compilation）：编译器将 hello.i 文件转化为由汇编语言组成的 hello.s 文件。每条汇编语句都描述了一条低级的机器语言指令，不同高级语言编译后的汇编语句是通用的； 汇编阶段（Assembly）：汇编器将 hello.s 文件转化为由二进制机器语言指令的 hello.o 文件。如果我们用文本编辑器打开该文件，将会展现出一堆乱码； 链接阶段（Linking）：由于我们的程序调用了printf函数，而它存在于一个名为 printf.o 的预编译文件中。链接器负责将该文件并入，得到最终的可执行文件 hello。  系统的硬件组成  总线（Buses）：贯穿整个系统的一组电子管道，负责在各个组件之间传递给定大小的字节块（称为word）。word的大小是系统的基本参数，一般有 4 字节（32 位）或 8 字节（64 位）两种； I/O 设备：系统与外部世界连接的桥梁。图中的 I/O 设备有用于用户输入的键盘⌨️和鼠标🖱️、用于展示的用户输出以及用于长期存储数据和程序的磁盘驱动，每个 I/O 设备都通过控制器（Controller）或适配器（Adapter）与 I/O 总线相连。其中，控制器是设备自身或系统主板（Motherboard）上的芯片组，而适配器则是插在主板插槽上的卡； 主存储器（Main Memory）：处理器执行程序时存放程序和数据的临时存储。物理上来说，内存是由动态随机存取存储器（DRAM, Dynamic Random Access Memory）芯片组成的集合。而逻辑上则是一个线性的字节数组，每个字节都有其唯一地址（从 0 开始的数组索引）； CPU（Central Processing Unit ）: 解释或执行主存储器中指令的引擎。  PC：CPU 的核心是一个大小与word相同的存储设备（或寄存器），称为程序计数器（Program Counter）。PC 始终指向主存储器中某条机器语言指令，即内含其地址。CPU 会不断地重复执行 PC 指向的机器指令，并更新 PC 使其指向下一条指令； Register file：寄存器文件是一个小型存储设备，由一组word大小的寄存器组成，而每个寄存器都有自己的唯一名称； ALU： 算术/逻辑单元（Arithmetic/Logic Unit），能够计算新的数据和地址值。    程序的运行过程 从键盘上读取命令：当我们在终端中输入命令./hello后，bash 程序将逐一读取命令字符串到寄存器（Register），然后存储于内存中；\n从磁盘加载可执行文件到内存：当我们输入回车键后，bash 程序得知输入结束，于是开始加载可执行文件 hello，其中的代码和数据将通过直接存储器访问技术（DMA, Direct Memory Access）从磁盘拷贝到内存中；\n从内存中将结果输出到显示器：处理器执行 hello 文件中的机器语言指令，然后将hello world\\n字符串中的字节从内存拷贝到寄存器文件中，最终传输到用于展示的屏幕🖥上。\n高速缓冲存储 在程序运行的过程中，操作系统花费了大量时间将信息从一个地方拷贝另一个地方，CPU 从寄存器文件中读取数据要比从主存储器中读取快近百倍。因此系统设计者引入了一种更小、更快的存储设备，称为高速缓冲存储（Cache Memories or Caches），它会暂存 CPU 在短期内需要用到的数据。\nL1 级别的 Caches 位于 CPU 芯片之上，容量为上万字节并且拥有和寄存器文件相当的访问速度。而 L2 级别的 Caches 则通过一条特殊总线连接到 CPU，容量可达十万到百万字节。虽然其访问速度比 L1 Cache 慢五倍左右，但依然比主存储器要快五到十倍。在某些先进的操作系统中，还会使用 L3 级别的 Cache，它们均是通过静态随机存取存储器（SRAM, Static Random Access Memory）实现的。\n计算机系统中的存储器层级结构如下图所示，低层次的存储器作为相邻高层次存储器的 Cache：\n操作系统对硬件的管理 操作系统是应用程序和硬件的中间层，应用程序对硬件的所有操作必须通过操作系统实现。\n操作系统有两个主要功能：防止硬件被失控的应用程序所滥用；为应用程序提供一种简单而统一的机制来处理复杂且通常差异很大的低级硬件设备。上述两种功能是通过下图中的几个基本抽象实现的：\n文件（FIles）是对 I/O 设备的抽象，虚拟内存 (Virtual Memory) 是对主存储器和 I/O 设备的抽象，而进程（Processes）则是对处理器，主存储器和 I/O 设备的抽象。\n进程 进程是操作系统对正在运行的程序的抽象，它让我们的 hello 程序看起来像是系统中唯一运行的程序。多个进程可以并行地在同一个系统中运行，同时每个进程都好像在独占硬件的使用权。而实际上不同进程中的指令是交错执行的，基于下图中的上下文切换：\n操作系统会跟踪进程运行所需的所有状态信息，即上下文（Context），它包括了程序计数器 PC 的当前值，寄存器文件和主存储器的内容之类的信息。 在任何时间点，单处理器系统只能为单个进程执行代码。 当操作系统决定将控制权从当前进程转移到某个新进程时，它需要首先保存当前进程的上下文，然后还原新进程的上下文，最后将控制权传递给新进程以完成上下文切换。\n进程间的转换是由操作系统内核（kernel）管理的。kernel 并不是一个单独的进程，而是操作系统代码的一部分，始终存在于内存中。当一个应用程序需要操作系统完成一些操作，比如读写文件时，它便会执行一个特殊的系统调用指令，然后将控制权移交给 kernel。kernel 负责实现程序需要进行的操作，并将结果返回给程序。\n线程 每个进程可以由多个执行单元（线程）组成。由于每个线程都运行在进程的上下文中，且不同线程之间可以共享进程内的代码和全局数据，因此线程要比进程更加高效。\n虚拟内存 虚拟内存让每个进程都看起来独占了主存储器的使用权。每个进程看到的内存空间都是相同的，称为虚拟地址空间（VIrtual Address Space），其组成如下：\n 程序代码和数据：所有进程的代码都始于相同的固定地址，随后则是与全局变量相关的数据区。它们的大小在进程开始运行时固定； 堆（Heap）：运行时堆是调用malloc或free这样的 C 标准库生成的结果，其大小可以在进程运行时动态扩缩容； 共享库（Share libraries）：存放如 C 标准库、数学库这样的共享库的代码和数据的区域； 栈（Stack）：编译器实现函数调用的区域，其大小同样可以在进程运行时动态扩缩容。如果我们调用一个函数，栈就会增长。而每当一个函数返回时，栈便会缩小； 内核虚拟内存：为 kernel 预留的内存空间。  文件 文件是由字节组成的序列，因此所有的 I/O 设备（包括网络）都可以被看作文件。系统中的所有输入和输出都可以通过 Unix I/O（一组系统调用），对文件进行读写来实现。\n多处理器系统 由单一操作系统内核控制的多个处理器可以共同组成一个多处理器系统（Multiprocessor System），它基于多核（Multi-core）处理器以及超线程技术（Hyperthreading）。\n多核处理器 多核处理器将多个 CPU 集成到单个集成电路芯片中，每个 CPU 称为一个核（cores）。下图展示了一个典型的多核处理器的架构。其中 L1 级别的 Cache 被分成了两部分，分别存储短期内需要使用的指令（i-cache）和数据（d-cache）：\n超线程技术 超线程技术允许单个 CPU 执行多个控制流，有时也被称为同步多线程（Simultaneous Multi-threading）。通过对 CPU 中的程序计数器、寄存器文件等硬件资源进行拷贝，将一个物理 CPU 虚拟为多个逻辑 CPU，从而实现多个线程的并行计算。常规的 CPU 需要大约两万个时钟周期（clock cycle）完成不同线程间的切换，而超线程的 CPU 可以在单个时钟周期内决定要执行哪一个线程，这使得 CPU 能够更好地利用它的执行资源。比如一个逻辑 CPU 执行的线程需要等待数据加载到 Cache 中，那么另一个逻辑 CPU 就可以向其借用执行资源继续执行其他线程。\n与使用多个物理 CPU 的传统多处理器系统不同，超线程内核中的逻辑 CPU 共享执行资源。因此当两个线程同时需要某个执行资源时，其中一个线程必须让出资源暂时挂起，直到这些资源空闲后才能继续执行。\n","description":"","tags":["OS"],"title":"CSAPP 读书笔记：计算机系统之旅","uri":"/posts/a-tour-of-computer-systems-note/"},{"content":"前言 在 Kubernetes Pod 是如何跨节点通信的？ 中，我们简单地介绍了 Kubernetes 中的两种 SDN 网络模型：Underlay 和 Overlay。而 Openshift 中的 SDN 则是由 Overlay 网络 OVS（Open vSwitch）实现的，其使用的插件如下：\n ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamespaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。  OVS 在每个 Openshift 节点上都创建了如下网络接口：\n br0：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离； vxlan0：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信； tun0：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信； veth：Pod 通过veth-pair连接到br0网桥的端点。  使用 ovs-ofctl -O OpenFlow13 show br0 命令可以查看br0上的所有端口及其编号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  [root@node1 ~]# ovs-ofctl -O OpenFlow13 show br0 OFPT_FEATURES_REPLY (OF1.3) (xid=0x2): dpid:0000ea00372f1940 n_tables:254, n_buffers:0 capabilities: FLOW_STATS TABLE_STATS PORT_STATS GROUP_STATS QUEUE_STATS OFPST_PORT_DESC reply (OF1.3) (xid=0x3): 1(vxlan0): addr:72:23:a0:a9:14:a7 config: 0 state: 0 speed: 0 Mbps now, 0 Mbps max 2(tun0): addr:62:80:67:c6:38:58 config: 0 state: 0 speed: 0 Mbps now, 0 Mbps max 8381(vethd040c191): addr:7a:d9:f4:12:94:5f config: 0 state: 0 current: 10GB-FD COPPER speed: 10000 Mbps now, 0 Mbps max ... LOCAL(br0): addr:76:ab:cf:6f:e1:46 config: PORT_DOWN state: LINK_DOWN speed: 0 Mbps now, 0 Mbps max OFPT_GET_CONFIG_REPLY (OF1.3) (xid=0x5): frags=nx-match miss_send_len=0   考虑到 Openshift 集群的复杂性，我们分别按以下几种场景分析数据包的流向：\n 节点内 Pod 互访：Pod to Local Pod Pod 跨节点互访：Pod to Remote Pod Pod 访问 Service：Pod to Service Pod 与集群外部互访：Pod to External  由于高版本（3.11 以上）的 Openshift 不再以守护进程而是以 Pod 的形式部署 OVS 组件，不方便对 OpenFlow 流表进行查看，因此本文选用的集群版本为 3.6：\n1 2 3 4 5 6 7 8  [root@node1 ~]# oc version  oc v3.6.173.0.5 kubernetes v1.6.1+5115d708d7 features: Basic-Auth GSSAPI Kerberos SPNEGO Server https://test-cluster.ocp.koktlzz.com:8443 openshift v3.6.173.0.5 kubernetes v1.6.1+5115d708d7   另外，实验用集群并未开启 ovs-multitenant，即未进行多租户隔离。整个集群 Pod 网络是扁平化的，所有 Pod 的 VNID 都为默认值 0。\nPod to Local Pod 数据包首先通过veth-pair送往 OVS 网桥br0，随后便进入了br0上的 OpenFlow 流表。我们可以用 ovs-ofctl -O OpenFlow13 dump-flows br0 命令查看流表中的规则，同时为了让输出结果更加简洁，略去 cookie 和 duration 的信息：\n table=0, n_packets=62751550874, n_bytes=25344802160312, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.130.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]-\u003eNXM_NX_REG0[],goto_table:10 table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30 table=0, n_packets=833353346930, n_bytes=329854403266173, priority=100,ip actions=goto_table:20  table0 中关于 IP 数据包的规则主要有三条，其中前两条分别对应流入端口in_port为 1 号端口vxlan0和 2 号端口tun0的数据包。这两条规则的优先级priority都是 200，因此只有在两者均不符合情况下，才会匹配第三条规则。由于本地 Pod 发出的数据包是由veth端口进入的，因此将转到 table20；\n table=20, n_packets=607178746, n_bytes=218036511085, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0-\u003eNXM_NX_REG0[],goto_table:21 table=21, n_packets=833757781068, n_bytes=329871389393381, priority=0 actions=goto_table:30  table20 会匹配源地址nw_src为 10.130.9.154 且流入端口in_port为 8422 的数据包，随后将 Pod1 的 VNID 0 作为源 VNID 存入寄存器 0 中，经由 table21 转到 table30；\n table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70 table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90 table=30, n_packets=21061319859, n_bytes=29568807363654, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60 table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100  table30 中匹配数据包目的地址nw_dst的规则有四条，前三条分别对应本节点内 Pod 的 CIDR 网段 10.130.8.0/23、集群内 Pod 的 CIDR 网段 10.128.0.0/14 和 Service 的 ClusterIP 网段 172.30.0.0/16。第四条优先级最低，用于 Pod 对集群外部的访问。由于数据包的目的地址 10.130.9.158 符合第一条规则，且第一条规则的优先级最高，因此将转到 table70；\n table=70, n_packets=597219981, n_bytes=243824445346, priority=100,ip,nw_dst=10.130.9.158 actions=load:0-\u003eNXM_NX_REG1[],load:0x20ea-\u003eNXM_NX_REG2[],goto_table:80  table70 匹配目的地址nw_dst为 Pod2 IP 10.130.9.158 的数据包，并将 Pod2 的 VNID 0 作为目的 VNID 存入寄存器 1 中。同时端口号0x20ea被保存到寄存器 2 中，然后转到 table80；\n table=80, n_packets=1112713040332, n_bytes=293801616636499, priority=200 actions=output:NXM_NX_REG2[]  table80 比较寄存器 0 和寄存器 1 中保存的源/目的 VNID。若二者一致，则根据寄存器 2 中保存的端口号将数据包送出。\n端口号0x20ea是一个十六进制数字，即十进制数 8426。而 Pod2 正是通过 8426 号端口设备vethba48c6de连接到br0上，因此数据包便最终通过它流入到了 Pod2 中。\n1 2  [root@node1 ~]# ovs-ofctl -O OpenFlow13 show br0 | grep 8426 8426(vethba48c6de): addr:e6:b2:7e:42:41:91   Pod to Remote Pod Packet in Local Pod 数据包依然首先通过veth-pair送往 OVS 网桥br0，随后便进入了br0上的 OpenFlow 流表：\n table=0, n_packets=830232155588, n_bytes=328613498734351, priority=100,ip actions=goto_table:20 table=20, n_packets=1901, n_bytes=299279, priority=100,ip,in_port=6635,nw_src=10.130.9.154 actions=load:0-\u003eNXM_NX_REG0[],goto_table:21 table=21, n_packets=834180030914, n_bytes=330064497351030, priority=0 actions=goto_table:30  与 Pod to Local Pod 的流程一致，数据包根据规则转到 table30；\n table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90 table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70  数据包的目的地址为 Pod2 IP 10.131.8.206，不属于本节点 Pod 的 CIDR 网段 10.130.8.0/23，而属于集群 Pod 的 CIDR 网段 10.128.0.0/14，因此转到 table90；\n table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]-\u003eNXM_NX_TUN_ID[0..31],set_field:10.122.28.8-\u003etun_dst,output:1  table90 根据目的 IP 的所属网段 10.131.8.0/23 判断其位于 Node2 上，于是将 Node2 IP 10.122.28.8 设置为tun_dst。并且从寄存器 0 中取出 VNID 的值，从 1 号端口vxlan0输出。\nvxlan0作为一个 VTEP 设备（参见 Overlay Network），将根据 table90 发来的信息，对数据包进行一层封装：\n 目的地址（dst IP） –\u003e tun_dst –\u003e 10.122.28.8 源地址（src IP） –\u003e Node1 IP –\u003e 10.122.28.7 源 VNID –\u003e NXM_NX_TUN_ID[0..31] –\u003e 0  由于封装后的数据包源/目的地址均为节点 IP，因此从 Node1 的网卡流出后，可以通过物理网络设备转发到 Node2 上。\nPacket in Remote Pod Node2 上的vxlan0对数据包进行解封，随后从br0上的 1 号端口进入 OpenFlow 流表中：\n table=0, n_packets=52141153195, n_bytes=17269645342781, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.131.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]-\u003eNXM_NX_REG0[],goto_table:10  table0 判断数据包的流入端口in_port、源 IP 所属网段nw_src和目的 IP 所属网段nw_dst均符合该条规则，于是保存数据包中的源 VNID 到寄存器 0 后转到 table10；\n table=10, n_packets=10147760036, n_bytes=4060517391502, priority=100,tun_src=10.122.28.7 actions=goto_table:30  table10 确认 VxLAN 隧道的源 IPtun_src就是节点 Node1 的 IP 地址，于是转到 table30；\n table=30, n_packets=678759566065, n_bytes=172831151192704, priority=200,ip,nw_dst=10.131.8.0/23 actions=goto_table:70  table30 确认数据包的目的 IP（即 Pod2 IP）存在于 Node2 中 Pod 的 CIDR 网段内，因此转到 table70；\n table=70, n_packets=193211683, n_bytes=27881218388, priority=100,ip,nw_dst=10.131.8.206 actions=load:0-\u003eNXM_NX_REG1[],load:0x220-\u003eNXM_NX_REG2[],goto_table:80  table70 发现数据包的目的 IP 与 Pod2 IP 相符，于是将 Pod2 的 VNID 作为目的 VNID 存于寄存器 1 中，将0x220（十进制数 544）保存在寄存器 2 中，然后转到 table80；\n table=80, n_packets=676813794014, n_bytes=172576112594488, priority=200 actions=output:NXM_NX_REG2[]  table80 会检查保存在寄存器 0 和寄存器 1 中的源/目的 VNID，若相等（此例中均为 0），则从 544 号端口输出。\nbr0上的 544 端口对应的网络接口是vethe9f523a9，因此数据包便最终通过它流入到了 Pod2 中。\n1 2  [root@node2 ~]# ovs-ofctl -O OpenFlow13 show br0 | grep 544 544(vethe9f523a9): addr:b2:a1:61:00:dc:3b   Pod to Service 在本例中，Pod1 通过 Service 访问其后端的 Pod2，其 ClusterIP 为 172.30.107.57，监听的端口为 8080：\n1 2 3  [root@node1 ~]# oc get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE myService 172.30.107.57 \u003cnone\u003e 8080/TCP 2y    table=30, n_packets=21065939280, n_bytes=29573447694924, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60  数据包在送到 OpenFlow 流表 table30 前的步骤与 Pod to Local Pod 和 Pod to Remote Pod 中的情况一致，但数据包的目的地址变为了 myService 的 ClusterIP。因此将匹配nw_dst中的 172.30.0.0/16 网段，转到 table60；\n table=60, n_packets=0, n_bytes=0, priority=100,tcp,nw_dst=172.30.107.57,tp_dst=8080 actions=load:0-\u003eNXM_NX_REG1[],load:0x2-\u003eNXM_NX_REG2[],goto_table:80  table60 匹配目的地址nw_dst为 172.30.107.57 且目的端口为 8080 的数据包，并将 Pod1 的 VNID 0 保存到寄存器 1 中，将0x2（十进制数字 2）保存到寄存器 2 中，转到 table80；\n table=80, n_packets=1113435014018, n_bytes=294106102133061, priority=200 actions=output:NXM_NX_REG2[]  table80 首先检查目的 Service 的 VNID 是否与寄存器 1 中的 VNID 一致，然后根据寄存器 2 中的数字将数据包从 2 号端口tun0送出，最后进入节点的 iptables 规则中。\niptables 对数据包的处理流程如下图所示：\n由于 Service 的实现依赖于 NAT（上图中的紫色方框），因此我们可以在 NAT 表中查看到与之相关的规则：\n1 2 3 4 5 6 7 8  [root@node1 ~]# iptables -t nat -nvL Chain OUTPUT (policy ACCEPT 4753 packets, 489K bytes) pkts bytes target prot opt in out source destination 2702M 274G KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ Chain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 4 240 KUBE-SVC-QYWOVDCBPMWAGC37 tcp -- * * 0.0.0.0/0 172.30.107.57 /* demo/myService:8080-8080 cluster IP */ tcp dpt:8080   本机产生的数据包（Locally-generated Packet）首先进入OUTPUT链，然后匹配到自定义链KUBE-SERVICES。由于其目的地址为 Service 的 ClusterIP 172.30.107.57，因此将再次跳转到对应的KUBE-SVC-QYWOVDCBPMWAGC37链：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  Chain KUBE-SVC-QYWOVDCBPMWAGC37 (1 references) pkts bytes target prot opt in out source destination 1 60 KUBE-SEP-AF5DIL6JV3XLLV6G all -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ statistic mode random probability 0.50000000000 1 60 KUBE-SEP-ADAJHSV7RYS5DUBX all -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ Chain KUBE-SEP-ADAJHSV7RYS5DUBX (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.131.8.206 0.0.0.0/0 /* demo/myService:8080-8080 */ 0 0 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ tcp to:10.131.8.206:8080 Chain KUBE-SEP-AF5DIL6JV3XLLV6G (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.128.10.57 0.0.0.0/0 /* demo/myService:8080-8080 */ 23 1380 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* demo/myService:8080-8080 */ tcp to:10.128.10.57:8080   KUBE-SVC-QYWOVDCBPMWAGC37链下有两条完全相同的匹配规则，对应了该 Service 后端的两个 Pod。KUBE-SEP-ADAJHSV7RYS5DUBX链和 KUBE-SEP-AF5DIL6JV3XLLV6G链能够执行 DNAT 操作，分别将数据包的目的地址转化为 Pod IP 10.131.8.206 和 10.128.10.57。在一次通信中只会有一条链生效，这体现了 Service 的负载均衡能力。\n完成OUTPUTDNAT 的数据包将进入节点的路由判断（Routing Decision）。由于当前目的地址已经属于集群内 Pod 的 CIDR 网段 10.128.0.0/14，因此将再次从tun0端口再次进入 OVS 网桥br0中。\n1 2 3 4 5 6 7 8 9  [rootnode1 ~]# route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.122.28.1 0.0.0.0 UG 0 0 0 eth0 10.122.28.0 0.0.0.0 255.255.255.128 U 0 0 0 eth0 10.128.0.0 0.0.0.0 255.252.0.0 U 0 0 0 tun0 169.254.0.0 0.0.0.0 255.255.0.0 U 1008 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 172.30.0.0 0.0.0.0 255.255.0.0 U 0 0 0 tun0   不过数据包在进入br0之前，还需要经过 iptables 中的POSTROUTING链，完成一次 MASQUERADE 操作：数据包的源地址转换为其流出端口的 IP，即tun0的 IP 10.130.8.1。\n1 2 3 4 5 6 7 8 9 10 11  [root@node1 ~]# iptables -t nat -nvL  Chain POSTROUTING (policy ACCEPT 5083 packets, 524K bytes) pkts bytes target prot opt in out source destination 2925M 288G OPENSHIFT-MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* rules for masquerading OpenShift traffic */ Chain OPENSHIFT-MASQUERADE (1 references) pkts bytes target prot opt in out source destination 321M 19G MASQUERADE all -- * * 10.128.0.0/14 0.0.0.0/0 /* masquerade pod-to-service and pod-to-external traffic */ [root@node1 ~]# ip a | grep tun0 16: tun0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1450 qdisc noqueue state UNKNOWN qlen 1000 inet 10.130.8.1/23 scope global tun0   本例中 Service 的后端 Pod 均在 Pod1 所在的节点外，因此数据包第二次进入 OpenFlow 流表时匹配的规则基本与 Pod to Remote Pod 一致：\n table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30 table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90 table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]-\u003eNXM_NX_TUN_ID[0..31],set_field:10.122.28.8-\u003etun_dst,output:1  其传递流程如下图所示：\nPod2 返回的数据包在到达 Node1 后将被vxlan0解封装，然后根据其目的地址tun0进入 OpenFlow 流表：\n table=0, n_packets=1084362760247, n_bytes=297224518823222, priority=200,ip,in_port=2 actions=goto_table:30 table=30, n_packets=20784385211, n_bytes=4742514750371, priority=300,ip,nw_dst=10.130.8.1 actions=output:2  数据包从 2 号端口tun0流出后进入节点的 iptables 规则，随后将触发 iptables 的 Connection Tracking 操作：根据 /proc/net/nf_conntrack 文件中的记录进行“DeNAT”。返回数据包的源/目的地址从 Pod2 IP 10.131.8.206 和 tun0 IP 10.130.8.1，变回 Service 的 ClusterIP 172.30.107.57 和 Pod1 IP 10.130.9.154。\n1 2  [root@node1 ~]# cat /proc/net/nf_conntrack | grep -E \"src=10.130.9.154.*dst=172.30.107.57.*dport=8080.*src=10.131.8.206\" ipv4 2 tcp 6 431986 ESTABLISHED src=10.130.9.154 dst=172.30.107.57 sport=80 dport=8080 src=10.131.8.206 dst=10.130.8.1 sport=8080 dport=80 [ASSURED] mark=0 secctx=system_u:object_r:unlabeled_t:s0 zone=0 use=2   Pod to External 数据包依然首先通过veth-pair送往 OVS 网桥br0，随后便进入了br0上的 OpenFlow 流表：\n table=0, n_packets=837268653828, n_bytes=331648403594327, priority=100,ip actions=goto_table:20 table=20, n_packets=613807687, n_bytes=220557571042, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0-\u003eNXM_NX_REG0[],goto_table:21 table=21, n_packets=837674296060, n_bytes=331665441915651, priority=0 actions=goto_table:30 table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100 table=100, n_packets=761732023982, n_bytes=282091648536325, priority=0 actions=output:2  数据包从tun0端口流出后进入节点的路由表及 iptables 规则：\n1 2 3 4 5 6 7 8  Chain POSTROUTING (policy ACCEPT 2910 packets, 299K bytes) pkts bytes target prot opt in out source destination 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 2940M 289G OPENSHIFT-MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* rules for masquerading OpenShift traffic */ Chain OPENSHIFT-MASQUERADE (1 references) pkts bytes target prot opt in out source destination 322M 19G MASQUERADE all -- * * 10.128.0.0/14 0.0.0.0/0 /* masquerade pod-to-service and pod-to-external traffic */   访问集群外部显然需要通过节点的默认网关，因此数据包将从节点网卡eth0送出。而在POSTROUTING链中，数据包的源地址由 Pod IP 转换为了eth0的 IP 10.122.28.7。完整流程如下图所示（图中的 Router 指的是路由器而非 Openshift 中的概念）：\nFuture Work  本文并未涉及 External to Pod 的场景，它是如何实现的？我们都知道 Openshift 是通过 Router（HAProxy）来暴露集群内部服务的，那么数据包在传输过程中的 NAT 操作是怎样进行的？ 除了本文提到的几种网络接口外，Openshift 节点上还存在着ovs-system和vxlan_sys_4789。它们的作用是什么？ Openshift 4.X 版本的网络模型与本文实验用的 3.6 版本相比有那些变化？  参考文献 OpenFlow - Wikipedia\nOVS 在云项目中的使用\nOpenShift SDN - OpenShift Container Platform 3.11\n理解 OpenShift（3）：网络之 SDN\n[译] 深入理解 iptables 和 netfilter 架构\nLinux Netfilter: How does connection tracking track connections changed by NAT?\n","description":"","tags":["Openshift","Container Network","CNI","Open vSwitch"],"title":"对 Openshift SDN 网络模型的一些探索","uri":"/posts/explorations-on-the-openshift-sdn-network-model/"},{"content":"前言 A Guide to the Kubernetes Networking Model 一文生动形象地介绍了 Kubernetes 中的网络模型，然而受篇幅所限，作者并没有对 Pod 跨节点通信时数据包在节点之间传递的细节进行过多讨论。\n我们已经知道，Docker 使用端口映射的方式实现不同主机间容器的通信，Kubernetes 中同样也有 hostPort 的概念。但是当节点和 Pod 的数量上升后，手动管理节点上绑定的端口是十分困难的，这也是NodePort类型的 Service 的缺点之一。而一旦 Pod 不再“借用”节点的 IP 和端口来暴露自身的服务，就不得不面临一个棘手的问题：Pod 的本质是节点中的进程，节点外的物理网络设备（交换机/路由器）并不知晓 Pod 的存在。它们在接收目的地址为 Pod IP 的数据包时，无法完成进一步的传输工作。\n为此我们需要使用一些 CNI（Container Network Interface）插件来完善 Kubernetes 集群的网络模型，这种新型的网络设计理念称为 SDN（Software-defined Networking）。根据 SDN 实现的层级，我们可以将其分为 Underlay Network 和 Overlay Network：\n Overlay 网络允许设备跨越底层物理网络（Underlay Network）进行通信，而底层却并不知晓 Overlay 网络的存在。Underlay 网络是专门用来承载用户 IP 流量的基础架构层，它与 Overlay 网络之间的关系有点类似物理机和虚拟机。Underlay 网络和物理机都是真正存在的实体，它们分别对应着真实存在的网络设备和计算设备，而 Overlay 网络和虚拟机都是依托在下层实体使用软件虚拟出来的层级。\n Underlay Network 利用 Underlay Network 实现 Pod 跨节点通信，既可以只依赖 TCP/IP 模型中的二层协议，也可以使用三层。但无论哪种实现方式，都必须对底层的物理网络有所要求。\n二层 如图所示，Pod 与节点的 IP 地址均处于同一网段。当 Pod1 向另一节点上的 Pod2 发起通信时，数据包首先通过veth-pair和cbr0送往 Node1 的网卡。由于目的地址 10.86.44.4 与 Node1 同网段，因此 Node1 将通过 ARP 广播请求 10.86.44.4 的 MAC 地址。\nCNI 插件不仅为 Pod 分配 IP 地址，它还会将每个 Pod 所在的节点信息下发给 SDN 交换机。这样当 SDN 交换机接收到 ARP 请求时，将会答复 Pod2 所在节点 Node2 的 MAC 地址，数据包也就顺利地送到了 Node2 上。\n阿里云 Terway 模式的 ACK 服务使用的便是这种网络模型，只不过 Pod 间通信使用的 SDN 交换机不再是节点的交换机（下图中的 Node VSwitch），而是单独创建的 Pod VSwitch：\n三层 如图所示，Pod 与节点的 IP 地址不再处于同一网段。当 Pod1 向另一节点上的 Pod2 发起通信时，数据包首先通过veth-pair和cbr0进入宿主机内核的路由表（Routing Table）。CNI 插件在该表中添加了若干条路由规则，如目的地址为 Pod2 IP 的网关为 Node2 的 IP。这样数据包的目的 MAC 地址就变为了 Node2 的 MAC 地址，它将会通过交换机发送到 Node2 上。\n由于这种实现方式基于三层协议，因此不要求两节点处于同一网段。不过需要将目的地址为 Pod2 IP 的网关设置为 SDN 路由器的 IP，且该路由器能够知晓目的 Pod 所在的节点。这样数据包的目的 MAC 地址就会首先变为 SDN 路由器的 MAC 地址，经过路由器后再变为 Node2 的 MAC 地址：\n通过上面的讨论我们发现，想要实现三层的 Underlay 网络，需要在多个节点间下发和同步路由表。于是很容易想到用于交换路由信息的 BGP（Border Gateway Protocol）协议：\n 边界网关协议（英语：Border Gateway Protocol，缩写：BGP）是互联网上一个核心的去中心化自治路由协议。它通过维护 IP 路由表或“前缀”表来实现自治系统（AS）之间的可达性，属于矢量路由协议。BGP 不使用传统的内部网关协议（IGP）的指标，而使用基于路径、网络策略或规则集来决定路由。因此，它更适合被称为矢量性协议，而不是路由协议。\n 对于 Calico 的 BGP 模式来说，我们可以把集群网络模型视为在每个节点上都部署了一台虚拟路由器。路由器可以与其他节点上的路由器通过 BGP 协议互通，它们称为一对 BGP Peers。Calico 的默认部署方式为 Full-mesh，即创建一个完整的内部 BGP 连接网，每个节点上的路由器均互为 BGP Peers。这种方式仅适用于 100 个节点以内的中小型集群，在大型集群中使用的效率低下。而 Route reflectors 模式则将部分节点作为路由反射器，其他节点上的路由器只需与路由反射器互为 BGP Peers。这样便可以大大减少集群中 BGP Peers 的数量，从而提升效率。\nOverlay Network Overlay 网络可以通过多种协议实现，但通常是对 IP 数据包进行一层外部封装（Encapsulation）。这样底层的 Underlay 网络便只会看到外部封装的数据，而无需处理内部的原有数据。Overlay 网络发送数据包的方式取决于其类型和使用的协议，如基于 VxLAN 实现 Overlay 网络，数据包将被外部封装后以 UDP 协议进行发送：\nOverlay 网络的实现并不依赖于底层物理网络设备，因此我们就以一个两节点不处于同一网段且 Pod 与节点亦处于不同网段的例子来说明 Overlay 网络中的数据包传递过程。集群网络使用 VxLAN 技术组建，虚拟网络设备 VTEP（Virtual Tunnel End Point）将会完成数据包的封装和解封操作。\nNode1 上的 VTEP 收到 Pod1 发来的数据包后，首先会在本地的转发表中查找目的 Pod 所在节点的 IP，即 192.168.1.100。随后它将本机 IP 地址 10.86.44.2、Node2 的 IP 地址 192.168.1.100 和 Pod1 的 VNID（VxLAN Network Identifier）封装在原始数据包外，从 Node1 的网络接口 eth0 送出。由于新构建的数据包源/目的地址均为节点的 IP，因此外部的路由器可以将其转发到 Node2 上。Node2 中的 VTEP 在接收到数据包后会首先进行解封，若源 VNID（Pod1 的 VNID）与目的 VNID（Pod2 的 VNID）一致，便会根据原始数据包中的目的地址 172.100.1.2 将其发送到 Pod2 上。此处的 VNID 检查，主要是为了实现集群的网络策略管理和多租户隔离。\n通过对上述几种 SDN 网络模型的讨论，我们发现只有 Overlay 网络需要对数据包进行封装和解封，因此它的性能相比于 Underlay 网络较差。但 Overlay 网络也有以下优点：\n 对底层网络设备的依赖性最小。即使 Pod 所在的节点发生迁移，依然可以通过 Overlay 网络与原集群实现二层网络的互通； VNID 共有 24 位，因此可以构造出约 1600 万个互相隔离的虚拟网络。  Future Work  除了 VxLAN 以外，还有哪些技术可以实现 Overlay 网络？它们是怎样传输数据的呢？ 本文在讨论 Underlay 网络时提到了 Terway 和 Calico，那么有哪些使用 Overlay 网络的 CNI 插件呢？ 更新：我在 对 Openshift SDN 网络模型的一些探索 中介绍了基于 Overlay 网络的 Open vSwitch； 近年来发展迅速的 Cilium 是怎样实现 SDN 网络的？它所依赖的 eBPF 技术又是什么？  参考文献 Software-defined networking - Wikipedia\nAbout Kubernetes Networking\n使用 Terway 网络插件\n边界网关协议 - Wikipedia\nConfigure BGP peering - Calico\n为什么集群需要 Overlay 网络\n","description":"","tags":["Kubernetes","Container Network","CNI","Calico"],"title":"Kubernetes Pod 是如何跨节点通信的？","uri":"/posts/how-kubernetes-pods-communicate-across-nodes/"},{"content":"前言 通常，Kafka 中的每个 Partiotion 中有多个副本 (Replica) 以实现高可用。想象一个场景，Consumer 正在消费 Leader 中 Offset=10 的数据，而此时 Follower 中只同步到 Offset=8。那么当 Leader 所在的 Broker 宕机后，当前 Follower 经选举成为新的 Leader，Consumer 再次消费时便会报错。因此，Kafka 引入了 HW（High Watermark，高水位）机制来保证副本数据的可靠性和一致性。\nHW 是什么？ HW 定义了消息的可见性，即标识 Partition 中的哪些消息是可以被 Consumer 消费的，只有小于 HW 值的消息才被认为是已备份或已提交的（committed）。而 LEO（Log End Offset）则表示副本写入下一条消息的 Offset，因此同一副本的 HW 值永远不会大于其 LEO 值。\n当集群中副本所在的 Broker 发生故障而后恢复时，副本先将数据截断（Truncation）到其 HW 处（LEO 等于 HW），然后再开始向 Leader 同步数据。\nHW 的更新机制 每一个副本都保存了其 HW 值和 LEO 值，即 Leader HW（实际上也是 Partition HW）、Leader LEO 和 Follower HW、Follower LEO。而 Leader 所在的 Broker 上还保存了其他 Follower 的 LEO 值，称为 Remote LEO。上述几个值的更新流程如下：\n如图所示，当 Producer 向 log 文件写入数据时，Leader LEO 首先被更新。而 Remote LEO 要等到 Follower 向 Leader 发送同步请求（Fetch）时，才会根据请求携带的当前 Follower LEO 值更新。随后，Leader 计算所有副本 LEO 的最小值，将其作为新的 Leader HW。考虑到 Leader HW 只能单调递增，因此还增加了一个 LEO 最小值与当前 Leader HW 的比较，防止 Leader HW 值降低（max[Leader HW, min(All LEO)]）。\nFollower 在接收到 Leader 的响应（Response）后，首先将消息写入 log 文件中，随后更新 Follower LEO。由于 Response 中携带了新的 Leader HW，Follower 将其与刚刚更新过的 Follower LEO 相比较，取最小值作为 Follower HW（min(Follower LEO, Leader HW)）。\n举例来说，如果一开始 Leader 和 Follower 中没有任何数据，即所有值均为 0。那么当 Prouder 向 Leader 写入第一条消息，上述几个值的变化顺序如下：\n    Leader LEO Remote LEO Leader HW Follower LEO Follower HW     Producer Write 1 0 0 0 0   Follower Fetch 1 0 0 0 0   Leader Update HW 1 0 0 0 0   Leader Response 1 0 0 1 0   Follower Update HW 1 0 0 1 0   Follower Fetch 1 1 0 1 0   Leader Update HW 1 1 1 1 0   Leader Response 1 1 1 1 0   Follower Update HW 1 1 1 1 1    HW 的隐患 通过上面的表格我们发现，Follower 往往需要进行两次 Fetch 请求才能成功更新 HW。Follower HW 在某一阶段内总是落后于 Leader HW，因此副本在根据 HW 值截取数据时将有可能发生数据的丢失或不一致。\n图中两副本的 LEO 均为 2，但 Leader 副本 B 上的 HW 为 2，Follower 副本 A 上的 HW 为 1。正常情况下，副本 A 将在接收 Leader Response 后根据 Leader HW 更新其 Follower HW 为 2。但假如此时副本 A 所在的 Broker 重启，它会把 Follower LEO 修改为重启前自身的 HW 值 1，因此数据 M1（Offset=1）被截断。当副本 A 重新向副本 B 发送同步请求时，如果副本 B 所在的 Broker 发生宕机，副本 A 将被选举成为新的 Leader。即使副本 B 所在的 Broker 能够成功重启且其 LEO 值依然为 2，但只要它向当前 Leader（副本 A）发起同步请求后就会更新其 HW 为 1（计算min(Follower LEO, Leader HW)），数据 M1（Offset=1）随即被截断。如果min.insync.replicas参数为 1，那么 Producer 不会因副本 A 没有同步成功而重新发送消息，M1 也就永远丢失了。\n图中 Leader 副本 B 写入了两条数据 M0 和 M1，Follower 副本 A 只写入了一条数据 M0。此时 Leader HW 为 2，Follower HW 为 1。如果在 Follower 同步第二条数据前，两副本所在的 Broker 均发生重启且副本 B 所在的 Broker 先重启成功，那么副本 A 将成为新的 Leader。这时 Producer 向其写入数据 M2，副本 A 作为集群中的唯一副本，更新其 HW 为 2。当副本 B 所在的 Broker 重启后，它将向当前的 Leader 副本 A 同步数据。由于两者的 HW 均为 2，因此副本 B 不需要进行任何截断操作。在这种情况下，副本 B 中的数据为重启前的 M0 和 M1，副本 A 中的数据却是 M0 和 M2，副本间的数据出现了不一致。\nLeader Epoch Kakfa 引入 Leader Epoch 后，Follower 就不再参考 HW，而是根据 Leader Epoch 信息来截断 Leader 中不存在的消息。这种机制可以弥补基于 HW 的副本同步机制的不足，Leader Epoch 由两部分组成：\n Epoch：一个单调增加的版本号。每当 Leader 副本发生变更时，都会增加该版本号。Epoch 值较小的 Leader 被认为是过期 Leader，不能再行使 Leader 的权力； 起始位移（Start Offset）：Leader 副本在该 Epoch 值上写入首条消息的 Offset。  举例来说，某个 Partition 有两个 Leader Epoch，分别为 (0, 0) 和 (1, 100)。这意味该 Partion 历经一次 Leader 副本变更，版本号为 0 的 Leader 从 Offset=0 处开始写入消息，共写入了 100 条。而版本号为 1 的 Leader 则从 Offset=100 处开始写入消息。\n每个副本的 Leader Epoch 信息既缓存在内存中，也会定期写入消息目录下的 leaderer-epoch-checkpoint 文件中。当一个 Follower 副本从故障中恢复重新加入 ISR 中，它将：\n 向 Leader 发送 LeaderEpochRequest，请求中包含了 Follower 的 Epoch 信息； Leader 将返回其 Follower 所在 Epoch 的 Last Offset； 如果 Leader 与 Follower 处于同一 Epoch，那么 Last Offset 显然等于 Leader LEO； 如果 Follower 的 Epoch 落后于 Leader，则 Last Offset 等于 Follower Epoch + 1 所对应的 Start Offset。这可能有点难以理解，我们还是以 (0, 0) 和 (1, 100) 为例进行说明：Offset=100 的消息既是 Epoch=1 的 Start Offset，也是 Epoch=0 的 Last Offset； Follower 接收响应后根据返回的 Last Offset 截断数据； 在数据同步期间，只要 Follower 发现 Leader 返回的 Epoch 信息与自身不一致，便会随之更新 Leader Epoch 并写入磁盘。  在刚刚介绍的数据丢失场景中，副本 A 所在的 Broker 重启后根据自身的 HW 将数据 M1 截断。而现在，副本 A 重启后会先向副本 B 发送一个请求（LeaderEpochRequest）。由于两副本的 Epoch 均为 0，副本 B 返回的 Last Offset 为 Leader LEO 值 2。而副本 A 上并没有 Offset 大于等 2 的消息，因此无需进行数据截断，同时其 HW 也会更新为 2。之后副本 B 所在的 Broker 宕机，副本 A 成为新的 Leader，Leader Epoch 随即更新为 (1, 2)。当副本 B 重启回来并向当前 Leader 副本 A 发送 LeaderEpochRequest，得到的 Last Offset 为 Epoch=1 对应的 Start Offset 值 2。同样，副本 B 中消息的最大 Offset 值只有 1，因此也无需进行数据截断，消息 M1 成功保留了下来。\n在刚刚介绍的数据不一致场景中，由于最后两副本 HW 值相等，因此没有将不一致的数据截断。而现在，副本 A 重启后并便会更新 Leader Epoch 为 (1, 1)，同时也会更新其 HW 值为 2。副本 B 重启后向当前 Leader 副本 A 发送 LeaderEpochRequest，得到的 Last Offset 为 Epoch=1 对应的 Start Offset 值 1，因此截断 Offset=1 的消息 M1。这样只要副本 B 再次发起请求同步消息 M2，两副本的数据便可以保持一致。\n值得一提的是，Leader Epoch 机制在min.insync.replicas参数为 1 且unclean.leader.election.enabled参数为true时依然无法保证数据的可靠性。感兴趣的读者可以阅读 KIP-101 - Alter Replication Protocol to use Leader Epoch rather than High Watermark for Truncation 文中的附录部分。\n参考文献 KIP-101 - Alter Replication Protocol to use Leader Epoch rather than High Watermark for Truncation\n","description":"","tags":["Kafka"],"title":"Kafka 是如何同步副本的？","uri":"/posts/how-does-kafka-synchronize-replicas/"}]